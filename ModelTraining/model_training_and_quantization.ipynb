{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Quantization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will use the dataset created in the previous stage to train a model to detect persons based on CSI data. Then, we will optimize and export it for execution in real time using a ESP32 board.\n",
        "\n",
        "This notebook is divided into four parts:\n",
        "1) **Data importing and feature engineering** (from our previously created dataset)\n",
        "2) **Model training using Tensorflow** (no expensive GPU required!)\n",
        "3) **Model quantization using LiteRT** (for minimizing model size and inference time)\n",
        "4) **Model convertion to C data array** (for execution on the ESP32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OvKTcc75bXt"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ChvElLf5Avc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deKb0IvS5KXE",
        "outputId": "62284afa-8ab3-4779-c3ae-a94b1ac434df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "# Let's check our Tensorflow version\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf6QZLqt6u4h"
      },
      "source": [
        "## Data importing and feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's import our train and test datasets. As a reminder, they are already separated from each other to prevent data contamination during the sliding window procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lMqnyDmu61WE"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_parquet(\"./DataProcessing/5_datasets/TRAIN_dataset_v2.parquet\")\n",
        "df_test = pd.read_parquet(\"./DataProcessing/5_datasets/TEST_dataset_v2.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>...</th>\n",
              "      <th>100x43</th>\n",
              "      <th>100x44</th>\n",
              "      <th>100x45</th>\n",
              "      <th>100x46</th>\n",
              "      <th>100x47</th>\n",
              "      <th>100x48</th>\n",
              "      <th>100x49</th>\n",
              "      <th>100x50</th>\n",
              "      <th>100x51</th>\n",
              "      <th>100x52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-51.705446</td>\n",
              "      <td>-51.423908</td>\n",
              "      <td>-51.496811</td>\n",
              "      <td>-51.510120</td>\n",
              "      <td>-51.557132</td>\n",
              "      <td>-51.535879</td>\n",
              "      <td>-51.565779</td>\n",
              "      <td>-51.570532</td>\n",
              "      <td>-51.630636</td>\n",
              "      <td>...</td>\n",
              "      <td>-56.989539</td>\n",
              "      <td>-56.998313</td>\n",
              "      <td>-56.958502</td>\n",
              "      <td>-56.939802</td>\n",
              "      <td>-56.921101</td>\n",
              "      <td>-56.871816</td>\n",
              "      <td>-56.838084</td>\n",
              "      <td>-56.855117</td>\n",
              "      <td>-56.872150</td>\n",
              "      <td>-56.863376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-53.126589</td>\n",
              "      <td>-52.897802</td>\n",
              "      <td>-52.951718</td>\n",
              "      <td>-52.957503</td>\n",
              "      <td>-52.940708</td>\n",
              "      <td>-52.973299</td>\n",
              "      <td>-52.996578</td>\n",
              "      <td>-53.063365</td>\n",
              "      <td>-53.097971</td>\n",
              "      <td>...</td>\n",
              "      <td>-56.826967</td>\n",
              "      <td>-56.888419</td>\n",
              "      <td>-56.874295</td>\n",
              "      <td>-56.882567</td>\n",
              "      <td>-56.863333</td>\n",
              "      <td>-56.844099</td>\n",
              "      <td>-56.858878</td>\n",
              "      <td>-56.841845</td>\n",
              "      <td>-56.856624</td>\n",
              "      <td>-56.871403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-52.180428</td>\n",
              "      <td>-52.346506</td>\n",
              "      <td>-52.346735</td>\n",
              "      <td>-52.429096</td>\n",
              "      <td>-52.440990</td>\n",
              "      <td>-52.502765</td>\n",
              "      <td>-52.546891</td>\n",
              "      <td>-52.593040</td>\n",
              "      <td>-52.628934</td>\n",
              "      <td>...</td>\n",
              "      <td>-55.518491</td>\n",
              "      <td>-55.544820</td>\n",
              "      <td>-55.522918</td>\n",
              "      <td>-55.550747</td>\n",
              "      <td>-55.578576</td>\n",
              "      <td>-55.548514</td>\n",
              "      <td>-55.466558</td>\n",
              "      <td>-55.436496</td>\n",
              "      <td>-55.454657</td>\n",
              "      <td>-55.431944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-52.812095</td>\n",
              "      <td>-52.877571</td>\n",
              "      <td>-52.899397</td>\n",
              "      <td>-52.964835</td>\n",
              "      <td>-52.927434</td>\n",
              "      <td>-52.966386</td>\n",
              "      <td>-52.994210</td>\n",
              "      <td>-53.025664</td>\n",
              "      <td>-53.050129</td>\n",
              "      <td>...</td>\n",
              "      <td>-56.101104</td>\n",
              "      <td>-56.097376</td>\n",
              "      <td>-56.039618</td>\n",
              "      <td>-55.969037</td>\n",
              "      <td>-55.953594</td>\n",
              "      <td>-55.941600</td>\n",
              "      <td>-55.929607</td>\n",
              "      <td>-55.846010</td>\n",
              "      <td>-55.818094</td>\n",
              "      <td>-55.777377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-52.203216</td>\n",
              "      <td>-51.991086</td>\n",
              "      <td>-52.103481</td>\n",
              "      <td>-52.072961</td>\n",
              "      <td>-52.093341</td>\n",
              "      <td>-52.076188</td>\n",
              "      <td>-52.140729</td>\n",
              "      <td>-52.189136</td>\n",
              "      <td>-52.184420</td>\n",
              "      <td>...</td>\n",
              "      <td>-56.145657</td>\n",
              "      <td>-56.176241</td>\n",
              "      <td>-56.153277</td>\n",
              "      <td>-56.179606</td>\n",
              "      <td>-56.156643</td>\n",
              "      <td>-56.187227</td>\n",
              "      <td>-56.154775</td>\n",
              "      <td>-56.168080</td>\n",
              "      <td>-56.205064</td>\n",
              "      <td>-56.191338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-72.377810</td>\n",
              "      <td>-72.377810</td>\n",
              "      <td>-72.517505</td>\n",
              "      <td>-72.521948</td>\n",
              "      <td>-72.524614</td>\n",
              "      <td>-72.481069</td>\n",
              "      <td>-72.546801</td>\n",
              "      <td>-72.558891</td>\n",
              "      <td>-72.568294</td>\n",
              "      <td>...</td>\n",
              "      <td>-73.800800</td>\n",
              "      <td>-73.821193</td>\n",
              "      <td>-73.892296</td>\n",
              "      <td>-73.912689</td>\n",
              "      <td>-73.983791</td>\n",
              "      <td>-74.004185</td>\n",
              "      <td>-74.037290</td>\n",
              "      <td>-74.070395</td>\n",
              "      <td>-74.103500</td>\n",
              "      <td>-74.083466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-72.672382</td>\n",
              "      <td>-72.672382</td>\n",
              "      <td>-72.672382</td>\n",
              "      <td>-72.647173</td>\n",
              "      <td>-72.731509</td>\n",
              "      <td>-72.704849</td>\n",
              "      <td>-72.736653</td>\n",
              "      <td>-72.739661</td>\n",
              "      <td>-72.760530</td>\n",
              "      <td>...</td>\n",
              "      <td>-74.016193</td>\n",
              "      <td>-74.038970</td>\n",
              "      <td>-74.062372</td>\n",
              "      <td>-74.104192</td>\n",
              "      <td>-74.102771</td>\n",
              "      <td>-74.054800</td>\n",
              "      <td>-74.006829</td>\n",
              "      <td>-73.958858</td>\n",
              "      <td>-73.910887</td>\n",
              "      <td>-73.886064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-73.016966</td>\n",
              "      <td>-72.811591</td>\n",
              "      <td>-72.959830</td>\n",
              "      <td>-72.982882</td>\n",
              "      <td>-73.077856</td>\n",
              "      <td>-73.073553</td>\n",
              "      <td>-73.094410</td>\n",
              "      <td>-73.061929</td>\n",
              "      <td>-73.079443</td>\n",
              "      <td>...</td>\n",
              "      <td>-72.806474</td>\n",
              "      <td>-72.828655</td>\n",
              "      <td>-72.916206</td>\n",
              "      <td>-72.990956</td>\n",
              "      <td>-73.006063</td>\n",
              "      <td>-72.963709</td>\n",
              "      <td>-72.978816</td>\n",
              "      <td>-73.055776</td>\n",
              "      <td>-73.015347</td>\n",
              "      <td>-73.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-73.954359</td>\n",
              "      <td>-73.851209</td>\n",
              "      <td>-74.123166</td>\n",
              "      <td>-74.110712</td>\n",
              "      <td>-74.103240</td>\n",
              "      <td>-74.098258</td>\n",
              "      <td>-74.094700</td>\n",
              "      <td>-74.092032</td>\n",
              "      <td>-74.117538</td>\n",
              "      <td>...</td>\n",
              "      <td>-75.004124</td>\n",
              "      <td>-75.035671</td>\n",
              "      <td>-75.003684</td>\n",
              "      <td>-74.963917</td>\n",
              "      <td>-74.955697</td>\n",
              "      <td>-74.947476</td>\n",
              "      <td>-74.939256</td>\n",
              "      <td>-74.899489</td>\n",
              "      <td>-74.859722</td>\n",
              "      <td>-74.819955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-72.970411</td>\n",
              "      <td>-73.149691</td>\n",
              "      <td>-73.209451</td>\n",
              "      <td>-73.133425</td>\n",
              "      <td>-73.151434</td>\n",
              "      <td>-73.163440</td>\n",
              "      <td>-73.172016</td>\n",
              "      <td>-73.178447</td>\n",
              "      <td>-73.217432</td>\n",
              "      <td>...</td>\n",
              "      <td>-75.211946</td>\n",
              "      <td>-75.220780</td>\n",
              "      <td>-75.229613</td>\n",
              "      <td>-75.226431</td>\n",
              "      <td>-75.285610</td>\n",
              "      <td>-75.344789</td>\n",
              "      <td>-75.372422</td>\n",
              "      <td>-75.454378</td>\n",
              "      <td>-75.414567</td>\n",
              "      <td>-75.464977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>266 rows × 5201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label        1x1        1x2        1x3        1x4        1x5        1x6  \\\n",
              "0      1.0 -51.705446 -51.423908 -51.496811 -51.510120 -51.557132 -51.535879   \n",
              "1      1.0 -53.126589 -52.897802 -52.951718 -52.957503 -52.940708 -52.973299   \n",
              "2      1.0 -52.180428 -52.346506 -52.346735 -52.429096 -52.440990 -52.502765   \n",
              "3      1.0 -52.812095 -52.877571 -52.899397 -52.964835 -52.927434 -52.966386   \n",
              "4      1.0 -52.203216 -51.991086 -52.103481 -52.072961 -52.093341 -52.076188   \n",
              "..     ...        ...        ...        ...        ...        ...        ...   \n",
              "261    0.0 -72.377810 -72.377810 -72.517505 -72.521948 -72.524614 -72.481069   \n",
              "262    0.0 -72.672382 -72.672382 -72.672382 -72.647173 -72.731509 -72.704849   \n",
              "263    0.0 -73.016966 -72.811591 -72.959830 -72.982882 -73.077856 -73.073553   \n",
              "264    0.0 -73.954359 -73.851209 -74.123166 -74.110712 -74.103240 -74.098258   \n",
              "265    0.0 -72.970411 -73.149691 -73.209451 -73.133425 -73.151434 -73.163440   \n",
              "\n",
              "           1x7        1x8        1x9  ...     100x43     100x44     100x45  \\\n",
              "0   -51.565779 -51.570532 -51.630636  ... -56.989539 -56.998313 -56.958502   \n",
              "1   -52.996578 -53.063365 -53.097971  ... -56.826967 -56.888419 -56.874295   \n",
              "2   -52.546891 -52.593040 -52.628934  ... -55.518491 -55.544820 -55.522918   \n",
              "3   -52.994210 -53.025664 -53.050129  ... -56.101104 -56.097376 -56.039618   \n",
              "4   -52.140729 -52.189136 -52.184420  ... -56.145657 -56.176241 -56.153277   \n",
              "..         ...        ...        ...  ...        ...        ...        ...   \n",
              "261 -72.546801 -72.558891 -72.568294  ... -73.800800 -73.821193 -73.892296   \n",
              "262 -72.736653 -72.739661 -72.760530  ... -74.016193 -74.038970 -74.062372   \n",
              "263 -73.094410 -73.061929 -73.079443  ... -72.806474 -72.828655 -72.916206   \n",
              "264 -74.094700 -74.092032 -74.117538  ... -75.004124 -75.035671 -75.003684   \n",
              "265 -73.172016 -73.178447 -73.217432  ... -75.211946 -75.220780 -75.229613   \n",
              "\n",
              "        100x46     100x47     100x48     100x49     100x50     100x51  \\\n",
              "0   -56.939802 -56.921101 -56.871816 -56.838084 -56.855117 -56.872150   \n",
              "1   -56.882567 -56.863333 -56.844099 -56.858878 -56.841845 -56.856624   \n",
              "2   -55.550747 -55.578576 -55.548514 -55.466558 -55.436496 -55.454657   \n",
              "3   -55.969037 -55.953594 -55.941600 -55.929607 -55.846010 -55.818094   \n",
              "4   -56.179606 -56.156643 -56.187227 -56.154775 -56.168080 -56.205064   \n",
              "..         ...        ...        ...        ...        ...        ...   \n",
              "261 -73.912689 -73.983791 -74.004185 -74.037290 -74.070395 -74.103500   \n",
              "262 -74.104192 -74.102771 -74.054800 -74.006829 -73.958858 -73.910887   \n",
              "263 -72.990956 -73.006063 -72.963709 -72.978816 -73.055776 -73.015347   \n",
              "264 -74.963917 -74.955697 -74.947476 -74.939256 -74.899489 -74.859722   \n",
              "265 -75.226431 -75.285610 -75.344789 -75.372422 -75.454378 -75.414567   \n",
              "\n",
              "        100x52  \n",
              "0   -56.863376  \n",
              "1   -56.871403  \n",
              "2   -55.431944  \n",
              "3   -55.777377  \n",
              "4   -56.191338  \n",
              "..         ...  \n",
              "261 -74.083466  \n",
              "262 -73.886064  \n",
              "263 -73.023000  \n",
              "264 -74.819955  \n",
              "265 -75.464977  \n",
              "\n",
              "[266 rows x 5201 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's have a look at how our data looks like!\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are two labels: ```0``` represents ambient background noise, and ```1``` represents a person walking between the ESP32 boards.\n",
        "\n",
        "For each dataset sample (row) there are 5200 features, denoting the CSI amplitudes of the 52 Wi-Fi subcarriers during a 100-frame window (a 1-second period)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separating our features from their labels\n",
        "X_train = df_train.copy()\n",
        "y_train = X_train.pop('label')\n",
        "X_test = df_test.copy()\n",
        "y_test = X_test.pop('label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WuScOsrt5HST"
      },
      "outputs": [],
      "source": [
        "# Reshaping our labels for the required input dimensions\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those 5200 features are a rather big input for our model, so now let's reduce them to a more manageable size using Principal Component Analisys (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will scale the features using the StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7sDTEGe59aA",
        "outputId": "f6937cc2-9917-4a56-8c1b-ad5f5e4ed70d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-62.856 -62.837 -62.866 ... -64.028 -64.    -63.992]\n",
            "[9.659 9.645 9.667 ... 8.251 8.247 8.25 ]\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "print(scaler.mean_)\n",
        "print(scaler.scale_)\n",
        "\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can train and apply our PCA! In this case, we will use a PCA that explains 99% of the data variance. Lower % values will reduce the number of features even more, but potentially result in lower accuracy as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.014  0.014  0.014 ...  0.014  0.014  0.014]\n",
            " [-0.002 -0.002 -0.002 ... -0.001 -0.001 -0.001]\n",
            " [-0.005 -0.005 -0.005 ... -0.002 -0.002 -0.002]\n",
            " ...\n",
            " [-0.009 -0.01  -0.01  ... -0.027 -0.027 -0.027]\n",
            " [-0.005 -0.005 -0.005 ... -0.002 -0.002 -0.002]\n",
            " [ 0.025  0.027  0.026 ...  0.019  0.019  0.019]]\n",
            "(42, 5200)\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=0.99)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "print(pca.components_)\n",
        "print(pca.components_.shape)\n",
        "\n",
        "input_shape = (X_train.shape[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what's our feature size now, after the PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(42,)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From 5200 features per sample to only 42! That's a substantial size reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can export our trained PCA and Scaler for later use in the ESP32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exporting PCA and Scaler for the ESP32\n",
        "\n",
        "n_features = scaler.mean_.size\n",
        "n_frame = n_features // 52\n",
        "n_components = pca.components_.shape[0]\n",
        "\n",
        "# Reshape the parameters\n",
        "scaler_mean = scaler.mean_.reshape(n_frame, 52)\n",
        "scaler_std = scaler.scale_.reshape(n_frame, 52)\n",
        "pca_mean = pca.mean_.reshape(n_frame, 52)\n",
        "pca_components = pca.components_\n",
        "\n",
        "with open('preprocessing.h', 'w') as f:\n",
        "    f.write('#pragma once\\n\\n')\n",
        "\n",
        "    f.write('#define SUBCARRIER_COUNT 52\\n')\n",
        "    f.write(f'#define FRAME_WINDOW_SIZE {n_frame}\\n')\n",
        "    f.write(f'#define PCA_COMPONENTS {n_components}\\n\\n')\n",
        "    \n",
        "    # Define the scaler struct\n",
        "    f.write('typedef struct {\\n')\n",
        "    f.write('  float mean;\\n')\n",
        "    f.write('  float std;\\n')\n",
        "    f.write('} standard_scaler;\\n\\n')\n",
        "\n",
        "    f.write(f'extern const standard_scaler scaler[FRAME_WINDOW_SIZE][SUBCARRIER_COUNT];\\n')\n",
        "    f.write(f'extern const float pca_means[FRAME_WINDOW_SIZE][SUBCARRIER_COUNT];\\n')\n",
        "    f.write(f'extern const float pca_matrix[PCA_COMPONENTS][SUBCARRIER_COUNT * FRAME_WINDOW_SIZE];\\n')\n",
        "\n",
        "with open('preprocessing.c', 'w') as f:\n",
        "    f.write('#include \"preprocessing.h\"\\n\\n')\n",
        "\n",
        "    # Dump scaler params\n",
        "    f.write('const standard_scaler scaler[FRAME_WINDOW_SIZE][SUBCARRIER_COUNT] = {\\n')\n",
        "    for i in range(n_frame):\n",
        "        f.write('    {')\n",
        "        for j in range(52):\n",
        "            mean_val = scaler_mean[i, j]\n",
        "            std_val = scaler_std[i, j]\n",
        "            entry = '' if j == 0 else ' '\n",
        "            entry += f'{{ .mean = {mean_val:.6f}f, .std = {std_val:.6f}f }}'\n",
        "            entry += ',' if j < 51 else ''\n",
        "            f.write(entry)\n",
        "        frame_end = '},\\n' if i < n_frame - 1 else '}\\n'\n",
        "        f.write(frame_end)\n",
        "    f.write('};\\n\\n')\n",
        "\n",
        "    # Dump PCA means\n",
        "    f.write('const float pca_means[FRAME_WINDOW_SIZE][SUBCARRIER_COUNT] = {\\n')\n",
        "    for i in range(n_frame):\n",
        "        entries = '{'\n",
        "        entries += ', '.join(f'{val:.6f}f' for val in pca_mean[i])\n",
        "        row_end = '},\\n' if i < n_frame - 1 else '}\\n'\n",
        "        f.write(f'    {entries}{row_end}')\n",
        "    f.write('};\\n\\n')\n",
        "    \n",
        "    # Dump PCA matrix\n",
        "    f.write('const float pca_matrix[PCA_COMPONENTS][SUBCARRIER_COUNT * FRAME_WINDOW_SIZE] = {\\n')\n",
        "    for idx, comp in enumerate(pca_components):\n",
        "        entries = '{'\n",
        "        entries += ', '.join(f'{val:.6f}f' for val in comp)\n",
        "        row_end = '},\\n' if idx < n_components - 1 else '}\\n'\n",
        "        f.write(f'    {entries}{row_end}')\n",
        "    f.write('};\\n')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PCA and Scaler are now saved in the ```preprocessing.c``` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have finished our feature engineering, we can finally train our model! Let's use a simple Multilayer Perceptron (MLP), to ensure the best compatibility possible with the ESP32. You could also try to use more complex models, but be sure to check for the [operation compatibility with the ESP-TFLITE-MICRO framework](https://github.com/espressif/esp-tflite-micro/blob/78e5532e682d3863a3c2985c3a74eed0a9ebaa61/tensorflow/lite/micro/micro_mutable_op_resolver.h#L45)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We will use an input layer with a number of neurons equal to the number of features, then add hidden layers dividing the number of neurons for 2 until we have our output.\n",
        "\n",
        "We will also add Dropout layers in between each fully connected layer to reduce the chance of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1UlDFhMYkwKw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\samuk\\anaconda3\\envs\\sbrc\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(42, activation='relu',  input_shape=input_shape),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(32, activation='relu',  input_shape=input_shape),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(16, activation='relu',  input_shape=input_shape),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VQG1h1-v-hc",
        "outputId": "15f58b48-d689-412d-aec1-d37bf4d55c7d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,806</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,376</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)             │         \u001b[38;5;34m1,806\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,376\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m10\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,895</span> (15.21 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,895\u001b[0m (15.21 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,895</span> (15.21 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,895\u001b[0m (15.21 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we train the model for 200 epochs, and save the model with the lowest train loss (which is correlated to the highest accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI9XG8KF3TVF",
        "outputId": "82e93386-ba4c-4121-d67d-1a1bd02a5be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\n",
            "Epoch 1: loss improved from inf to 0.74668, saving model to best_model.keras\n",
            "31/31 - 3s - 83ms/step - binary_accuracy: 0.4810 - loss: 0.7467 - val_binary_accuracy: 0.5188 - val_loss: 0.6921\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 2: loss improved from 0.74668 to 0.69788, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.5118 - loss: 0.6979 - val_binary_accuracy: 0.6128 - val_loss: 0.6914\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 3: loss improved from 0.69788 to 0.69650, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.5293 - loss: 0.6965 - val_binary_accuracy: 0.5940 - val_loss: 0.6889\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 4: loss improved from 0.69650 to 0.69034, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.5365 - loss: 0.6903 - val_binary_accuracy: 0.5564 - val_loss: 0.6872\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 5: loss improved from 0.69034 to 0.68833, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.5334 - loss: 0.6883 - val_binary_accuracy: 0.6353 - val_loss: 0.6839\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 6: loss did not improve from 0.68833\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.5385 - loss: 0.6895 - val_binary_accuracy: 0.5564 - val_loss: 0.6825\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 7: loss improved from 0.68833 to 0.68576, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.5447 - loss: 0.6858 - val_binary_accuracy: 0.7707 - val_loss: 0.6754\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 8: loss improved from 0.68576 to 0.67779, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.6053 - loss: 0.6778 - val_binary_accuracy: 0.7030 - val_loss: 0.6563\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 9: loss improved from 0.67779 to 0.66926, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.5940 - loss: 0.6693 - val_binary_accuracy: 0.7820 - val_loss: 0.6290\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 10: loss improved from 0.66926 to 0.64324, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.6680 - loss: 0.6432 - val_binary_accuracy: 0.7556 - val_loss: 0.5968\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 11: loss improved from 0.64324 to 0.61785, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.6968 - loss: 0.6178 - val_binary_accuracy: 0.8158 - val_loss: 0.5472\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 12: loss improved from 0.61785 to 0.57168, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.7626 - loss: 0.5717 - val_binary_accuracy: 0.9023 - val_loss: 0.5055\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 13: loss improved from 0.57168 to 0.53298, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8058 - loss: 0.5330 - val_binary_accuracy: 0.9060 - val_loss: 0.4520\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 14: loss improved from 0.53298 to 0.50388, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8212 - loss: 0.5039 - val_binary_accuracy: 0.9135 - val_loss: 0.4265\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 15: loss improved from 0.50388 to 0.48648, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8366 - loss: 0.4865 - val_binary_accuracy: 0.9474 - val_loss: 0.3941\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 16: loss improved from 0.48648 to 0.48252, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8232 - loss: 0.4825 - val_binary_accuracy: 0.9699 - val_loss: 0.3800\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 17: loss improved from 0.48252 to 0.44942, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8469 - loss: 0.4494 - val_binary_accuracy: 0.9624 - val_loss: 0.3564\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 18: loss did not improve from 0.44942\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.8325 - loss: 0.4525 - val_binary_accuracy: 0.9662 - val_loss: 0.3485\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 19: loss improved from 0.44942 to 0.43294, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8592 - loss: 0.4329 - val_binary_accuracy: 0.9624 - val_loss: 0.3558\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 20: loss improved from 0.43294 to 0.41756, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8633 - loss: 0.4176 - val_binary_accuracy: 0.9737 - val_loss: 0.3234\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 21: loss improved from 0.41756 to 0.41021, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.8705 - loss: 0.4102 - val_binary_accuracy: 0.9699 - val_loss: 0.3078\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 22: loss improved from 0.41021 to 0.38590, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.8890 - loss: 0.3859 - val_binary_accuracy: 0.9737 - val_loss: 0.3021\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 23: loss improved from 0.38590 to 0.38149, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.8859 - loss: 0.3815 - val_binary_accuracy: 0.9774 - val_loss: 0.2955\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 24: loss improved from 0.38149 to 0.35246, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9106 - loss: 0.3525 - val_binary_accuracy: 0.9737 - val_loss: 0.2845\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 25: loss improved from 0.35246 to 0.33433, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9239 - loss: 0.3343 - val_binary_accuracy: 0.9737 - val_loss: 0.2677\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 26: loss improved from 0.33433 to 0.30329, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9496 - loss: 0.3033 - val_binary_accuracy: 0.9812 - val_loss: 0.2540\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 27: loss improved from 0.30329 to 0.29277, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9486 - loss: 0.2928 - val_binary_accuracy: 0.9812 - val_loss: 0.2457\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 28: loss improved from 0.29277 to 0.28437, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9630 - loss: 0.2844 - val_binary_accuracy: 0.9887 - val_loss: 0.2341\n",
            "Epoch 29/200\n",
            "\n",
            "Epoch 29: loss improved from 0.28437 to 0.27114, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9630 - loss: 0.2711 - val_binary_accuracy: 0.9737 - val_loss: 0.2426\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 30: loss did not improve from 0.27114\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9651 - loss: 0.2714 - val_binary_accuracy: 0.9887 - val_loss: 0.2208\n",
            "Epoch 31/200\n",
            "\n",
            "Epoch 31: loss did not improve from 0.27114\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9435 - loss: 0.2763 - val_binary_accuracy: 0.9887 - val_loss: 0.2184\n",
            "Epoch 32/200\n",
            "\n",
            "Epoch 32: loss improved from 0.27114 to 0.24166, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9671 - loss: 0.2417 - val_binary_accuracy: 0.9887 - val_loss: 0.2145\n",
            "Epoch 33/200\n",
            "\n",
            "Epoch 33: loss did not improve from 0.24166\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9620 - loss: 0.2487 - val_binary_accuracy: 0.9925 - val_loss: 0.1982\n",
            "Epoch 34/200\n",
            "\n",
            "Epoch 34: loss did not improve from 0.24166\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9538 - loss: 0.2748 - val_binary_accuracy: 0.9850 - val_loss: 0.1999\n",
            "Epoch 35/200\n",
            "\n",
            "Epoch 35: loss improved from 0.24166 to 0.22811, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9671 - loss: 0.2281 - val_binary_accuracy: 0.9850 - val_loss: 0.1996\n",
            "Epoch 36/200\n",
            "\n",
            "Epoch 36: loss did not improve from 0.22811\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9651 - loss: 0.2302 - val_binary_accuracy: 0.9812 - val_loss: 0.2067\n",
            "Epoch 37/200\n",
            "\n",
            "Epoch 37: loss did not improve from 0.22811\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9599 - loss: 0.2299 - val_binary_accuracy: 0.9962 - val_loss: 0.1792\n",
            "Epoch 38/200\n",
            "\n",
            "Epoch 38: loss improved from 0.22811 to 0.21276, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9743 - loss: 0.2128 - val_binary_accuracy: 0.9962 - val_loss: 0.1728\n",
            "Epoch 39/200\n",
            "\n",
            "Epoch 39: loss improved from 0.21276 to 0.20119, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9753 - loss: 0.2012 - val_binary_accuracy: 0.9925 - val_loss: 0.1705\n",
            "Epoch 40/200\n",
            "\n",
            "Epoch 40: loss improved from 0.20119 to 0.19452, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9774 - loss: 0.1945 - val_binary_accuracy: 0.9962 - val_loss: 0.1660\n",
            "Epoch 41/200\n",
            "\n",
            "Epoch 41: loss improved from 0.19452 to 0.19325, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9733 - loss: 0.1932 - val_binary_accuracy: 0.9887 - val_loss: 0.1645\n",
            "Epoch 42/200\n",
            "\n",
            "Epoch 42: loss improved from 0.19325 to 0.18254, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9815 - loss: 0.1825 - val_binary_accuracy: 0.9850 - val_loss: 0.1628\n",
            "Epoch 43/200\n",
            "\n",
            "Epoch 43: loss did not improve from 0.18254\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9764 - loss: 0.1964 - val_binary_accuracy: 0.9887 - val_loss: 0.1645\n",
            "Epoch 44/200\n",
            "\n",
            "Epoch 44: loss improved from 0.18254 to 0.16882, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9866 - loss: 0.1688 - val_binary_accuracy: 0.9925 - val_loss: 0.1529\n",
            "Epoch 45/200\n",
            "\n",
            "Epoch 45: loss did not improve from 0.16882\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9794 - loss: 0.1714 - val_binary_accuracy: 0.9887 - val_loss: 0.1566\n",
            "Epoch 46/200\n",
            "\n",
            "Epoch 46: loss did not improve from 0.16882\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9805 - loss: 0.1752 - val_binary_accuracy: 0.9887 - val_loss: 0.1581\n",
            "Epoch 47/200\n",
            "\n",
            "Epoch 47: loss improved from 0.16882 to 0.15866, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9846 - loss: 0.1587 - val_binary_accuracy: 0.9850 - val_loss: 0.1654\n",
            "Epoch 48/200\n",
            "\n",
            "Epoch 48: loss improved from 0.15866 to 0.15016, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9866 - loss: 0.1502 - val_binary_accuracy: 0.9887 - val_loss: 0.1738\n",
            "Epoch 49/200\n",
            "\n",
            "Epoch 49: loss did not improve from 0.15016\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9753 - loss: 0.1764 - val_binary_accuracy: 0.9887 - val_loss: 0.1506\n",
            "Epoch 50/200\n",
            "\n",
            "Epoch 50: loss did not improve from 0.15016\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9825 - loss: 0.1533 - val_binary_accuracy: 0.9887 - val_loss: 0.1515\n",
            "Epoch 51/200\n",
            "\n",
            "Epoch 51: loss did not improve from 0.15016\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9764 - loss: 0.2019 - val_binary_accuracy: 0.9887 - val_loss: 0.1423\n",
            "Epoch 52/200\n",
            "\n",
            "Epoch 52: loss did not improve from 0.15016\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9723 - loss: 0.1697 - val_binary_accuracy: 0.9925 - val_loss: 0.1326\n",
            "Epoch 53/200\n",
            "\n",
            "Epoch 53: loss did not improve from 0.15016\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9702 - loss: 0.1729 - val_binary_accuracy: 0.9850 - val_loss: 0.1414\n",
            "Epoch 54/200\n",
            "\n",
            "Epoch 54: loss improved from 0.15016 to 0.14226, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9836 - loss: 0.1423 - val_binary_accuracy: 0.9887 - val_loss: 0.1422\n",
            "Epoch 55/200\n",
            "\n",
            "Epoch 55: loss did not improve from 0.14226\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9784 - loss: 0.1776 - val_binary_accuracy: 0.9925 - val_loss: 0.1325\n",
            "Epoch 56/200\n",
            "\n",
            "Epoch 56: loss did not improve from 0.14226\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9712 - loss: 0.1847 - val_binary_accuracy: 0.9887 - val_loss: 0.1292\n",
            "Epoch 57/200\n",
            "\n",
            "Epoch 57: loss did not improve from 0.14226\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9805 - loss: 0.1542 - val_binary_accuracy: 0.9887 - val_loss: 0.1253\n",
            "Epoch 58/200\n",
            "\n",
            "Epoch 58: loss did not improve from 0.14226\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9774 - loss: 0.1520 - val_binary_accuracy: 0.9887 - val_loss: 0.1248\n",
            "Epoch 59/200\n",
            "\n",
            "Epoch 59: loss improved from 0.14226 to 0.13833, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9887 - loss: 0.1383 - val_binary_accuracy: 0.9887 - val_loss: 0.1292\n",
            "Epoch 60/200\n",
            "\n",
            "Epoch 60: loss improved from 0.13833 to 0.13040, saving model to best_model.keras\n",
            "31/31 - 0s - 6ms/step - binary_accuracy: 0.9877 - loss: 0.1304 - val_binary_accuracy: 0.9850 - val_loss: 0.1260\n",
            "Epoch 61/200\n",
            "\n",
            "Epoch 61: loss did not improve from 0.13040\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9794 - loss: 0.1408 - val_binary_accuracy: 0.9925 - val_loss: 0.1152\n",
            "Epoch 62/200\n",
            "\n",
            "Epoch 62: loss did not improve from 0.13040\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9805 - loss: 0.1361 - val_binary_accuracy: 0.9925 - val_loss: 0.1190\n",
            "Epoch 63/200\n",
            "\n",
            "Epoch 63: loss did not improve from 0.13040\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9794 - loss: 0.1379 - val_binary_accuracy: 0.9962 - val_loss: 0.1058\n",
            "Epoch 64/200\n",
            "\n",
            "Epoch 64: loss did not improve from 0.13040\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.1367 - val_binary_accuracy: 0.9962 - val_loss: 0.1044\n",
            "Epoch 65/200\n",
            "\n",
            "Epoch 65: loss improved from 0.13040 to 0.11393, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9897 - loss: 0.1139 - val_binary_accuracy: 0.9962 - val_loss: 0.1020\n",
            "Epoch 66/200\n",
            "\n",
            "Epoch 66: loss did not improve from 0.11393\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.1205 - val_binary_accuracy: 0.9962 - val_loss: 0.1021\n",
            "Epoch 67/200\n",
            "\n",
            "Epoch 67: loss did not improve from 0.11393\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9836 - loss: 0.1342 - val_binary_accuracy: 0.9925 - val_loss: 0.1071\n",
            "Epoch 68/200\n",
            "\n",
            "Epoch 68: loss did not improve from 0.11393\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9815 - loss: 0.1221 - val_binary_accuracy: 0.9925 - val_loss: 0.1114\n",
            "Epoch 69/200\n",
            "\n",
            "Epoch 69: loss did not improve from 0.11393\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9784 - loss: 0.1333 - val_binary_accuracy: 0.9925 - val_loss: 0.1014\n",
            "Epoch 70/200\n",
            "\n",
            "Epoch 70: loss improved from 0.11393 to 0.10839, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9897 - loss: 0.1084 - val_binary_accuracy: 0.9887 - val_loss: 0.1223\n",
            "Epoch 71/200\n",
            "\n",
            "Epoch 71: loss did not improve from 0.10839\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9712 - loss: 0.1476 - val_binary_accuracy: 0.9887 - val_loss: 0.1065\n",
            "Epoch 72/200\n",
            "\n",
            "Epoch 72: loss did not improve from 0.10839\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9815 - loss: 0.1192 - val_binary_accuracy: 0.9887 - val_loss: 0.0998\n",
            "Epoch 73/200\n",
            "\n",
            "Epoch 73: loss did not improve from 0.10839\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.1113 - val_binary_accuracy: 0.9962 - val_loss: 0.0970\n",
            "Epoch 74/200\n",
            "\n",
            "Epoch 74: loss improved from 0.10839 to 0.10400, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9877 - loss: 0.1040 - val_binary_accuracy: 0.9962 - val_loss: 0.1001\n",
            "Epoch 75/200\n",
            "\n",
            "Epoch 75: loss improved from 0.10400 to 0.10272, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9877 - loss: 0.1027 - val_binary_accuracy: 0.9962 - val_loss: 0.0878\n",
            "Epoch 76/200\n",
            "\n",
            "Epoch 76: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.1090 - val_binary_accuracy: 0.9925 - val_loss: 0.0897\n",
            "Epoch 77/200\n",
            "\n",
            "Epoch 77: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.1244 - val_binary_accuracy: 0.9962 - val_loss: 0.0894\n",
            "Epoch 78/200\n",
            "\n",
            "Epoch 78: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9805 - loss: 0.1140 - val_binary_accuracy: 0.9925 - val_loss: 0.0914\n",
            "Epoch 79/200\n",
            "\n",
            "Epoch 79: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9815 - loss: 0.1129 - val_binary_accuracy: 1.0000 - val_loss: 0.0756\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 80: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9846 - loss: 0.1046 - val_binary_accuracy: 0.9887 - val_loss: 0.0951\n",
            "Epoch 81/200\n",
            "\n",
            "Epoch 81: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9836 - loss: 0.1055 - val_binary_accuracy: 1.0000 - val_loss: 0.0741\n",
            "Epoch 82/200\n",
            "\n",
            "Epoch 82: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9846 - loss: 0.1064 - val_binary_accuracy: 0.9925 - val_loss: 0.0856\n",
            "Epoch 83/200\n",
            "\n",
            "Epoch 83: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9774 - loss: 0.1251 - val_binary_accuracy: 0.9850 - val_loss: 0.0910\n",
            "Epoch 84/200\n",
            "\n",
            "Epoch 84: loss did not improve from 0.10272\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9846 - loss: 0.1033 - val_binary_accuracy: 0.9925 - val_loss: 0.0792\n",
            "Epoch 85/200\n",
            "\n",
            "Epoch 85: loss improved from 0.10272 to 0.09720, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9856 - loss: 0.0972 - val_binary_accuracy: 0.9925 - val_loss: 0.0927\n",
            "Epoch 86/200\n",
            "\n",
            "Epoch 86: loss improved from 0.09720 to 0.09049, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9877 - loss: 0.0905 - val_binary_accuracy: 0.9925 - val_loss: 0.0919\n",
            "Epoch 87/200\n",
            "\n",
            "Epoch 87: loss did not improve from 0.09049\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0919 - val_binary_accuracy: 0.9925 - val_loss: 0.0900\n",
            "Epoch 88/200\n",
            "\n",
            "Epoch 88: loss improved from 0.09049 to 0.08298, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9897 - loss: 0.0830 - val_binary_accuracy: 0.9962 - val_loss: 0.0685\n",
            "Epoch 89/200\n",
            "\n",
            "Epoch 89: loss did not improve from 0.08298\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0891 - val_binary_accuracy: 0.9962 - val_loss: 0.0772\n",
            "Epoch 90/200\n",
            "\n",
            "Epoch 90: loss did not improve from 0.08298\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0882 - val_binary_accuracy: 0.9962 - val_loss: 0.0813\n",
            "Epoch 91/200\n",
            "\n",
            "Epoch 91: loss improved from 0.08298 to 0.08100, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9908 - loss: 0.0810 - val_binary_accuracy: 0.9962 - val_loss: 0.0741\n",
            "Epoch 92/200\n",
            "\n",
            "Epoch 92: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.1236 - val_binary_accuracy: 0.9850 - val_loss: 0.0850\n",
            "Epoch 93/200\n",
            "\n",
            "Epoch 93: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9815 - loss: 0.0984 - val_binary_accuracy: 0.9925 - val_loss: 0.0812\n",
            "Epoch 94/200\n",
            "\n",
            "Epoch 94: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9774 - loss: 0.1131 - val_binary_accuracy: 0.9887 - val_loss: 0.0908\n",
            "Epoch 95/200\n",
            "\n",
            "Epoch 95: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9836 - loss: 0.0960 - val_binary_accuracy: 0.9962 - val_loss: 0.0978\n",
            "Epoch 96/200\n",
            "\n",
            "Epoch 96: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0899 - val_binary_accuracy: 0.9925 - val_loss: 0.0771\n",
            "Epoch 97/200\n",
            "\n",
            "Epoch 97: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9825 - loss: 0.1034 - val_binary_accuracy: 0.9925 - val_loss: 0.1025\n",
            "Epoch 98/200\n",
            "\n",
            "Epoch 98: loss did not improve from 0.08100\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0876 - val_binary_accuracy: 0.9850 - val_loss: 0.0939\n",
            "Epoch 99/200\n",
            "\n",
            "Epoch 99: loss improved from 0.08100 to 0.07630, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9887 - loss: 0.0763 - val_binary_accuracy: 0.9887 - val_loss: 0.1232\n",
            "Epoch 100/200\n",
            "\n",
            "Epoch 100: loss did not improve from 0.07630\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9887 - loss: 0.0795 - val_binary_accuracy: 0.9850 - val_loss: 0.1425\n",
            "Epoch 101/200\n",
            "\n",
            "Epoch 101: loss did not improve from 0.07630\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9887 - loss: 0.0986 - val_binary_accuracy: 0.9925 - val_loss: 0.1144\n",
            "Epoch 102/200\n",
            "\n",
            "Epoch 102: loss improved from 0.07630 to 0.07046, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9908 - loss: 0.0705 - val_binary_accuracy: 0.9925 - val_loss: 0.0885\n",
            "Epoch 103/200\n",
            "\n",
            "Epoch 103: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0740 - val_binary_accuracy: 0.9925 - val_loss: 0.0968\n",
            "Epoch 104/200\n",
            "\n",
            "Epoch 104: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0716 - val_binary_accuracy: 0.9925 - val_loss: 0.1021\n",
            "Epoch 105/200\n",
            "\n",
            "Epoch 105: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0738 - val_binary_accuracy: 0.9887 - val_loss: 0.1062\n",
            "Epoch 106/200\n",
            "\n",
            "Epoch 106: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9887 - loss: 0.0757 - val_binary_accuracy: 0.9925 - val_loss: 0.1206\n",
            "Epoch 107/200\n",
            "\n",
            "Epoch 107: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9887 - loss: 0.0935 - val_binary_accuracy: 0.9925 - val_loss: 0.0983\n",
            "Epoch 108/200\n",
            "\n",
            "Epoch 108: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.0815 - val_binary_accuracy: 0.9925 - val_loss: 0.0920\n",
            "Epoch 109/200\n",
            "\n",
            "Epoch 109: loss did not improve from 0.07046\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9836 - loss: 0.0827 - val_binary_accuracy: 0.9925 - val_loss: 0.1069\n",
            "Epoch 110/200\n",
            "\n",
            "Epoch 110: loss improved from 0.07046 to 0.06582, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9897 - loss: 0.0658 - val_binary_accuracy: 0.9925 - val_loss: 0.1182\n",
            "Epoch 111/200\n",
            "\n",
            "Epoch 111: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9887 - loss: 0.0775 - val_binary_accuracy: 0.9887 - val_loss: 0.1334\n",
            "Epoch 112/200\n",
            "\n",
            "Epoch 112: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0759 - val_binary_accuracy: 0.9887 - val_loss: 0.0940\n",
            "Epoch 113/200\n",
            "\n",
            "Epoch 113: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0718 - val_binary_accuracy: 0.9925 - val_loss: 0.1130\n",
            "Epoch 114/200\n",
            "\n",
            "Epoch 114: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0946 - val_binary_accuracy: 0.9925 - val_loss: 0.1008\n",
            "Epoch 115/200\n",
            "\n",
            "Epoch 115: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.0933 - val_binary_accuracy: 0.9774 - val_loss: 0.0945\n",
            "Epoch 116/200\n",
            "\n",
            "Epoch 116: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0709 - val_binary_accuracy: 0.9887 - val_loss: 0.1264\n",
            "Epoch 117/200\n",
            "\n",
            "Epoch 117: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9815 - loss: 0.0894 - val_binary_accuracy: 0.9737 - val_loss: 0.1073\n",
            "Epoch 118/200\n",
            "\n",
            "Epoch 118: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0718 - val_binary_accuracy: 0.9812 - val_loss: 0.1419\n",
            "Epoch 119/200\n",
            "\n",
            "Epoch 119: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0800 - val_binary_accuracy: 0.9850 - val_loss: 0.0797\n",
            "Epoch 120/200\n",
            "\n",
            "Epoch 120: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.0913 - val_binary_accuracy: 0.9850 - val_loss: 0.0792\n",
            "Epoch 121/200\n",
            "\n",
            "Epoch 121: loss did not improve from 0.06582\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9836 - loss: 0.0779 - val_binary_accuracy: 0.9850 - val_loss: 0.1287\n",
            "Epoch 122/200\n",
            "\n",
            "Epoch 122: loss improved from 0.06582 to 0.05557, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9928 - loss: 0.0556 - val_binary_accuracy: 0.9887 - val_loss: 0.1513\n",
            "Epoch 123/200\n",
            "\n",
            "Epoch 123: loss improved from 0.05557 to 0.05356, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9938 - loss: 0.0536 - val_binary_accuracy: 0.9887 - val_loss: 0.1540\n",
            "Epoch 124/200\n",
            "\n",
            "Epoch 124: loss did not improve from 0.05356\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0572 - val_binary_accuracy: 0.9887 - val_loss: 0.1502\n",
            "Epoch 125/200\n",
            "\n",
            "Epoch 125: loss did not improve from 0.05356\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9938 - loss: 0.0788 - val_binary_accuracy: 0.9887 - val_loss: 0.1208\n",
            "Epoch 126/200\n",
            "\n",
            "Epoch 126: loss did not improve from 0.05356\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0619 - val_binary_accuracy: 0.9887 - val_loss: 0.1256\n",
            "Epoch 127/200\n",
            "\n",
            "Epoch 127: loss did not improve from 0.05356\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0734 - val_binary_accuracy: 0.9887 - val_loss: 0.1029\n",
            "Epoch 128/200\n",
            "\n",
            "Epoch 128: loss improved from 0.05356 to 0.05005, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9949 - loss: 0.0500 - val_binary_accuracy: 0.9925 - val_loss: 0.1062\n",
            "Epoch 129/200\n",
            "\n",
            "Epoch 129: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0636 - val_binary_accuracy: 0.9887 - val_loss: 0.1043\n",
            "Epoch 130/200\n",
            "\n",
            "Epoch 130: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0600 - val_binary_accuracy: 0.9812 - val_loss: 0.0882\n",
            "Epoch 131/200\n",
            "\n",
            "Epoch 131: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0590 - val_binary_accuracy: 0.9887 - val_loss: 0.1046\n",
            "Epoch 132/200\n",
            "\n",
            "Epoch 132: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0573 - val_binary_accuracy: 0.9887 - val_loss: 0.1592\n",
            "Epoch 133/200\n",
            "\n",
            "Epoch 133: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0552 - val_binary_accuracy: 0.9850 - val_loss: 0.1364\n",
            "Epoch 134/200\n",
            "\n",
            "Epoch 134: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9836 - loss: 0.0727 - val_binary_accuracy: 0.9850 - val_loss: 0.1317\n",
            "Epoch 135/200\n",
            "\n",
            "Epoch 135: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9959 - loss: 0.0831 - val_binary_accuracy: 0.9850 - val_loss: 0.1296\n",
            "Epoch 136/200\n",
            "\n",
            "Epoch 136: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0660 - val_binary_accuracy: 0.9887 - val_loss: 0.1178\n",
            "Epoch 137/200\n",
            "\n",
            "Epoch 137: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9846 - loss: 0.0845 - val_binary_accuracy: 0.9925 - val_loss: 0.1206\n",
            "Epoch 138/200\n",
            "\n",
            "Epoch 138: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0563 - val_binary_accuracy: 0.9850 - val_loss: 0.1406\n",
            "Epoch 139/200\n",
            "\n",
            "Epoch 139: loss did not improve from 0.05005\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0523 - val_binary_accuracy: 0.9850 - val_loss: 0.1293\n",
            "Epoch 140/200\n",
            "\n",
            "Epoch 140: loss improved from 0.05005 to 0.04270, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9959 - loss: 0.0427 - val_binary_accuracy: 0.9887 - val_loss: 0.1180\n",
            "Epoch 141/200\n",
            "\n",
            "Epoch 141: loss did not improve from 0.04270\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9938 - loss: 0.0472 - val_binary_accuracy: 0.9887 - val_loss: 0.1390\n",
            "Epoch 142/200\n",
            "\n",
            "Epoch 142: loss did not improve from 0.04270\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0482 - val_binary_accuracy: 0.9887 - val_loss: 0.1353\n",
            "Epoch 143/200\n",
            "\n",
            "Epoch 143: loss did not improve from 0.04270\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0534 - val_binary_accuracy: 0.9925 - val_loss: 0.1452\n",
            "Epoch 144/200\n",
            "\n",
            "Epoch 144: loss improved from 0.04270 to 0.04224, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9949 - loss: 0.0422 - val_binary_accuracy: 0.9887 - val_loss: 0.1570\n",
            "Epoch 145/200\n",
            "\n",
            "Epoch 145: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0550 - val_binary_accuracy: 0.9850 - val_loss: 0.1529\n",
            "Epoch 146/200\n",
            "\n",
            "Epoch 146: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0516 - val_binary_accuracy: 0.9887 - val_loss: 0.1610\n",
            "Epoch 147/200\n",
            "\n",
            "Epoch 147: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0670 - val_binary_accuracy: 0.9887 - val_loss: 0.1632\n",
            "Epoch 148/200\n",
            "\n",
            "Epoch 148: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0451 - val_binary_accuracy: 0.9850 - val_loss: 0.1202\n",
            "Epoch 149/200\n",
            "\n",
            "Epoch 149: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0521 - val_binary_accuracy: 0.9925 - val_loss: 0.1854\n",
            "Epoch 150/200\n",
            "\n",
            "Epoch 150: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0649 - val_binary_accuracy: 0.9887 - val_loss: 0.1485\n",
            "Epoch 151/200\n",
            "\n",
            "Epoch 151: loss did not improve from 0.04224\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9825 - loss: 0.0764 - val_binary_accuracy: 0.9850 - val_loss: 0.1667\n",
            "Epoch 152/200\n",
            "\n",
            "Epoch 152: loss improved from 0.04224 to 0.03716, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9959 - loss: 0.0372 - val_binary_accuracy: 0.9887 - val_loss: 0.1311\n",
            "Epoch 153/200\n",
            "\n",
            "Epoch 153: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9949 - loss: 0.0380 - val_binary_accuracy: 0.9850 - val_loss: 0.1432\n",
            "Epoch 154/200\n",
            "\n",
            "Epoch 154: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0517 - val_binary_accuracy: 0.9850 - val_loss: 0.1325\n",
            "Epoch 155/200\n",
            "\n",
            "Epoch 155: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0556 - val_binary_accuracy: 0.9850 - val_loss: 0.1449\n",
            "Epoch 156/200\n",
            "\n",
            "Epoch 156: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0500 - val_binary_accuracy: 0.9887 - val_loss: 0.1577\n",
            "Epoch 157/200\n",
            "\n",
            "Epoch 157: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9856 - loss: 0.0703 - val_binary_accuracy: 0.9737 - val_loss: 0.1296\n",
            "Epoch 158/200\n",
            "\n",
            "Epoch 158: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0611 - val_binary_accuracy: 0.9887 - val_loss: 0.1669\n",
            "Epoch 159/200\n",
            "\n",
            "Epoch 159: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0795 - val_binary_accuracy: 0.9812 - val_loss: 0.0868\n",
            "Epoch 160/200\n",
            "\n",
            "Epoch 160: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9815 - loss: 0.0721 - val_binary_accuracy: 0.9925 - val_loss: 0.1212\n",
            "Epoch 161/200\n",
            "\n",
            "Epoch 161: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0622 - val_binary_accuracy: 0.9925 - val_loss: 0.1033\n",
            "Epoch 162/200\n",
            "\n",
            "Epoch 162: loss did not improve from 0.03716\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0462 - val_binary_accuracy: 0.9925 - val_loss: 0.1086\n",
            "Epoch 163/200\n",
            "\n",
            "Epoch 163: loss improved from 0.03716 to 0.03546, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9959 - loss: 0.0355 - val_binary_accuracy: 0.9925 - val_loss: 0.1250\n",
            "Epoch 164/200\n",
            "\n",
            "Epoch 164: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0556 - val_binary_accuracy: 0.9925 - val_loss: 0.1383\n",
            "Epoch 165/200\n",
            "\n",
            "Epoch 165: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9938 - loss: 0.0429 - val_binary_accuracy: 0.9925 - val_loss: 0.1314\n",
            "Epoch 166/200\n",
            "\n",
            "Epoch 166: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0612 - val_binary_accuracy: 0.9887 - val_loss: 0.1409\n",
            "Epoch 167/200\n",
            "\n",
            "Epoch 167: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0538 - val_binary_accuracy: 0.9925 - val_loss: 0.1428\n",
            "Epoch 168/200\n",
            "\n",
            "Epoch 168: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0450 - val_binary_accuracy: 0.9887 - val_loss: 0.1404\n",
            "Epoch 169/200\n",
            "\n",
            "Epoch 169: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0427 - val_binary_accuracy: 0.9887 - val_loss: 0.1380\n",
            "Epoch 170/200\n",
            "\n",
            "Epoch 170: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0410 - val_binary_accuracy: 0.9925 - val_loss: 0.1721\n",
            "Epoch 171/200\n",
            "\n",
            "Epoch 171: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0515 - val_binary_accuracy: 0.9925 - val_loss: 0.1806\n",
            "Epoch 172/200\n",
            "\n",
            "Epoch 172: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0553 - val_binary_accuracy: 0.9774 - val_loss: 0.1222\n",
            "Epoch 173/200\n",
            "\n",
            "Epoch 173: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9805 - loss: 0.0934 - val_binary_accuracy: 0.9887 - val_loss: 0.1060\n",
            "Epoch 174/200\n",
            "\n",
            "Epoch 174: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9887 - loss: 0.0553 - val_binary_accuracy: 0.9925 - val_loss: 0.1467\n",
            "Epoch 175/200\n",
            "\n",
            "Epoch 175: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0833 - val_binary_accuracy: 0.9925 - val_loss: 0.1278\n",
            "Epoch 176/200\n",
            "\n",
            "Epoch 176: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9877 - loss: 0.0563 - val_binary_accuracy: 0.9925 - val_loss: 0.1043\n",
            "Epoch 177/200\n",
            "\n",
            "Epoch 177: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0419 - val_binary_accuracy: 0.9887 - val_loss: 0.1552\n",
            "Epoch 178/200\n",
            "\n",
            "Epoch 178: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0593 - val_binary_accuracy: 0.9774 - val_loss: 0.1368\n",
            "Epoch 179/200\n",
            "\n",
            "Epoch 179: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0555 - val_binary_accuracy: 0.9925 - val_loss: 0.1502\n",
            "Epoch 180/200\n",
            "\n",
            "Epoch 180: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0512 - val_binary_accuracy: 0.9925 - val_loss: 0.1469\n",
            "Epoch 181/200\n",
            "\n",
            "Epoch 181: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0413 - val_binary_accuracy: 0.9925 - val_loss: 0.1507\n",
            "Epoch 182/200\n",
            "\n",
            "Epoch 182: loss did not improve from 0.03546\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9846 - loss: 0.0689 - val_binary_accuracy: 0.9925 - val_loss: 0.1247\n",
            "Epoch 183/200\n",
            "\n",
            "Epoch 183: loss improved from 0.03546 to 0.03411, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9949 - loss: 0.0341 - val_binary_accuracy: 0.9925 - val_loss: 0.1491\n",
            "Epoch 184/200\n",
            "\n",
            "Epoch 184: loss did not improve from 0.03411\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0487 - val_binary_accuracy: 0.9925 - val_loss: 0.1551\n",
            "Epoch 185/200\n",
            "\n",
            "Epoch 185: loss improved from 0.03411 to 0.03371, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9949 - loss: 0.0337 - val_binary_accuracy: 0.9925 - val_loss: 0.1529\n",
            "Epoch 186/200\n",
            "\n",
            "Epoch 186: loss did not improve from 0.03371\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9949 - loss: 0.0340 - val_binary_accuracy: 0.9925 - val_loss: 0.1650\n",
            "Epoch 187/200\n",
            "\n",
            "Epoch 187: loss did not improve from 0.03371\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0424 - val_binary_accuracy: 0.9925 - val_loss: 0.1711\n",
            "Epoch 188/200\n",
            "\n",
            "Epoch 188: loss did not improve from 0.03371\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9866 - loss: 0.0672 - val_binary_accuracy: 0.9925 - val_loss: 0.1557\n",
            "Epoch 189/200\n",
            "\n",
            "Epoch 189: loss did not improve from 0.03371\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0387 - val_binary_accuracy: 0.9925 - val_loss: 0.1206\n",
            "Epoch 190/200\n",
            "\n",
            "Epoch 190: loss did not improve from 0.03371\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0417 - val_binary_accuracy: 0.9925 - val_loss: 0.1445\n",
            "Epoch 191/200\n",
            "\n",
            "Epoch 191: loss improved from 0.03371 to 0.03261, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9949 - loss: 0.0326 - val_binary_accuracy: 0.9925 - val_loss: 0.1608\n",
            "Epoch 192/200\n",
            "\n",
            "Epoch 192: loss did not improve from 0.03261\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9938 - loss: 0.0378 - val_binary_accuracy: 0.9925 - val_loss: 0.0981\n",
            "Epoch 193/200\n",
            "\n",
            "Epoch 193: loss did not improve from 0.03261\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0440 - val_binary_accuracy: 0.9887 - val_loss: 0.0936\n",
            "Epoch 194/200\n",
            "\n",
            "Epoch 194: loss did not improve from 0.03261\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9908 - loss: 0.0471 - val_binary_accuracy: 0.9812 - val_loss: 0.0771\n",
            "Epoch 195/200\n",
            "\n",
            "Epoch 195: loss did not improve from 0.03261\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9897 - loss: 0.0476 - val_binary_accuracy: 0.9887 - val_loss: 0.1347\n",
            "Epoch 196/200\n",
            "\n",
            "Epoch 196: loss improved from 0.03261 to 0.02265, saving model to best_model.keras\n",
            "31/31 - 0s - 5ms/step - binary_accuracy: 0.9969 - loss: 0.0226 - val_binary_accuracy: 0.9925 - val_loss: 0.1535\n",
            "Epoch 197/200\n",
            "\n",
            "Epoch 197: loss did not improve from 0.02265\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9918 - loss: 0.0400 - val_binary_accuracy: 0.9925 - val_loss: 0.1714\n",
            "Epoch 198/200\n",
            "\n",
            "Epoch 198: loss did not improve from 0.02265\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9928 - loss: 0.0350 - val_binary_accuracy: 0.9925 - val_loss: 0.1882\n",
            "Epoch 199/200\n",
            "\n",
            "Epoch 199: loss did not improve from 0.02265\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9969 - loss: 0.0272 - val_binary_accuracy: 0.9925 - val_loss: 0.1936\n",
            "Epoch 200/200\n",
            "\n",
            "Epoch 200: loss did not improve from 0.02265\n",
            "31/31 - 0s - 4ms/step - binary_accuracy: 0.9959 - loss: 0.0304 - val_binary_accuracy: 0.9925 - val_loss: 0.1852\n"
          ]
        }
      ],
      "source": [
        "mc = callbacks.ModelCheckpoint('best_model.keras', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=['binary_accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=200, shuffle=True, batch_size=32, verbose=2, validation_data=(X_test, y_test), callbacks=mc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's make some plots to see how our training went"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9LG6fiue2rgx",
        "outputId": "09d96543-0ce0-4111-fcbb-70f43a1d677d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Training', 'Testing'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "h1AEgj0l2tdL",
        "outputId": "ba180472-298e-4af7-f06f-e95ab9f3fd21"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['binary_accuracy'])\n",
        "plt.plot(history.history['val_binary_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Training', 'Testing'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far, so good! It's a little unusual to see a training accuracy _lower_ than the testing accuracy, but this happens due to our use of Dropout. Our small dataset may also play a part in this, as the training set may have more difficult samples than the testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's load our best model and evaluate its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ctFdmyJB7foy"
      },
      "outputs": [],
      "source": [
        "test_features = X_test\n",
        "test_labels = y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njMCwlbu2-Vv",
        "outputId": "551cb592-91f8-4d16-c66f-01df3a17e07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 - 0s - 27ms/step - binary_accuracy: 0.9925 - loss: 0.1535\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.15353737771511078, 0.9924812316894531]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saved_model = tf.keras.models.load_model(\"best_model.keras\")\n",
        "saved_model.evaluate(test_features, test_labels, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over 98% accuracy! Pretty good performance, given our small dataset and simple MLP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot our model's Confusion Matrix to analyse the performance for each class separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IsFlcm2ny5SC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "LKVByK0vy8gi",
        "outputId": "137b418a-3f07-45ce-d149-143e0b4c5e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_predictions = np.round(saved_model.predict(test_features))\n",
        "ConfusionMatrixDisplay.from_predictions(test_labels, test_predictions, values_format= '.1%', normalize=\"true\", display_labels=[\"Noise\", \"Event\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "zO0QmFwezwtY",
        "outputId": "7ef47b80-0203-4339-d98f-f9741c868ea7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAGwCAYAAADv1swzAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANnJJREFUeJzt3QuczdX6+PFnD8a4jVsYl3HP/Roll0SRLkdIKU2lOJwOcif+5ZaikjhU6EaKdCVU/ISoqBOiklxqTsZl6By5zDBmzP7+X8/S7GYzara91+zZsz/vXus1s7/f71577XlN9jPPetZ3uRzHcQQAAMCCCBudAgAAKAINAABgDYEGAACwhkADAABYQ6ABAACsIdAAAADWEGgAAABr8tvrOm9zu91y8OBBKVasmLhcrmAPBwDgI72N1MmTJ6VChQoSEWHn7+6UlBRJTU0NSF+RkZESFRUloYZA4xJpkBEbGxvsYQAA/JSQkCCVKlWyEmRUq1JUEo+kB6S/mJgYiY+PD7lgg0DjEmkmQ/2ytapEF2UGCnlTt1oNgz0EwJqzkiafy0eef88DTTMZiUfS5ZctVSW6mH+fEydOuqVKs/+YPgk0wkTGdIkGGf7+AgG5VX5XgWAPAbDn9w04bE9/Fy3mMs0fbgndKXoCDQAALEp33JLu+N9HqCLQAADAIrc4pvnbR6gi5w8AAKwhowEAgEVu85//fYQqAg0AACxKdxzT/O0jVDF1AgAArCGjAQCARe4wLwYl0AAAwCK3OJIexoEGUycAAMAaMhoAAFjkZuoEAADYks6qEwAAADvIaAAAYJH79+ZvH6GKQAMAAIvSA7DqxN/nBxOBBgAAFqU755q/fYQqajQAAIA1ZDQAALDITY0GAACwxS0uSReX332EKqZOAACANWQ0AACwyO2ca/72EaoINAAAsCg9AFMn/j4/mJg6AQAA1pDRAADAovQwz2gQaAAAYJHbcZnmbx+hiqkTAABgDRkNAAAsSmfqBAAA2JIuEab510foYuoEAACLnN9rNPxp2ocvNmzYIJ07d5YKFSqIy+WSpUuXes6lpaXJww8/LA0bNpQiRYqYa+677z45ePCgVx9Hjx6VuLg4iY6OlhIlSkifPn0kKSnJ5/dPoAEAQB6TnJwsjRs3lueff/6Cc6dOnZKtW7fK2LFjzdf3339fdu3aJbfeeqvXdRpk7NixQ1avXi0rVqwwwUu/fv18HgtTJwAA5LEajZtuusm0rBQvXtwED5k999xzctVVV8m+ffukcuXKsnPnTlm5cqV8/fXX0rx5c3PNrFmz5Oabb5ZnnnnGZEGyi4wGAAAWpTsRAWnqxIkTXu3MmTMBGePx48fNFItOkahNmzaZ7zOCDNWhQweJiIiQr776yqe+CTQAAAgRsbGxJiOR0aZMmeJ3nykpKaZmo2fPnqYeQyUmJkrZsmW9rsufP7+UKlXKnPMFUycAAFjkFpe4/fy73i3ndlVLSEjwBAOqYMGCfvWrhaE9evQQx3Fk9uzZYgOBBgAAIVKjER0d7RVoBCLI+OWXX2Tt2rVe/cbExMiRI0e8rj979qxZiaLnfMHUCQAAYSbt9yBjz5498sknn0jp0qW9zrds2VKOHTsmW7Zs8RzTYMTtdkuLFi18ei0yGgAAWJSeqZjz0vs4N3WSXXq/i71793oex8fHy7Zt20yNRfny5eX22283S1t12Wp6erqn7kLPR0ZGSt26deXGG2+Uvn37ypw5c0xgMnDgQLnrrrt8WnGiCDQAALBeo+Hyuw9fbN68Wdq3b+95PGzYMPO1V69eMmHCBFm2bJl53KRJE6/nrVu3Ttq1a2e+X7hwoQkurr/+erPapHv37jJz5kyfx06gAQBAHtOuXTtT4Hkxf3Yug2Y3Fi1a5PdYCDQAALDIHYC9TjJWnYQiAg0AAPJYjUZuQqABAIDljIY7jDMaLG8FAADWkNEAAMCidMdlmr99hCoCDQAALEoPQDFoOlMnAAAAFyKjAQCARW4nwjT/+gjdjAaBBgAAFqUzdQIAAGAHGQ0AACxyB2DViPYRqgg0AADI9TfsipBQFbojBwAAuR4ZDQAAcv1eJxESqgg0AACwyC0u0/ztI1QRaAAAYFF6mGc0QnfkAAAg1yOjAQBArr9hV4SEKgINAAAscjsu0/ztI1SFbogEAAByPTIaAABY5A7A1Eko37CLQAMAgFy/e2uEhKrQHTkAAMj1yGgAAGBRurhM87ePUEWgAQCARW6mTgAAAOwgowEAgEXpAZj60D5CFYEGAAAWucN86oRAAwAAi9LZVA0AAMAOMhoAAFjkiEvcftZoaB+hikADAACL0pk6AQAAsIOMBgAAFrnDfJt4Ag0AACxKD8Durf4+P5hCd+QAACDXI6MBAIBFbqZOAACALW6JMM3fPkJV6I4cAADkemQ0AACwKN1xmeZvH6GKQAMAAIvc1GgAAABbnADs3qp9hKrQHTkAAMj1yGgAAGBRurhM87ePUEWgAQCARW7H/xoL7SNUMXUCAACsIdBAUH33ZREZd1816dm0vnSq0EQ2flzc6/zrz8RIn2vqyK01Gkr3ug3k4R415MethT3nExMi5dlhsXJfi7rSuXojub9lXVkwNUbSUkM3zYjw0qBFkkx8LV4Wbd0hqw5ul5Y3Hg/2kBBg7t+LQf1tvtiwYYN07txZKlSoIC6XS5YuXep13nEcGTdunJQvX14KFSokHTp0kD179nhdc/ToUYmLi5Po6GgpUaKE9OnTR5KSksI30KhatarMmDEj2MOAj1JORUj1+qdl4OT9WZ6vWD1FBjyxX+au3SXTlu6VmNhUGdOzhhz7Xz5zPmFvQXG7RQY/tV9eXPej/GPCAfnw9dIyb0r5HH4nwKWJKuyWn3dEyXP/r1KwhwJL3OIKSPNFcnKyNG7cWJ5//vkszz/99NMyc+ZMmTNnjnz11VdSpEgR6dSpk6SkpHiu0SBjx44dsnr1almxYoUJXvr16xd6NRr333+/vPbaazJlyhQZPXq057hGX926dTNRV3Z8/fXX5geF0HLldSdNu5jrbjvm9bjfhAOy8s3SEv9DIWl6TZJc2f6kaRnKV0mV/T8dkRULLpN+4w9aHTsQCJvXRZsGZMeJEye8HhcsWNC08910002mZUU/V/UP80cffVS6dOliji1YsEDKlStnPnvvuusu2blzp6xcudJ8tjZv3txcM2vWLLn55pvlmWeeMZmSkMpoREVFyVNPPSW//fbbJfdRpkwZKVz4j5Q68h6dDvnojdJSJDpdqtc7fdHrkk/mk2Il0nN0bADwV3cGTfezqdjYWClevLin6R/pvoqPj5fExEQzXZJB+2rRooVs2rTJPNavOl2SEWQovT4iIsJkQHyRKwINHXxMTMyf/sDee+89qV+/voncdJpk2rRpF5060WhtwoQJUrlyZXO9Rl6DBg3yXHvmzBkZMWKEVKxY0WRB9If76aefWnyH8MeXq6OlS82G0rlaI1nyUhmZsnivFC+ddSBxID5SPni1jNx8739zfJwAYLtGIyEhQY4fP+5pY8aM8Xk8GmQozWBkpo8zzunXsmXLep3Pnz+/lCpVynNNSAUa+fLlk8mTJ5u0zP79F87Vb9myRXr06GHSOd99950JIsaOHSvz58+/aFAyffp0mTt3rilu0VRQw4YNPecHDhxoorXFixfLt99+K3fccYfceOONFxTCZKbBiaasMjfkjCatk+SF1btk+rI90rzdSXniH1Xl2H8vnPX776EC8khcDWn7t2Nyc9zRoIwVAGyKjo72allNm+Q2uSLQUFqP0aRJExk/fvwF55599lm5/vrrTXBRq1YtU9ehwcLUqVOz7Gvfvn0mQ6KZEs1qXHXVVdK3b1/PuXnz5sk777wj11xzjdSoUcNkN9q0aWOOX4xmWzKnqzR9hZwrlqtYLVXqNjslw55NkHz5RVa+Wcrrmv8l5pdRd9SQes2TZfDUhKCNFQDO59ZiTsfPFsAbdunnozp8+LDXcX2ccU6/HjlyxOv82bNnzUqUjGtCLtBQWqehhaFahJKZPm7durXXMX2sGYj09AtT6JqhOH36tFSvXt0EGEuWLDE/IKUZEX2OBixFixb1tPXr18tPP/100bFpeipzukrTVwgOxy2SdibCK5Mx8vaacnnD0zJ8+j6JyFW/1QDCnROAFSfaR6BUq1bNBAtr1qzxHNMsvdZetGzZ0jzWr8eOHTMzChnWrl0rbrfblBuE1KqTzNq2bWuW1+iHumYtLpVmG3bt2iWffPKJWZbTv39/k/3QYELXAOtUjf7w9GtmGnBczMUqe+Gf08kRcjC+oNd9MX76vpAUK3FWokuly6J/lZOWNxyXUuXS5MTR/LJs3mXy38QCck3nY15BRtmKqdJ33EE5/r8/fqVLlT0XXAK5WVThdKlQLdXzWJdw65Lvk8fyya8HIoM6NoTu7q1JSUmyd+9erwLQbdu2mRoLzfQPGTJEHn/8cbn88stN4KEzBlrP2LVrV3N93bp1TUmB/rGuS2DT0tLMTIKWMPiy4iTXBRrqySefNFMotWvX9hzTN/zFF194XaePNStxfrCQQW9Aojcr0TZgwACpU6eOyWY0bdrUZDQ0JaRTJwiu3dsLy6jba3oez51Q0Xzt2OOoDHoyQfbvLSiT3qlqgoxiJdOlVuNTMm3JHqla+9xa760biplARVtcs/pefa86uC2H3w3gu1qNT8vU9/7Ipj448dyy7P97q6RMG1o5iCNDKNu8ebO0b9/e83jYsGHma69evUx946hRo8y9NvS+GJq50PIBXc6qq0AzLFy40AQXWrqgq026d+9u7r3hq1wXaGjRpt4kJPObGT58uFx55ZUyadIkufPOO00h53PPPScvvPBCln3oD1GDCU3v6JLXN954wwQeVapUkdKlS5v+77vvPrNyRQOPX3/91aSQGjVqJLfccksOvls0bpX0pwHBuFf+86fPv+HOo6YBoerbTUWlU4XGwR4GLHIHYJt4X5/frl27P70Pld4t9LHHHjPtYjT7sWjRIvFXrpzN1jeu80AZrrjiCnn77bfNKpEGDRqY26bqNRebXtG1vy+99JKp49DgQadQli9fboIMpUWfGmhoAKOZE00V6U1JNJ0EAEAguf0tBA3A1EswuZzs3noTXrRwRlef/La7ukQXy5XxGuA33X8GyKvOOmnyqXxgCvx1qaitz4ku/9dbChTxr94mLTlVPrjhVWtjtSnXTZ0AAJCXuC9hr5Ks+ghVBBoAAOSxVSe5CTl/AABgDRkNAAAscod5RoNAAwAAi9xhHmgwdQIAAKwhowEAgEXuMM9oEGgAAGCRE4DlqaF8wysCDQAALHKHeUaDGg0AAGANGQ0AACxyh3lGg0ADAACL3GEeaDB1AgAArCGjAQCARe4wz2gQaAAAYJHjuEzzt49QxdQJAACwhowGAAAWucXl9w27/H1+MBFoAABgkTvMazSYOgEAANaQ0QAAwCInzItBCTQAALDIHeZTJwQaAABY5IR5RoMaDQAAYA0ZDQAALHICMHUSyhkNAg0AACxyTKDgfx+hiqkTAABgDRkNAAAscovL/OdvH6GKQAMAAItYdQIAAGAJGQ0AACxyOy5xccMuAABgg+MEYNVJCC87YeoEAABYQ0YDAACLnDAvBiXQAADAIodAAwAA2OIO82JQajQAAIA1ZDQAALDICfNVJwQaAABYDzRcfvcRqpg6AQAA1pDRAADAIodVJwAAwBbn9+ZvH6GKqRMAAGANGQ0AACxywnzqhIwGAAA5MXfi+Nl8kJ6eLmPHjpVq1apJoUKFpEaNGjJp0iRxMi1f0e/HjRsn5cuXN9d06NBB9uzZE/C3T6ABAIBNzrmMhj9N+/DFU089JbNnz5bnnntOdu7caR4//fTTMmvWLM81+njmzJkyZ84c+eqrr6RIkSLSqVMnSUlJCejbZ+oEAIA8ZuPGjdKlSxe55ZZbzOOqVavKm2++Kf/+97892YwZM2bIo48+aq5TCxYskHLlysnSpUvlrrvuCthYyGgAAJADdwZ1/GzqxIkTXu3MmTNZvmarVq1kzZo1snv3bvN4+/bt8vnnn8tNN91kHsfHx0tiYqKZLslQvHhxadGihWzatCmg75+MBgAAIVIMGhsb63V8/PjxMmHChAuuHz16tAlE6tSpI/ny5TM1G0888YTExcWZ8xpkKM1gZKaPM84FCoEGAAAhIiEhQaKjoz2PCxYsmOV1b7/9tixcuFAWLVok9evXl23btsmQIUOkQoUK0qtXrxwcMYEGAAB2Ob4Xc2bZh4gJMjIHGhczcuRIk9XIqLVo2LCh/PLLLzJlyhQTaMTExJjjhw8fNqtOMujjJk2aSCBRowEAQIjUaGTXqVOnJCLC+yNep1Dcbrf5Xpe9arChdRwZdKpFV5+0bNlSAomMBgAAeUznzp1NTUblypXN1Mk333wjzz77rPTu3ducd7lcZirl8ccfl8svv9wEHnrfDZ1a6dq1a0DHQqABAEAe2+xk1qxZJnDo37+/HDlyxAQQ//jHP8wNujKMGjVKkpOTpV+/fnLs2DFp06aNrFy5UqKioiSQXE7m24Qh2zTFpEuBfttdXaKLMQOFvKlThcDO1QK5yVknTT6VD+T48ePZqnu41M+Jyi+Ok4jC/n14u0+lyL5+j1kbq03ZymgsW7Ys2x3eeuut/owHAADkIdkKNLI7X6NzPrpWFwAAZBLGcwfZCjQyqlQBAIBvHHZvvXSB3ngFAIA8x8n53VtDOtDQqRHdarZixYpStGhR+fnnn81xrW595ZVXbIwRAACES6Ch63Lnz59vtpeNjIz0HG/QoIG8/PLLgR4fAAAhzhWgFiaBhm4j++KLL5qNWfQuYxkaN24sP/74Y6DHBwBAaHOYOvHJgQMHpGbNmlkWjKalpQVqXAAAIA/wOdCoV6+efPbZZxccf/fdd6Vp06aBGhcAAHmDE94ZDZ9vQa63L9Wd3zSzoVmM999/X3bt2mWmVFasWGFnlAAAhConcLu3hkVGo0uXLrJ8+XL55JNPpEiRIibw2LlzpznWsWNHO6MEAAAh6ZI2Vbvmmmtk9erVgR8NAAB5jHMJ27xn1UeouuTdWzdv3mwyGRl1G82aNQvkuAAAyBucnN+9NaQDjf3790vPnj3liy++kBIlSphjur1sq1atZPHixVKpUiUb4wQAAOFQo/H3v//dLGPVbMbRo0dN0++1MFTPAQCALIpBHT9buGQ01q9fLxs3bpTatWt7jun3s2bNMrUbAADgDy7nXPO3j7AJNGJjY7O8MZfugVKhQoVAjQsAgLzBCe8aDZ+nTqZOnSoPPfSQKQbNoN8PHjxYnnnmmUCPDwAASB7PaJQsWVJcrj/mh5KTk6VFixaSP/+5p589e9Z837t3b+natau90QIAEGqc8L5hV7YCjRkzZtgfCQAAeZET3lMn2Qo09JbjAAAAOXbDLpWSkiKpqalex6Kjo/3pEgCAvMUJ74yGz8WgWp8xcOBAKVu2rNnrROs3MjcAAJCJE967t/ocaIwaNUrWrl0rs2fPloIFC8rLL78sEydONEtbdQdXAACAS5460V1aNaBo166dPPDAA+YmXTVr1pQqVarIwoULJS4uztcuAQDIu5zwXnXic0ZDbzlevXp1Tz2GPlZt2rSRDRs2BH6EAADkgTuDuvxsYRNoaJARHx9vvq9Tp468/fbbnkxHxiZrAAAAlxRo6HTJ9u3bzfejR4+W559/XqKiomTo0KEycuRIfqoAAGTmhHcxqM81GhpQZOjQoYP8+OOPsmXLFlOn0ahRo0CPDwAAhOt9NJQWgWoDAAAXcgVg91VXXg80Zs6cme0OBw0a5M94AABAHpKtQGP69OnZ6kw3Xgu3QKNbrYaS31Ug2MMArFh1cFuwhwBYc+KkW0rWyoEXcsJ7eWu2Ao2MVSYAAMBHDrcgBwAAyJ3FoAAA4E844Z3RINAAAMAiVwDu7BlWdwYFAADILjIaAADY5IT31MklZTQ+++wzueeee6Rly5Zy4MABc+z111+Xzz//PNDjAwAgtDnhfQtynwON9957Tzp16iSFChWSb775Rs6cOWOOHz9+XCZPnmxjjAAAIFwCjccff1zmzJkjL730khQo8MeNqlq3bi1bt24N9PgAAAhprjDfJt7nGo1du3ZJ27ZtLzhevHhxOXbsWKDGBQBA3uCE951Bfc5oxMTEyN69ey84rvUZ1atXD9S4AADIGxxqNHzSt29fGTx4sHz11Vdmb5ODBw/KwoULZcSIEfLPf/7TzigBAEB4TJ2MHj1a3G63XH/99XLq1CkzjVKwYEETaDz00EN2RgkAQIhyhfkNu3wONDSL8cgjj8jIkSPNFEpSUpLUq1dPihYtameEAACEMof7aFySyMhIE2BcddVVBBkAAOQyBw4cMPe8Kl26tLklRcOGDWXz5s2e847jyLhx46R8+fLmfIcOHWTPnj3Bz2i0b9/eZDUuZu3atf6OCQCAvMMJwNSHj8//7bffzG0n9DP7448/ljJlypggomTJkp5rnn76aZk5c6a89tprUq1aNRk7dqy5T9YPP/wgUVFRErRAo0mTJl6P09LSZNu2bfL9999Lr169AjYwAADyBCdwUycnTpzwOqw1ktrO99RTT0lsbKzMmzfPc0yDCU93jiMzZsyQRx99VLp06WKOLViwQMqVKydLly6Vu+66S4IWaEyfPj3L4xMmTDD1GgAAwI7Y2Fivx+PHjzefv+dbtmyZyU7ccccdsn79eqlYsaL079/frBxV8fHxkpiYaKZLMt8Pq0WLFrJp06aABhoB271V54FeffXVQHUHAEDe4ATuPhoJCQlmy4+MNmbMmCxf8ueff5bZs2fL5ZdfLqtWrTK3nxg0aJCZJlEaZCjNYGSmjzPO5brdWzUCCuScDgAAeYErgMtbo6OjTfsrehuK5s2be/Yga9q0qSlx0C1EcrrMwedA47bbbvN6rPM8hw4dMpWsWkgCAACCq3z58mZlaGZ169Y1G6Nm3OVbHT582FybQR+fX4uZ41MnOoeTuZUqVUratWsnH330kZkrAgAAwdW6dWuzN1lmu3fvlipVqngKQzXYWLNmjee8FprqXb9btmwZvIxGenq6PPDAA2YtbuYlMgAAIPfcsGvo0KHSqlUrM3XSo0cP+fe//y0vvviiaUpvUzFkyBCzI7vWcWQsb61QoYJ07dpVghZo5MuXT2644QbZuXMngQYAALn0FuRXXnmlLFmyxBSLPvbYYyaQ0OWscXFxnmtGjRolycnJ0q9fP7P7eps2bWTlypUBr7f0uUajQYMGppo183pcAACQu/ztb38z7WI0q6FBiDabfK7R0DSLbqC2YsUKUwSqczqZGwAAOI8TnlvE+5TR0Ihn+PDhcvPNN5vHt956q9etyHX1iT7WOg4AAPC7MN9ULduBxsSJE+XBBx+UdevW2R0RAADIM7IdaGjGQl177bU2xwMAQJ7iCkIxaG7iUzHon+3aCgAAsuAwdZJttWrV+stg4+jRo/6OCQAA5BE+BRpap6F3AwUAANnjYuok+3Tb2LJly9obDQAAeY0T3lMn2b6PBvUZAADA+qoTAADgAye8MxrZDjR0b3sAAOAbFzUaAADAGie8Mxo+73UCAACQXWQ0AACwyQnvjAaBBgAAFrnCvEaDqRMAAGANGQ0AAGxymDoBAACWuJg6AQAAsIOMBgAANjlMnQAAAFuc8A40mDoBAADWkNEAAMAi1+/N3z5CFYEGAAA2OeE9dUKgAQCARS6WtwIAANhBRgMAAJscpk4AAIBNjoQtpk4AAIA1ZDQAALDIFebFoAQaAADY5IR3jQZTJwAAwBoyGgAAWORi6gQAAFjjMHUCAABgBRkNAAAscjF1AgAArHHCe+qEQAMAAJuc8A40qNEAAADWkNEAAMAiFzUaAADAGoepEwAAACvIaAAAYJHLcUzzt49QRaABAIBNDlMnAAAAVhBoAACQA6tOXH62S/Xkk0+Ky+WSIUOGeI6lpKTIgAEDpHTp0lK0aFHp3r27HD58WGwg0AAAICemThw/2yX4+uuvZe7cudKoUSOv40OHDpXly5fLO++8I+vXr5eDBw/KbbfdJjYQaAAAkAclJSVJXFycvPTSS1KyZEnP8ePHj8srr7wizz77rFx33XXSrFkzmTdvnmzcuFG+/PLLgI+DQAMAgBCZOjlx4oRXO3PmzEVfV6dGbrnlFunQoYPX8S1btkhaWprX8Tp16kjlypVl06ZNAX//BBoAAITI1ElsbKwUL17c06ZMmZLlSy5evFi2bt2a5fnExESJjIyUEiVKeB0vV66cORdoLG8FACBEbkGekJAg0dHRnuMFCxa84Fq9ZvDgwbJ69WqJioqSYCOjAQBAiIiOjvZqWQUaOjVy5MgRueKKKyR//vymacHnzJkzzfeauUhNTZVjx455PU9XncTExAR8zGQ0AADIQzfsuv766+W7777zOvbAAw+YOoyHH37YTL8UKFBA1qxZY5a1ql27dsm+ffukZcuWEmgEGgAAWObKwTt7FitWTBo0aOB1rEiRIuaeGRnH+/TpI8OGDZNSpUqZzMhDDz1kgoyrr7464OMh0AAAIMxMnz5dIiIiTEZDV6506tRJXnjhBSuvRaABAIBNjnOu+duHHz799FOvx1ok+vzzz5tmG4EGAAAhsuokFLHqBAAAWENGAwAAm5zw3iaeQAMAAItc7nPN3z5CFVMnAADAGjIayPUatEiSO/r/Kpc3PCWlY87KhN5VZdPK4sEeFpAt331ZRN55oazs+a6wHD1cQMa/Ei+tbjruOf/6MzHy6Qcl5NeDBaRApCM1G56WB0YfkjpXnDLnExMiZdH0crLti6Ly268FpHS5NLnutt+k5+DD5nqEACe8p07IaCDXiyrslp93RMlz/69SsIcC+CzlVIRUr39aBk7en+X5itVTZMAT+2Xu2l0ybeleiYlNlTE9a8ix/+Uz5xP2FhS3W2TwU/vlxXU/yj8mHJAPXy8t86aUz+F3gtywe2soCmpG4/7775fXXnvtguN645CVK1fmyBgmTJggS5culW3btuXI68F3m9dFmwaEoiuvO2naxVx3m/d+E/0mHJCVb5aW+B8KSdNrkuTK9idNy1C+Sqrs/+mIrFhwmfQbf9Dq2JF37qMR1lMnN954o8ybN8/rWFabxABAXpeW6pKP3igtRaLTpXq90xe9LvlkPilWIj1HxwaE7NSJBhW6W1zmVrJkSbn77rvlzjvv9Lo2LS1NLrvsMlmwYIF57Ha7ZcqUKVKtWjUpVKiQNG7cWN59912vO6G5XC6zcUzz5s2lcOHC0qpVK7N5jJo/f75MnDhRtm/fbq7TpseyordoPXHihFcDgED4cnW0dKnZUDpXayRLXiojUxbvleKlsw4kDsRHygevlpGb7/1vjo8Tl8YV5lMnQQ80LiYuLk6WL18uSUlJnmOrVq2SU6dOSbdu3cxjDTI06JgzZ47s2LFDhg4dKvfcc4/ZDjezRx55RKZNmyabN282W+T27t3bHNdAZvjw4VK/fn05dOiQaecHNxn0tYoXL+5puvsdAARCk9ZJ8sLqXTJ92R5p3u6kPPGPqnLsvxcmnP97qIA8EldD2v7tmNwcdzQoY4UfxaCOny1EBT3QWLFihRQtWtSrTZ482dRp6G5zS5Ys8Vy7aNEiufXWW83OdJph0OteffVVc2316tVNzYcGGnPnzvV6jSeeeEKuvfZaqVevnowePVo2btwoKSkpJguir6fBR0Y2RY9lZcyYMXL8+HFPS0hIsP6zARA+Bc8Vq6VK3WanZNizCZIvv8jKN0t5XfO/xPwy6o4aUq95sgyeyr8/CB1Br9Fo3769zJ492+uYblurH/49evSQhQsXyr333ivJycnywQcfyOLFi801e/fuNdmNjh07ej03NTVVmjZt6nWsUaNGnu/Llz9XqX3kyBGpXLmyT1M81I4AyAmOWyTtTIRXJkODjMsbnpbh0/dJRND/RIQvXGG+10nQAw3NWtSsWfOi0yeaidCgYPXq1SbboMWjKmNK5cMPP5SKFSt6Pe/8gKBAgQKe77UOI6O+A6EhqnC6VKiW6nmsy/90ueDJY/nk1wORQR0b8FdOJ0fIwfg//k3S+2L89H0hKVbirESXSpdF/yonLW84LqXKpcmJo/ll2bzL5L+JBeSazsc8QcbI22tK2Yqp0nfcQTn+vz/+2S5V9mxQ3hN85LDqJNfSwk2thXjrrbfk448/ljvuuMMTNOg0iAYU+/btM8HIpYqMjJT0dKq3c7NajU/L1Pd+8jx+cOK5JX3/91ZJmTY0+1kpIBh2by8so27/44+puRPO/WHUscdRGfRkguzfW1AmvVPVBBnFSqZLrcanZNqSPVK1doq5buuGYiZQ0RbXrL5X36sOsiwfuV/QAw2ttUhMTPQ6ptMmurpE6eoTLfbcvXu3rFu3znON1mmMGDHCFIBqdqJNmzamduKLL76Q6Oho6dWrV7Zev2rVqhIfH2/uo1GpUiXTL1Mkucu3m4pKpwqNgz0M4JI0bpX0pwHBuFf+86fPv+HOo6YhdLnCfOok6DN9emMurZvI3DRoyDx98sMPP5jpkdatW3s9d9KkSTJ27FizIqRu3bpmWkWnUnS5a3Z1797dPE9rRcqUKSNvvvlmQN8fACDMOeG96sTlOCE88RNEeh8NXebaTrpIftcfNSBAXkJqHnnZiZNuKVnrZ5MN10y4rc+Jljc+JvkLRPnV19m0FNm0cpy1sebpqRMAAPIyV5hPnRBoAABgk9s51/ztI0QRaAAAYJPDNvEAAABWkNEAAMAiVwBqLM7dajI0EWgAAGCTE953BmXqBAAAWENGAwAAi1wsbwUAANY4rDoBAACwgowGAAAWuRzHNH/7CFUEGgAA2OT+vfnbR4hi6gQAAFhDRgMAAItcTJ0AAABrnPBedUKgAQCATQ53BgUAALCCjAYAABa5uDMoAACwxmHqBAAAwAoyGgAAWORyn2v+9hGqCDQAALDJYeoEAADACjIaAADY5HDDLgAAYIkrzG9BztQJAACwhowGAAA2ORSDAgAAWxwRcfvZfIwzpkyZIldeeaUUK1ZMypYtK127dpVdu3Z5XZOSkiIDBgyQ0qVLS9GiRaV79+5y+PDhwL53Ag0AAHKmRsPlZ/PF+vXrTRDx5ZdfyurVqyUtLU1uuOEGSU5O9lwzdOhQWb58ubzzzjvm+oMHD8ptt90W8PfP1AkAAHnMypUrvR7Pnz/fZDa2bNkibdu2lePHj8srr7wiixYtkuuuu85cM2/ePKlbt64JTq6++uqAjYWMBgAA1pe3On62c12dOHHCq505cyZbQ9DAQpUqVcp81YBDsxwdOnTwXFOnTh2pXLmybNq0KaBvn0ADAACbHH+DjD+KSWNjY6V48eKeprUYf8XtdsuQIUOkdevW0qBBA3MsMTFRIiMjpUSJEl7XlitXzpwLJKZOAAAIEQkJCRIdHe15XLBgwb98jtZqfP/99/L5559LMBBoAABgk1srQgPQh4gJMjIHGn9l4MCBsmLFCtmwYYNUqlTJczwmJkZSU1Pl2LFjXlkNXXWi5wKJqRMAAPLYqhPHcUyQsWTJElm7dq1Uq1bN63yzZs2kQIECsmbNGs8xXf66b98+admypQQSGQ0AAPKYAQMGmBUlH3zwgbmXRkbdhdZ1FCpUyHzt06ePDBs2zBSIapbkoYceMkFGIFecKAINAADy2J1BZ8+ebb62a9fO67guYb3//vvN99OnT5eIiAhzoy5dvdKpUyd54YUXJNAINAAAyGOBhpON66OiouT55583zSZqNAAAgDVkNAAAsMkJ703VCDQAAAiR5a2hiEADAACLXJewPDWrPkIVNRoAAMAaMhoAANjkUKMBAABscTs69+F/HyGKqRMAAGANGQ0AAGxymDoBAADWOAEIFEI30GDqBAAAWENGAwAAmxymTgAAgC1uDRJYdQIAABBwZDQAALDJcZ9r/vYRogg0AACwyaFGAwAA2OKmRgMAAMAKMhoAANjkMHUCAABscQIQKIRunMHUCQAAsIeMBgAANjlMnQAAAFvceg8MdwD6CE1MnQAAAGvIaAAAYJPD1AkAALDFCe9Ag6kTAABgDRkNAABscof3LcgJNAAAsMhx3Kb520eoItAAAMAmx/E/I0GNBgAAwIXIaAAAYJMTgBqNEM5oEGgAAGCT2y3i8rPGIoRrNJg6AQAA1pDRAADAJoepEwAAYInjdovjCt/lrUydAAAAa8hoAABgk8PUCQAAsMXtiLjCN9Bg6gQAAFhDRgMAAJsczUa4wzajQaABAIBFjtsRx8+pE4dAAwAAZMnRbAZ3BgUAAAg4MhoAAFjkMHUCAACsccJ76oRAw8/o8qyk+X0fFiC3OnEydP9xA/7KiSR3jmQLzgbgc8L0EaIINC7RyZMnzdfP5aNgDwWwpmStYI8AyJl/z4sXLx7wfiMjIyUmJkY+TwzM54T2pX2GGpcTyhM/QeR2u+XgwYNSrFgxcblcwR5OnnfixAmJjY2VhIQEiY6ODvZwgIDjdzzn6cefBhkVKlSQiAg7ayNSUlIkNTU1IH1pkBEVFSWhhozGJdJfykqVKgV7GGFH/wHmH2HkZfyO5ywbmYzMoqKiQjI4CCSWtwIAAGsINAAAgDUEGggJBQsWlPHjx5uvQF7E7zjyKopBAQCANWQ0AACANQQaAADAGgINAABgDYEGQkrVqlVlxowZwR4GACCbCDSQI+6//35zB9Unn3zS6/jSpUt9urPq119/Lf369bMwQsD/3+/z24033phjY5gwYYI0adIkx14PyC4CDeQYvTveU089Jb/99tsl91GmTBkpXLhwQMcFBIIGFYcOHfJqb775ZrCHBQQdgQZyTIcOHcymQFOmTLnoNe+9957Ur1/f3EtAp0mmTZt20akTXZmtf8VVrlzZXK/7FQwaNMhz7ZkzZ2TEiBFSsWJFKVKkiLRo0UI+/fRTi+8Q4Ux/B/X3O3MrWbKk3H333XLnnXd6XZuWliaXXXaZLFiwwLN3kv5/Ua1aNSlUqJA0btxY3n33Xc/1+nurGZI1a9ZI8+bNTbDdqlUr2bVrlzk/f/58mThxomzfvt2TTdFjQG5AoIEcky9fPpk8ebLMmjVL9u/ff8H5LVu2SI8ePeSuu+6S7777zgQRY8eOveg/mBqUTJ8+XebOnSt79uwx0zANGzb0nB84cKBs2rRJFi9eLN9++63ccccd5q9OvRbIKXFxcbJ8+XJJSkryHFu1apWcOnVKunXrZh5rkKFBx5w5c2THjh0ydOhQueeee2T9+vVefT3yyCMm+N68ebPkz59fevfubY5rIDN8+HATpGdkU84PboCg0Rt2Abb16tXL6dKli/n+6quvdnr37m2+X7Jkid4wznx/9913Ox07dvR63siRI5169ep5HlepUsWZPn26+X7atGlOrVq1nNTU1Ate75dffnHy5cvnHDhwwOv49ddf74wZM8bCO0S4/37r71uRIkW82hNPPOGkpaU5l112mbNgwQLP9T179nTuvPNO831KSopTuHBhZ+PGjV599unTx1yn1q1bZ/4/+eSTTzznP/zwQ3Ps9OnT5vH48eOdxo0b59A7BrKPjAZynNZpvPbaa7Jz506v4/q4devWXsf0sWYg0tPTL+hHMxSnT5+W6tWrS9++fWXJkiVy9uxZc04zIvqcWrVqSdGiRT1N/0L86aefLL9DhKP27dvLtm3bvNqDDz5oMg+aqVu4cKG5Ljk5WT744AOT6VB79+412Y2OHTt6/a5qhuP839VGjRp5vi9fvrz5euTIkRx9n4Cv2CYeOa5t27bSqVMnGTNmjKnWv1SxsbFmjvqTTz6R1atXS//+/WXq1KkmmNA0tU7V6HSMfs1M/xEHAk3rgGrWrJnlOQ0qrr32WhMU6O+q1mFkrEjJmFL58MMPTT1RZufve1KgQAHP9xmrtbS+A8jNCDQQFLrMVZfi1a5d23Osbt268sUXX3hdp481K3F+sJBB/8Hu3LmzaQMGDJA6deqYbEbTpk1NRkP/Yb/mmmusvx/gz2jhpgbGb731lnz88ccmG5cRNNSrV88EFPv27TPByKWKjIzMMvMHBBuBBoJCizb1r7yZM2d6jmkx25VXXimTJk0yhWxayPncc8/JCy+8kGUfWiSq/7DqahKtwn/jjTdM4FGlShUpXbq06f++++4zxXMaePz666+mal/Tz7fccksOvluEA13llJiY6HVMp010dYnS1Sda7Ll7925Zt26d55pixYqZ1VFaAKrZiTZt2sjx48dNkB0dHS29evXK1uvriqz4+HgzZVOpUiXTLzvBIlfwoZ4DCEgxaIb4+HgnMjLSUwyq3n33XVP8WaBAAady5crO1KlTvZ6TuRhUC0lbtGjhREdHm8I7LTLNXCynRaLjxo1zqlatavorX768061bN+fbb7+1/n4Rfr/f+nt8fqtdu7bnmh9++MEc099ht9vt9Xx9PGPGDHO9/q6WKVPG6dSpk7N+/XqvYtDffvvN85xvvvnGHNP/jzKKSrt37+6UKFHCHJ83b16OvX/gz7BNPAAAsIZVJwAAwBoCDQAAYA2BBgAAsIZAAwAAWEOgAQAArCHQAAAA1hBoAAAAawg0AACANQQaQAjTTem6du3qedyuXTsZMmRIjo/j008/NZt8HTt27KLX6PmlS5dmu88JEyaY/XD88Z///Me8rt6WG0BwEGgAFj789cNNm250pTt6PvbYY54t7G16//33zV4xgQoOAMBfbKoGWKBbgM+bN89stPXRRx+ZnWV1t84xY8ZccG1qaqoJSAKhVKlSAekHAAKFjAZgge6aGRMTY3aS/ec//ykdOnSQZcuWeU13PPHEE1KhQgWpXbu2OZ6QkCA9evSQEiVKmIChS5cuJvWfQXeqHTZsmDmvu9OOGjVKd6Pzet3zp0400Hn44YfNFuU6Js2uvPLKK6bf9u3bm2tKlixpMhs6LqU7iE6ZMkWqVatmdsNt3LixvPvuu16vo8FTrVq1zHntJ/M4s0vHpX3ozrvVq1eXsWPHSlpa2gXXzZ0714xfr9Ofj+5smtnLL78sdevWlaioKKlTp85Fd/sFEBwEGkAO0A9kzVxk0O3qd+3aJatXr5YVK1aYD9hOnTqZrb0/++wzs0V40aJFTWYk43m63f38+fPl1Vdflc8//1yOHj0qS5Ys+dPXve++++TNN9+UmTNnys6dO82HtvarH9zvvfeeuUbHcejQIfnXv/5lHmuQsWDBArOl+Y4dO8z25ffcc4+sX7/eExDddttt0rlzZ1P78Pe//11Gjx7t889E36u+nx9++MG89ksvvSTTp0/3umbv3r3y9ttvy/Lly2XlypXyzTffSP/+/T3nFy5cKOPGjTNBm76/yZMnm4Dltdde83k8ACz5071dAVzSluFdunTxbP+9evVqp2DBgs6IESM858uVK+ecOXPG85zXX3/dbBGeeftwPV+oUCFn1apV5rFuc//00097zqelpTmVKlXyvJa69tprncGDB5vvd+3aZbYL19fPSlZbj+tW44ULF3Y2btzodW2fPn2cnj17mu/HjBnj1KtXz+v8ww8/fEFf59PzS5Ysuej5qVOnOs2aNfM8Hj9+vJMvXz5n//79nmMff/yxExER4Rw6dMg8rlGjhrNo0SKvfiZNmuS0bNnSfK9bqOvr6pbqAIKDGg3AAs1SaOZAMxU6FXH33XebVRQZGjZs6FWXsX37dvPXu/6Vn1lKSor89NNPZrpAsw4tWrTwnMufP780b978gumTDJptyJcvn1x77bXZHreO4dSpU9KxY0ev45pVadq0qfleMweZx6FatmwpvnrrrbdMpkXfX1JSkimWjY6O9rqmcuXKUrFiRa/X0Z+nZmH0Z6XP7dOnj/Tt29dzjfZTvHhxn8cDwA4CDcACrVuYPXu2CSa0DkODgsyKFCni9Vg/aJs1a2amAs5XpkyZS56u8ZWOQ3344YdeH/BKazwCZdOmTRIXFycTJ040U0YaGCxevNhMD/k6Vp1yOT/w0QALQO5AoAFYoIGEFl5m1xVXXGH+wi9btuwFf9VnKF++vHz11VfStm1bz1/uW7ZsMc/NimZN9K9/ra3QYtTzZWRUtMg0Q7169UxAsW/fvotmQrTwMqOwNcOXX34pvti4caMplH3kkUc8x3755ZcLrtNxHDx40ARrGa8TERFhCmjLlStnjv/8888maAGQO1EMCuQC+kF52WWXmZUmWgwaHx9v7nMxaNAg2b9/v7lm8ODB8uSTT5qbXv3444+mKPLP7oFRtWpV6dWrl/Tu3ds8J6NPLa5U+kGvq010mufXX381GQKdjhgxYoQpANWCSp2a2Lp1q8yaNctTYPnggw/Knj17ZOTIkWYKY9GiRaao0xeXX365CSI0i6GvoVMoWRW26koSfQ86taQ/F/156MoTXdGjNCOixav6/N27d8t3331nlhU/++yzPo0HgD0EGkAuoEs3N2zYYGoSdEWHZg209kBrNDIyHMOHD5d7773XfPBqrYIGBd26dfvTfnX65vbbbzdBiS791FqG5ORkc06nRvSDWleMaHZg4MCB5rje8EtXbugHuI5DV77oVIoud1U6Rl2xosGLLn3V1Sm62sMXt956qwlm9DX17p+a4dDXPJ9mhfTncfPNN8sNN9wgjRo18lq+qitedHmrBheawdEsjAY9GWMFEHwurQgN9iAAAEDeREYDAABYQ6ABAACsIdAAAADWEGgAAABrCDQAAIA1BBoAAMAaAg0AAGANgQYAALCGQAMAAFhDoAEAAKwh0AAAAGLL/weZDgzrkdHBfgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(test_labels, test_predictions, display_labels=[\"Noise\", \"Event\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because this is a binary classification task, we have the false negative (miss rate) in the upper right corner, and the false positive (false alarm) rate in the lower left corner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training our model, we need to quantize it for execution on the ESP32, transforming its weights and activations from floats to 8-bit integers. We will use the [LiteRT library](https://ai.google.dev/edge/litert/models/post_training_integer_quant) for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before quantizing, let's define a function to evaluate our model after quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O2PujvegEnwr"
      },
      "outputs": [],
      "source": [
        "def evaluate(interpreter):\n",
        "    prediction = []\n",
        "    #prediction_quantized = []\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    input_format = interpreter.get_output_details()[0]['dtype']\n",
        "    input_details = interpreter.get_input_details()[0]\n",
        "\n",
        "    for i, x in enumerate(X_test):\n",
        "        if i % 100 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "\n",
        "        print(\"Non-quantized input:\")\n",
        "        print(x)\n",
        "\n",
        "        # Quantize the input to uint8\n",
        "        if input_details['dtype'] == np.uint8:\n",
        "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "            x = x / input_scale + input_zero_point\n",
        "\n",
        "        x = np.expand_dims(x, axis=0).astype(input_format)\n",
        "        print(\"Quantized input: \")\n",
        "        print(x)\n",
        "\n",
        "        interpreter.set_tensor(input_index, x)\n",
        "\n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Gather and dequantize output\n",
        "        output = interpreter.tensor(output_index)\n",
        "        print(\"Quantized output:\")\n",
        "        print(output()[0])\n",
        "        predicted_label = np.round(output()[0]/255)\n",
        "        print(\"De-quantized output:\")\n",
        "        print(predicted_label)\n",
        "        print(\"-----------------------------\")\n",
        "        prediction.append(predicted_label)\n",
        "\n",
        "    print('\\n')\n",
        "    # Comparing prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction = np.array(prediction)\n",
        "    #prediction_quantized = np.array(prediction_quantized)\n",
        "    accuracy = (prediction == y_test).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INT8 post-training quantization requires a representative dataset. We will use 100 samples from our training set for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gX7TN3GOEsog"
      },
      "outputs": [],
      "source": [
        "# Defining the representative dataset from the testing data\n",
        "def representative_data_gen():\n",
        "    for data in tf.data.Dataset.from_tensor_slices((X_test)).batch(1).take(100):\n",
        "        #yield [tf.dtypes.cast(data, tf.float32)]\n",
        "        yield [tf.dtypes.cast(data, tf.float32)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load our previously trained model and quantize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IOx96McfEu9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\samuk\\AppData\\Local\\Temp\\tmpueebiu5a\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\samuk\\AppData\\Local\\Temp\\tmpueebiu5a\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\samuk\\AppData\\Local\\Temp\\tmpueebiu5a'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2622187759632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622187759824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622187762128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185932176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185931600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185935056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185932368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185935632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185935440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185936208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185933712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185937360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185936016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2622185938512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\samuk\\anaconda3\\envs\\sbrc\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Our converter\n",
        "model_to_convert = tf.keras.models.load_model(\"best_model.keras\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_to_convert)\n",
        "\n",
        "# Set the representative dataset for post-training quantization.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Using Integer Quantization.\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Setting the input and output tensors to uint8.\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Converting the model\n",
        "int_quant_model = converter.convert()\n",
        "\n",
        "# Saving the Integer Quantized TF Lite model.\n",
        "with open('int8_quant_mlp.tflite', 'wb') as f:\n",
        "    f.write(int_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's evaluate our model performance after quantization. Our evaluator will print both inputs and outputs before and after quantization, just to illustrate how the process works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHR2qSk1Ex2r",
        "outputId": "c5757e39-5f8d-4c86-cdba-2f17ff935ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Non-quantized input:\n",
            "[ 62.364 -11.65  -24.387 -14.616   0.821  12.287   1.48   -7.877  -0.348\n",
            "  -0.911   0.792   0.339   1.368   1.802   1.056  -0.108  -1.011  -0.823\n",
            "   0.381  -2.822   1.313  -0.457  -1.102   0.009   0.799  -0.167   0.863\n",
            "   0.13   -1.642   0.869   0.132   0.448   0.03   -0.493  -0.025  -0.261\n",
            "   1.968  -0.259   1.504   0.063  -1.455   0.34 ]\n",
            "Quantized input: \n",
            "[[213  61  34  54  86 110  88  68  84  83  86  85  87  88  87  84  82  83\n",
            "   85  79  87  84  82  85  86  84  86  85  81  86  85  85  85  83  84  84\n",
            "   89  84  88  85  82  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.073 -24.319   0.104  -6.926 -18.911 -10.352   0.826   6.256  -2.053\n",
            "  -0.903  -0.397   0.782  -1.34    0.076   1.253   1.492  -0.846  -0.434\n",
            "   0.581  -1.993   1.832  -0.452   1.773   0.424  -0.385  -0.122   1.878\n",
            "   1.034   0.055  -1.227  -0.853  -0.014  -1.197   0.207  -0.862   1.047\n",
            "   1.833  -0.163  -0.347   1.061  -0.736  -0.812]\n",
            "Quantized input: \n",
            "[[212  34  85  70  46  63  86  97  80  83  84  86  82  85  87  88  83  84\n",
            "   86  80  88  84  88  85  84  84  88  87  85  82  83  84  82  85  83  87\n",
            "   88  84  84  87  83  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.562 -19.578  16.47   18.745   2.292   2.198 -10.486  -8.179   2.233\n",
            "  -0.742   0.471  -1.441  -0.873  -1.845  -0.782   1.417   0.03    0.258\n",
            "  -0.733  -1.657   0.869   1.941   1.324  -0.572   2.567   0.11    0.358\n",
            "  -0.379  -0.889   0.517  -1.119  -0.539   0.192  -0.625  -2.128   0.543\n",
            "   0.134  -1.49   -0.537   1.751  -0.318   0.505]\n",
            "Quantized input: \n",
            "[[213  44 118 123  89  89  63  68  89  83  85  82  83  81  83  87  85  85\n",
            "   83  81  86  88  87  83  90  85  85  84  83  86  82  83  85  83  80  86\n",
            "   85  81  83  88  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.516   9.136 -25.507  12.192  10.337  -6.8    -9.302   6.245  -1.305\n",
            "  -1.126  -0.64   -0.797   0.752  -0.604  -0.568  -2.127  -0.779  -1.419\n",
            "  -0.207  -2.979   0.193   0.805  -0.106  -0.864  -0.64   -0.853  -1.436\n",
            "   0.532   0.265  -0.219  -0.181   0.555  -0.773   1.292   1.211   0.098\n",
            "   1.112  -0.797   0.335   0.497  -0.602   0.071]\n",
            "Quantized input: \n",
            "[[213 103  32 110 106  71  65  97  82  82  83  83  86  83  83  80  83  82\n",
            "   84  78  85  86  84  83  83  83  82  86  85  84  84  86  83  87  87  85\n",
            "   87  83  85  86  83  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.701  22.007  -2.545  16.439 -16.437  -2.06    7.511  -7.891  -1.557\n",
            "  -1.519   0.998   0.775  -1.425  -1.144  -1.796  -0.706  -0.256  -0.993\n",
            "   0.628  -2.454  -0.888  -1.323   0.293  -0.882   1.068   1.484  -0.927\n",
            "  -0.68    0.624   0.097   0.39   -0.384  -1.339   0.389   0.187  -1.369\n",
            "   0.655  -0.311  -0.384   0.667   0.01    0.079]\n",
            "Quantized input: \n",
            "[[214 130  79 118  51  80 100  68  81  81  87  86  82  82  81  83  84  82\n",
            "   86  79  83  82  85  83  87  88  83  83  86  85  85  84  82  85  85  82\n",
            "   86  84  84  86  85  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 61.987  18.917  16.226 -12.621  -9.68   10.246 -12.857   4.144  -2.461\n",
            "  -2.004   0.473  -1.375  -0.57    1.286  -1.812   2.563  -0.027  -0.603\n",
            "   1.599  -1.958  -2.421   0.484  -0.571   1.217  -0.716   0.22    1.628\n",
            "  -0.505   0.599   0.229   0.417   0.701  -1.025  -0.12    0.719   0.588\n",
            "   0.832   0.506  -0.631   1.06    1.099  -0.223]\n",
            "Quantized input: \n",
            "[[212 123 118  59  65 106  58  93  79  80  85  82  83  87  81  90  84  83\n",
            "   88  80  80  85  83  87  83  85  88  83  86  85  85  86  82  84  86  86\n",
            "   86  86  83  87  87  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 63.192   7.269  13.4   -20.997  16.073 -16.532   0.392  -9.23   -1.582\n",
            "  -0.391  -0.346   0.221   0.775  -0.885   2.634  -0.075  -1.978   2.095\n",
            "   0.486  -1.497  -2.942   0.369   0.456  -0.388  -0.047  -0.229   0.944\n",
            "  -0.552  -0.659   0.376   0.157   1.75   -1.07    0.233  -1.055   0.324\n",
            "   0.632   1.533   0.927   0.181   0.123   1.149]\n",
            "Quantized input: \n",
            "[[215  99 112  41 118  50  85  66  81  84  84  85  86  83  90  84  80  89\n",
            "   85  81  78  85  85  84  84  84  86  83  83  85  85  88  82  85  82  85\n",
            "   86  88  86  85  85  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 37.836 -16.094 -41.337 -16.697  -7.565   5.967  -4.845  -5.787  -5.403\n",
            "  -3.088  -3.767   2.835  -1.084  -2.834   1.801  -2.112   3.999  -0.893\n",
            "   0.836   2.369  -1.771   0.654   0.128  -4.098  -0.394   0.343   0.93\n",
            "   0.64    2.53   -0.312   0.773   0.421   1.134  -1.71   -4.091  -0.532\n",
            "   0.814  -0.607  -0.668   2.735  -1.286  -0.143]\n",
            "Quantized input: \n",
            "[[162  51   0  50  69  97  75  73  73  78  77  90  82  79  88  80  93  83\n",
            "   86  89  81  86  85  76  84  85  86  86  90  84  86  85  87  81  76  83\n",
            "   86  83  83  90  82  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 37.99  -39.727  -6.88   -5.836 -24.208  -9.342  -7.752  -3.346  -2.062\n",
            "  -3.48    3.559  -5.21    0.33    0.952   1.296  -0.787   4.129   0.134\n",
            "  -2.982  -0.545  -1.374   1.766  -3.32    2.653  -0.411   1.035  -0.652\n",
            "  -0.886   2.646   1.417   2.745   1.127   0.091   3.034  -1.762   2.666\n",
            "  -0.07   -1.236  -1.704   0.279   0.005  -0.99 ]\n",
            "Quantized input: \n",
            "[[163   3  70  72  35  65  69  78  80  77  92  74  85  86  87  83  93  85\n",
            "   78  83  82  88  78  90  84  87  83  83  90  87  90  87  85  91  81  90\n",
            "   84  82  81  85  85  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 37.249 -35.821  19.502  23.48   -0.959   3.016 -12.066  -7.11   -8.326\n",
            "  -3.11   -0.468   5.253   1.125   0.517   0.11    0.95    4.     -2.55\n",
            "   0.687   1.204  -0.214  -5.585  -0.565   0.502   1.136  -2.214  -1.833\n",
            "   0.719  -0.642   0.688   2.543  -1.911  -2.108   0.965  -1.022  -0.358\n",
            "  -2.915   0.221   0.317   0.002  -0.179   1.   ]\n",
            "Quantized input: \n",
            "[[161  11 125 133  83  91  60  70  67  78  84  95  87  86  85  86  93  79\n",
            "   86  87  84  73  83  86  87  80  81  86  83  86  90  81  80  86  82  84\n",
            "   79  85  85  85  84  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 37.288  18.942 -41.108  12.803   5.209  -3.764 -11.561  -3.218  -0.7\n",
            "  -3.188   5.423  -0.722  -2.5     3.336  -0.384  -0.975   4.084   0.775\n",
            "   3.89    2.541   0.862   1.128   1.246   1.025   3.47    0.684   1.187\n",
            "   1.112   0.286   0.169  -2.269  -0.613   3.412  -2.151  -0.027   1.519\n",
            "   2.368   2.652  -0.899   0.257   0.242  -0.803]\n",
            "Quantized input: \n",
            "[[161 123   0 111  95  77  61  78  83  78  96  83  79  91  84  82  93  86\n",
            "   93  90  86  87  87  87  92  86  87  87  85  85  80  83  92  80  84  88\n",
            "   89  90  83  85  85  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 36.301  40.234  -5.568  17.596 -19.278  -4.955  -1.355  -3.813  -4.418\n",
            "  -3.643  -4.025  -1.845   1.117   2.236   0.101  -2.657   2.216   5.623\n",
            "   0.456  -0.273   2.711   2.423  -1.004   0.028   0.289  -2.76    2.546\n",
            "  -0.295   1.641  -4.371  -0.168  -0.943  -1.558   1.756  -0.395   0.312\n",
            "  -1.882   2.629   4.469  -2.13    1.045  -0.633]\n",
            "Quantized input: \n",
            "[[159 167  73 121  45  74  82  77  75  77  76  81  87  89  85  79  89  96\n",
            "   85  84  90  89  82  85  85  79  90  84  88  76  84  83  81  88  84  85\n",
            "   81  90  94  80  87  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 36.514  34.923  25.734 -17.639 -11.946   5.363 -11.914  -6.012  -4.625\n",
            "  -2.056   3.971   2.147   1.604  -2.924  -2.378  -0.185   2.071   3.212\n",
            "  -0.162  -0.578   3.593   1.588   1.777  -1.771   1.618   1.111   1.084\n",
            "  -2.968  -3.264  -0.858   3.216   1.546   2.241   0.513   0.083   0.066\n",
            "  -1.575  -3.594  -1.783  -2.118  -0.043   1.93 ]\n",
            "Quantized input: \n",
            "[[160 156 137  48  60  96  60  72  75  80  93  89  88  78  80  84  89  91\n",
            "   84  83  92  88  88  81  88  87  87  78  78  83  91  88  89  86  85  85\n",
            "   81  77  81  80  84  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 42.292  14.322  25.046 -31.533  17.594 -18.243  -7.416  -8.331  -3.028\n",
            "  -1.484  -2.051  -3.881   1.041  -4.322   5.847   2.704   0.213   6.278\n",
            "   2.088  -2.863   3.492   0.905   2.499   1.721  -4.02    3.488  -2.049\n",
            "   0.367   0.29   -0.186  -0.681  -3.066   1.467   0.243   0.032   0.087\n",
            "  -1.023   0.282  -1.318   1.577   2.171  -2.031]\n",
            "Quantized input: \n",
            "[[172 114 136  20 121  47  69  67  78  81  80  77  87  76  97  90  85  97\n",
            "   89  79  92  86  90  88  76  92  80  85  85  84  83  78  88  85  85  85\n",
            "   82  85  82  88  89  80]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 58.18   -9.986 -19.209 -17.572  -3.245  12.582   4.637 -11.447  -1.601\n",
            "  -1.735  -2.244   0.326  -3.687  -1.463   0.349   2.356  -1.867   1.978\n",
            "   1.784  -0.251  -2.84    1.206  -1.728   0.781   0.688  -0.797  -2.134\n",
            "  -0.523   1.955   0.385  -0.69   -0.482   0.607  -1.219  -0.523   0.479\n",
            "   1.812  -0.303  -1.648   0.928  -0.655   0.226]\n",
            "Quantized input: \n",
            "[[204  64  45  48  78 110  94  61  81  81  80  85  77  81  85  89  81  89\n",
            "   88  84  79  87  81  86  86  83  80  83  89  85  83  84  86  82  83  85\n",
            "   88  84  81  86  83  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 57.237 -19.524   3.181  -3.572 -18.91  -14.931  -2.313   8.492  -1.706\n",
            "  -1.194   2.752  -2.208   0.944  -4.079  -2.556   0.011  -0.556   0.283\n",
            "   1.42   -0.601  -1.143  -2.099  -2.394  -1.67   -1.183   1.769  -1.15\n",
            "   1.088   1.515   1.563   1.093   1.387   0.091  -0.464  -0.555   0.004\n",
            "   0.423  -0.444   1.437   1.463   0.239  -0.012]\n",
            "Quantized input: \n",
            "[[202  44  91  77  46  54  80 102  81  82  90  80  86  76  79  85  83  85\n",
            "   87  83  82  80  80  81  82  88  82  87  88  88  87  87  85  84  83  85\n",
            "   85  84  87  88  85  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 56.202 -13.805  14.449  17.727   6.846   4.516 -11.148 -12.854   2.052\n",
            "  -0.78   -1.835   2.242   2.499   1.377  -3.015  -2.603  -0.878   1.289\n",
            "  -2.164  -2.49    0.552  -0.93    0.944   1.228  -2.452   0.97   -0.757\n",
            "  -0.603   0.607   2.01    2.121  -0.59   -0.804   1.783   0.732   0.552\n",
            "   0.893  -0.392  -1.455   0.012  -0.044  -0.457]\n",
            "Quantized input: \n",
            "[[200  56 114 121  99  94  62  58  89  83  81  89  90  87  78  79  83  87\n",
            "   80  79  86  83  86  87  79  86  83  83  86  89  89  83  83  88  86  86\n",
            "   86  84  82  85  84  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 59.458   7.753 -23.615   6.676  13.502  -5.306 -13.954   7.749  -1.579\n",
            "  -0.924   2.467   2.194   1.157   2.684   1.589   1.879  -2.232   2.361\n",
            "  -1.541   0.708  -2.911   0.221   2.8    -0.508  -0.083  -0.602   0.63\n",
            "   0.415   1.289  -2.014  -0.737  -0.639  -0.653  -1.495  -0.46    0.159\n",
            "   2.071   1.012   0.261   1.212  -1.404   0.389]\n",
            "Quantized input: \n",
            "[[207 100  36  98 112  74  56 100  81  83  90  89  87  90  88  88  80  89\n",
            "   81  86  79  85  90  83  84  83  86  85  87  80  83  83  83  81  84  85\n",
            "   89  87  85  87  82  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 61.109  20.234  -4.522  16.678 -13.152  -7.94    9.8   -10.685  -3.607\n",
            "  -0.902  -0.84   -3.342   2.602   0.005   1.57   -2.12   -2.464   0.519\n",
            "  -1.443   1.411  -0.298  -2.059   0.568   1.948   1.642   0.406   1.483\n",
            "  -0.564  -1.893  -0.619   0.078   0.562   0.73   -0.409   0.646   0.86\n",
            "   1.461   0.782   1.608  -0.034   0.34   -0.804]\n",
            "Quantized input: \n",
            "[[210 126  75 119  57  68 105  63  77  83  83  78  90  85  88  80  79  86\n",
            "   82  87  84  80  86  89  88  85  88  83  81  83  85  86  86  84  86  86\n",
            "   88  86  88  84  85  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.099  18.799  14.106  -9.645 -13.418  13.495 -13.898   4.318  -2.015\n",
            "  -0.533   0.356   1.557  -2.111  -1.982  -0.838  -1.917  -1.103  -0.931\n",
            "  -1.838   1.18    1.48    1.137  -1.467   0.707  -0.727  -0.571   0.905\n",
            "  -0.138  -0.259  -0.509  -1.118   0.52    0.825   1.938   0.993   0.312\n",
            "  -0.235  -0.533   0.711   0.425   0.409   1.   ]\n",
            "Quantized input: \n",
            "[[212 123 114  65  57 112  56  93  80  83  85  88  80  80  83  81  82  83\n",
            "   81  87  88  87  81  86  83  83  86  84  84  83  82  86  86  88  87  85\n",
            "   84  83  86  85  85  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.093   8.594  13.017 -22.103  15.056 -17.112  -1.113  -8.68   -2.771\n",
            "  -0.594   1.114  -2.088  -1.565   2.191  -1.472   3.469   0.876  -1.534\n",
            "  -0.225   0.83    1.971   0.537   0.792  -2.277   0.563   0.282  -0.481\n",
            "   0.409  -0.984  -0.058  -0.991   0.23    0.132  -0.279   0.203  -1.573\n",
            "  -1.485  -0.969  -0.592   1.097  -0.096   0.055]\n",
            "Quantized input: \n",
            "[[212 102 111  39 115  49  82  67  79  83  87  80  81  89  81  92  86  81\n",
            "   84  86  89  86  86  80  86  85  84  85  82  84  82  85  85  84  85  81\n",
            "   81  83  83  87  84  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 70.924  -8.049 -11.374  -8.958  -2.843  12.213   5.714  -9.997  -0.405\n",
            "   0.444   0.814  -8.09    3.617  -8.417  -8.237   1.895  -4.707   2.245\n",
            "  -0.109   0.482   1.958   0.829   2.274   2.412  -0.859  -1.892  -1.733\n",
            "  -2.341   1.893   0.015   0.953   0.447   1.416   0.421   0.243   1.242\n",
            "  -0.523   1.506   1.877  -0.815   0.534   0.087]\n",
            "Quantized input: \n",
            "[[230  68  61  66  79 110  96  64  84  85  86  68  92  67  68  88  75  89\n",
            "   84  85  89  86  89  89  83  81  81  80  88  85  86  85  87  85  85  87\n",
            "   83  88  88  83  86  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 70.667 -14.091   1.735   0.692 -11.176 -11.09    1.015  10.814  -3.264\n",
            "   0.975   2.145   5.634   6.439   4.364  -8.909  -4.456  -4.784   5.827\n",
            "   0.238   0.462   1.192  -0.191   0.127  -1.138   3.079  -1.031   0.21\n",
            "   1.088   0.18   -0.226   2.487  -0.281  -0.801  -3.156   1.275  -0.242\n",
            "   0.24    1.854  -0.338  -0.687   0.977   0.881]\n",
            "Quantized input: \n",
            "[[230  56  88  86  62  62  87 107  78  87  89  96  98  93  66  75  75  96\n",
            "   85  85  87  84  85  82  91  82  85  87  85  84  90  84  83  78  87  84\n",
            "   85  88  84  83  87  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 69.876 -10.962   9.097  12.124   8.556   5.9    -5.061  -8.689   6.45\n",
            "  -0.438  -4.494  -1.466  -6.392   7.778   0.397  -5.547  -5.381   4.383\n",
            "   5.752   0.495  -0.739   1.614   0.65   -0.526  -0.199  -0.35    0.375\n",
            "   2.995   2.147  -1.142  -0.381   2.179  -0.247   1.205  -2.263  -2.261\n",
            "   2.396   0.839   0.407  -1.271  -0.163   0.093]\n",
            "Quantized input: \n",
            "[[228  62 103 109 102  97  74  67  98  84  75  81  71 101  85  73  73  94\n",
            "   96  86  83  88  86  83  84  84  85  91  89  82  84  89  84  87  80  80\n",
            "   89  86  85  82  84  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 70.685   3.05  -14.469   4.19    9.316  -3.453 -11.234   9.567  -0.192\n",
            "   0.264  -1.968   3.474  -9.903  -6.098  -0.132   6.928  -6.442   2.715\n",
            "  -1.715  -0.104   1.862  -2.933  -0.714  -0.459  -2.377   1.79    0.213\n",
            "  -0.426   0.138  -2.906   1.527   1.344   1.061  -1.23   -1.113  -0.958\n",
            "  -1.545  -0.305   1.326  -1.255   0.064   0.956]\n",
            "Quantized input: \n",
            "[[230  91  55  93 104  77  61 104  84  85  80  92  64  72  84  99  71  90\n",
            "   81  84  88  78  83  84  80  88  85  84  85  79  88  87  87  82  82  83\n",
            "   81  84  87  82  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[70.925 11.814 -4.197 10.212 -7.99  -6.074 11.623 -8.395 -1.411  0.241\n",
            "  3.517  1.889  2.621  8.037  3.99  10.278 -5.764 -1.446 -2.139 -0.129\n",
            " -1.176  0.044 -2.432  0.478 -0.692  0.385 -2.329  0.19  -2.249 -0.307\n",
            "  1.001 -1.263  3.123 -2.219  0.234  0.529  0.114 -0.435 -0.355 -1.543\n",
            "  1.119 -0.96 ]\n",
            "Quantized input: \n",
            "[[230 109  76 106  68  72 108  67  82  85  92  88  90 101  93 106  73  82\n",
            "   80  84  82  85  79  85  83  85  80  85  80  84  87  82  91  80  85  86\n",
            "   85  84  84  81  87  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[71.322 12.362  7.621 -5.062 -7.507 12.573 -9.215  6.908  0.404  0.894\n",
            " -3.395 -3.732  6.993 -0.415  7.404 -5.441 -7.693 -4.257 -4.723  1.394\n",
            " -3.838 -1.368  1.268 -1.14  -0.43  -1.478 -2.738  1.626  0.507 -1.889\n",
            " -0.143 -1.928  1.52   0.104  0.863  0.941  0.633 -0.634  1.077 -1.373\n",
            "  0.268  2.018]\n",
            "Quantized input: \n",
            "[[231 110 100  74  69 110  66  99  85  86  78  77  99  84 100  73  69  76\n",
            "   75  87  77  82  87  82  84  81  79  88  86  81  84  81  88  85  86  86\n",
            "   86  83  87  82  85  89]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 72.597   6.282   9.015 -12.121  11.092 -12.588   0.719  -6.898  -4.25\n",
            "   0.424   2.033   0.808  -7.888   0.446  -6.183  -6.44   -1.79  -11.142\n",
            "  -3.577   3.529  -1.601  -3.117   1.21    2.385   2.738   0.442  -0.246\n",
            "   0.284  -1.237  -0.918   2.012  -0.477   1.057   0.874   0.204  -1.281\n",
            "  -0.33   -2.119   1.438   1.033   0.567  -0.262]\n",
            "Quantized input: \n",
            "[[234  97 103  60 107  59  86  70  76  85  89  86  68  85  72  71  81  62\n",
            "   77  92  81  78  87  89  90  85  84  85  82  83  89  84  87  86  85  82\n",
            "   84  80  87  87  86  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 68.499  -8.889 -13.615  -8.795  -1.515  12.063   5.209  -9.136   0.701\n",
            "   0.582   3.261  -5.541   5.525  -5.207  -7.618  -0.393  -3.133   1.181\n",
            "   0.799  -2.411   6.527  -0.909   2.119  -1.09   -0.347   0.503   0.14\n",
            "  -1.985   0.988  -1.057   0.628   2.427   0.922  -1.036   1.251   0.723\n",
            "  -0.073   0.891   2.129   0.2     1.036   0.101]\n",
            "Quantized input: \n",
            "[[225  66  56  66  81 109  95  66  86  86  91  73  96  74  69  84  78  87\n",
            "   86  80  98  83  89  82  84  86  85  80  87  82  86  89  86  82  87  86\n",
            "   84  86  89  85  87  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 69.208 -16.38    0.672  -0.079 -12.243  -9.399   1.498  10.688  -3.37\n",
            "   0.559  -1.123   5.79    3.48    6.254  -6.     -4.499  -3.331   3.617\n",
            "   0.419  -1.389   5.133   3.073   2.694   0.04    2.77   -2.387   1.719\n",
            "   0.073   0.714   0.249   1.195   1.199  -0.084  -0.416   0.623  -1.608\n",
            "   0.677   1.701   0.613   1.441   1.44    0.065]\n",
            "Quantized input: \n",
            "[[227  51  86  84  59  65  88 106  78  86  82  96  92  97  72  75  78  92\n",
            "   85  82  95  91  90  85  90  80  88  85  86  85  87  87  84  84  86  81\n",
            "   86  88  86  87  87  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 69.86  -13.911   9.243  14.605   6.227   6.389  -4.851  -7.287   6.005\n",
            "  -0.386  -1.258  -3.418  -6.936   5.518   1.88   -3.931  -3.556   1.416\n",
            "   6.249   1.159   1.343   3.053   0.781  -0.61    4.205  -1.414   4.054\n",
            "   3.143   1.476   0.129  -0.53    0.902   0.674   1.411   0.338  -0.849\n",
            "   2.816  -1.485  -0.592   0.961   1.33   -1.063]\n",
            "Quantized input: \n",
            "[[228  56 104 115  97  98  75  70  97  84  82  77  70  96  88  76  77  87\n",
            "   97  87  87  91  86  83  93  82  93  91  88  85  83  86  86  87  85  83\n",
            "   90  81  83  86  87  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 68.054   3.751 -15.855   6.148   8.823  -3.688  -9.853   9.61   -0.679\n",
            "   0.002  -3.625   0.08   -7.533  -6.836  -2.272   4.519  -4.944   1.336\n",
            "   1.098  -4.816   3.559  -0.297  -2.809  -0.139  -2.491   1.615  -1.602\n",
            "  -1.707  -0.808  -1.551   0.301  -0.798   1.834  -1.135   0.556  -1.643\n",
            "  -0.487   0.087   1.205  -1.39    0.566   0.688]\n",
            "Quantized input: \n",
            "[[225  92  52  97 103  77  64 104  83  85  77  85  69  70  80  94  74  87\n",
            "   87  75  92  84  79  84  79  88  81  81  83  81  85  83  88  82  86  81\n",
            "   83  85  87  82  86  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[67.614 12.769 -3.327 11.157 -8.909 -4.338 10.737 -8.358 -0.298  0.231\n",
            "  3.828  4.604 -0.297  4.633  1.806  9.136 -5.051 -0.863  0.109 -4.419\n",
            " -2.304 -0.267 -0.551 -2.393 -2.421 -0.043 -3.09   1.588 -1.303 -1.117\n",
            " -0.151 -2.099  2.128 -0.496  1.935 -0.682 -1.139 -0.339 -1.022 -1.059\n",
            "  1.708 -0.8  ]\n",
            "Quantized input: \n",
            "[[224 111  78 107  66  76 107  67  84  85  92  94  84  94  88 103  74  83\n",
            "   85  75  80  84  83  80  80  84  78  88  82  82  84  80  89  83  88  83\n",
            "   82  84  82  82  88  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[67.504 12.416  9.009 -5.973 -7.452 11.475 -9.416  7.336  0.879 -0.112\n",
            " -2.86  -4.593  6.876  2.274  5.322 -0.756 -6.316 -3.14  -1.99  -2.512\n",
            " -6.226 -3.303  2.144  1.401  1.66   0.359 -2.503  0.948 -0.212 -1.029\n",
            "  0.461 -1.526  0.444  0.156  0.787 -0.813 -1.176 -0.018  0.269 -1.377\n",
            "  1.333 -0.206]\n",
            "Quantized input: \n",
            "[[223 110 103  72  69 108  65 100  86  84  79  75  99  89  95  83  72  78\n",
            "   80  79  72  78  89  87  88  85  79  86  84  82  85  81  85  85  86  83\n",
            "   82  84  85  82  87  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 67.601   5.761   9.343 -13.306  11.884 -12.474   2.556  -6.552  -1.998\n",
            "   0.227   0.691   2.009  -4.246  -1.734  -3.59   -7.005  -3.697  -9.245\n",
            "  -3.657   0.916  -5.605  -3.159   0.706   4.176   0.028  -1.615   2.715\n",
            "   0.146  -0.647  -0.167   2.245   0.446   0.708   0.494   0.937   0.589\n",
            "  -0.635  -1.146  -1.208   0.565  -0.362   0.366]\n",
            "Quantized input: \n",
            "[[224  96 104  57 109  59  90  71  80  85  86  89  76  81  77  70  77  65\n",
            "   77  86  73  78  86  93  85  81  90  85  83  84  89  85  86  86  86  86\n",
            "   83  82  82  86  84  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 61.887 -13.353 -17.54  -16.543  -2.959  17.03    5.954 -12.436   1.259\n",
            "  -1.642   0.205   0.012  -1.2     4.233   6.575   0.88    2.815  -3.662\n",
            "   0.992  -3.213   0.142   2.409  -1.874  -0.39   -0.199   0.343  -1.484\n",
            "   1.129  -0.787  -1.383   1.378  -1.793   0.264  -0.6    -0.697  -1.081\n",
            "  -1.925  -5.468   0.313   0.878  -0.044   1.554]\n",
            "Quantized input: \n",
            "[[212  57  48  50  78 120  97  59  87  81  85  85  82  93  98  86  90  77\n",
            "   87  78  85  89  81  84  84  85  81  87  83  82  87  81  85  83  83  82\n",
            "   81  73  85  86  84  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.176 -21.44    4.787  -0.813 -19.398 -15.2     1.232  12.461  -3.391\n",
            "  -1.113   0.408   1.218  -1.661  -3.717   3.724   4.957   2.951  -5.333\n",
            "  -0.176  -1.762   3.105  -1.583  -1.86   -1.318  -1.081  -0.143   1.169\n",
            "  -1.022   0.524  -1.013  -0.662  -1.003   1.476   2.938  -0.452   1.844\n",
            "  -0.061   1.384   0.913  -3.496   0.047   0.739]\n",
            "Quantized input: \n",
            "[[212  40  94  83  45  53  87 110  78  82  85  87  81  77  92  95  91  74\n",
            "   84  81  91  81  81  82  82  84  87  82  86  82  83  82  88  91  84  88\n",
            "   84  87  86  77  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.953 -15.077  14.785  20.789   9.825   6.807 -10.122 -11.736   4.445\n",
            "  -0.661   0.364  -2.276  -0.604  -3.305  -1.301   1.431   2.489  -0.419\n",
            "  -8.896  -2.685   4.029   0.123   0.67    1.829   1.781   0.733   0.77\n",
            "  -0.221  -2.742  -1.616   1.059  -1.397  -0.744  -2.054  -1.139   0.885\n",
            "  -1.264   0.595  -0.229   0.297   1.044  -2.672]\n",
            "Quantized input: \n",
            "[[214  53 115 127 105  99  64  60  94  83  85  80  83  78  82  87  90  84\n",
            "   66  79  93  85  86  88  88  86  86  84  79  81  87  82  83  80  82  86\n",
            "   82  86  84  85  87  79]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 61.296   3.903 -23.766   7.076  15.237  -6.646 -13.677  12.278  -0.372\n",
            "  -1.873  -1.471   1.702   2.912   3.369   1.551  -6.087   3.548  -3.444\n",
            "  -0.308  -2.383  -1.958  -0.76    3.577  -2.107  -0.293  -0.334  -0.597\n",
            "   1.001  -0.758   0.185   1.105  -1.812   1.2     2.064  -0.286  -0.633\n",
            "   1.959  -0.163  -2.823   1.209   0.811  -3.002]\n",
            "Quantized input: \n",
            "[[211  93  36  99 116  71  56 110  84  81  81  88  90  91  88  72  92  77\n",
            "   84  80  80  83  92  80  84  84  83  87  83  85  87  81  87  89  84  83\n",
            "   89  84  79  87  86  78]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.174  17.17   -6.286  18.882 -13.056  -6.087  14.45  -11.967  -1.769\n",
            "  -2.051   2.285  -1.594   0.198  -1.722  -3.114  -6.471   4.431  -0.919\n",
            "   3.164  -1.957  -0.283  -1.974  -0.739   2.011   3.18    2.283   1.502\n",
            "   2.467   2.537   0.222   1.192  -1.404  -0.625   0.963   1.071  -0.023\n",
            "   0.822   1.791   0.555  -0.4     0.619   1.175]\n",
            "Quantized input: \n",
            "[[212 120  72 123  58  72 114  60  81  80  89  81  85  81  78  71  94  83\n",
            "   91  80  84  80  83  89  91  89  88  90  90  85  87  82  83  86  87  84\n",
            "   86  88  86  84  86  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 62.26   16.86   12.891  -6.752 -13.489  16.66  -15.359   8.455  -2.062\n",
            "  -2.889  -1.635   0.382  -2.866  -1.568  -4.068   0.208   4.59    3.591\n",
            "   3.105  -2.982  -0.722   3.269  -1.045  -0.407  -0.773  -4.051   2.939\n",
            "  -1.36    3.246   0.955   1.971   0.321   0.907  -0.737   0.611  -0.932\n",
            "  -1.205   0.244  -0.218   0.383   0.121  -0.836]\n",
            "Quantized input: \n",
            "[[213 119 111  71  57 119  53 102  80  79  81  85  79  81  76  85  94  92\n",
            "   91  78  83  91  82  84  83  76  91  82  91  86  89  85  86  83  86  83\n",
            "   82  85  84  85  85  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 60.739   7.131  13.142 -20.194  14.941 -18.114   0.443 -10.807  -3.572\n",
            "  -1.9     2.675   0.214  -0.659   0.981  -1.644   4.977   3.228   4.307\n",
            "   2.053  -2.485  -1.093   0.877   4.382  -0.795   0.967   0.896  -0.896\n",
            "  -1.115  -0.597  -2.122   1.207   5.888   1.664  -0.999  -1.71    0.02\n",
            "  -0.122   1.537  -0.003  -0.81    0.272   1.171]\n",
            "Quantized input: \n",
            "[[209  99 112  43 115  47  85  62  77  81  90  85  83  87  81  95  91  93\n",
            "   89  79  82  86  94  83  86  86  83  82  83  80  87  97  88  82  81  85\n",
            "   84  88  84  83  85  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 64.802  -8.025 -27.127 -20.086   3.47   20.391  -0.507 -11.33    3.316\n",
            "  -0.429   3.96   -0.315  -1.063  -2.503  -2.012   0.894  -4.965   2.13\n",
            "   1.363   3.52    0.579  -0.443   0.01   -3.57    3.142  -0.279   0.659\n",
            "   0.608  -0.782  -1.625   0.49   -2.255  -1.728  -0.171   1.121  -1.215\n",
            "  -0.127   1.687   2.7     0.213   0.456   0.648]\n",
            "Quantized input: \n",
            "[[218  68  29  43  92 126  83  61  91  84  93  84  82  79  80  86  74  89\n",
            "   87  92  86  84  85  77  91  84  86  86  83  81  86  80  81  84  87  82\n",
            "   84  88  90  85  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 64.907 -22.715  -1.301 -10.881 -27.077 -16.098   4.593   7.469  -5.705\n",
            "  -1.121  -3.425   0.343   1.617  -0.017  -5.312  -0.51   -4.066  -0.583\n",
            "   1.415   3.133  -1.175   3.296  -1.742   0.383  -2.627  -0.065  -0.828\n",
            "   0.662   0.958  -1.557  -2.302  -0.315   0.366   1.03   -0.598   0.93\n",
            "   3.764   2.544  -0.612   1.491   1.085  -1.29 ]\n",
            "Quantized input: \n",
            "[[218  38  82  62  29  51  94 100  73  82  77  85  88  84  74  83  76  83\n",
            "   87  91  82  91  81  85  79  84  83  86  86  81  80  84  85  87  83  86\n",
            "   92  90  83  88  87  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 64.939 -19.684  16.532  22.493   2.228  -0.928 -19.356 -11.131   7.436\n",
            "  -0.659   3.302  -1.919  -1.806   3.227  -0.971  -3.336  -4.75    2.285\n",
            "  -0.638   0.798  -1.025  -1.382  -2.682  -0.546   2.159   3.441   0.21\n",
            "  -3.775   0.046   0.793   2.923   2.111   0.289  -2.104  -0.551  -0.074\n",
            "   1.274  -2.992  -2.516   2.103  -1.471  -2.149]\n",
            "Quantized input: \n",
            "[[218  44 119 131  89  83  45  62 100  83  91  81  81  91  83  78  75  89\n",
            "   83  86  82  82  79  83  89  92  85  77  85  86  91  89  85  80  83  84\n",
            "   87  78  79  89  81  80]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 65.086  13.669 -26.08   14.467  16.243 -14.559 -10.573  11.173  -3.528\n",
            "  -1.338  -3.12   -1.593  -1.645   1.786  -1.656   3.333  -5.156   1.116\n",
            "  -0.941   1.378   2.206   2.296   1.756   3.725   0.163  -0.461  -0.015\n",
            "  -2.304  -0.371   0.82    0.558  -0.088   1.834   1.68    2.518  -0.109\n",
            "  -2.046  -1.438  -0.929  -0.405  -0.351   0.702]\n",
            "Quantized input: \n",
            "[[218 113  31 114 118  55  63 107  77  82  78  81  81  88  81  91  74  87\n",
            "   83  87  89  89  88  92  85  84  84  80  84  86  86  84  88  88  90  84\n",
            "   80  82  83  84  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 66.067  24.993   1.512  19.903 -20.668   0.391  13.922 -12.774  -1.119\n",
            "   0.256   3.255   2.774   2.113  -1.308   2.952  -0.553  -6.23    1.742\n",
            "  -3.199   0.645   2.654  -2.071   0.462  -2.076  -3.078   1.452  -1.197\n",
            "  -0.308   0.152  -0.691  -0.456   1.3     0.136   0.933  -0.597  -0.797\n",
            "  -1.744  -1.086  -0.117   0.666   0.582  -1.686]\n",
            "Quantized input: \n",
            "[[220 136  88 125  42  85 113  58  82  85  91  90  89  82  91  83  72  88\n",
            "   78  86  90  80  85  80  78  87  82  84  85  83  84  87  85  86  83  83\n",
            "   81  82  84  86  86  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 65.092  18.266  21.809 -16.565  -9.23   13.69  -18.964   9.643  -1.958\n",
            "  -1.636  -1.979  -4.388  -0.452   2.323  -3.196   2.789  -3.517  -6.212\n",
            "  -0.253   2.238   1.47   -1.468  -3.051  -0.412   1.259   2.463  -1.422\n",
            "   1.151  -1.156   0.55   -0.384  -0.526   0.815  -1.705   0.011  -0.02\n",
            "   0.695  -0.945  -0.781  -0.795   1.943  -0.334]\n",
            "Quantized input: \n",
            "[[218 122 129  50  66 113  45 104  80  81  80  75  84  89  78  90  77  72\n",
            "   84  89  88  81  78  84  87  90  82  87  82  86  84  83  86  81  85  84\n",
            "   86  83  83  83  88  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 64.986   3.709  14.496 -24.043  21.208 -22.594   6.436 -12.51   -0.841\n",
            "   0.545   1.399   1.607  -2.006  -1.062   2.006  -2.196  -5.033  -2.022\n",
            "  -1.969  -0.004  -0.638   1.617  -1.075  -0.181  -1.242  -2.785  -0.921\n",
            "   1.425   1.88    0.398  -1.674  -4.326   0.442  -0.132   0.979  -0.143\n",
            "   0.032   1.011  -0.706  -0.156   0.992   0.17 ]\n",
            "Quantized input: \n",
            "[[218  92 114  35 128  38  98  59  83  86  87  88  80  82  89  80  74  80\n",
            "   80  84  83  88  82  84  82  79  83  87  88  85  81  76  85  84  87  84\n",
            "   85  87  83  84  87  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 56.399 -17.101 -25.239 -20.363  -6.452  10.41   -0.24   -1.999  -1.793\n",
            "  -1.975  -3.468   1.899  -3.144  -1.799   0.009  -2.006   0.351  -0.881\n",
            "  -2.464   2.264   1.734   0.48   -1.609   0.415   0.334  -0.09    0.471\n",
            "  -0.871  -3.794   0.899   2.122  -1.334  -0.009  -1.224   3.908   0.637\n",
            "  -1.92    0.083  -0.241   0.271   1.251  -0.706]\n",
            "Quantized input: \n",
            "[[201  49  33  43  71 106  84  80  81  80  77  88  78  81  85  80  85  83\n",
            "   79  89  88  85  81  85  85  84  85  83  77  86  89  82  84  82  93  86\n",
            "   81  85  84  85  87  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 57.143 -29.421   4.363  -0.425 -22.499 -10.431  -1.825  -0.417   1.652\n",
            "  -1.331   2.547  -5.161   1.878   0.242   0.514   0.091   0.522  -1.061\n",
            "  -1.202   3.271   1.472  -0.812   0.61    0.188  -0.1     0.741  -0.256\n",
            "   0.383  -1.7    -3.66   -1.541   0.118  -0.833   1.235   0.085  -4.839\n",
            "  -0.004  -0.683  -0.131  -1.973   1.986   0.388]\n",
            "Quantized input: \n",
            "[[202  24  93  84  38  63  81  84  88  82  90  74  88  85  86  85  86  82\n",
            "   82  91  88  83  86  85  84  86  84  85  81  77  81  85  83  87  85  75\n",
            "   84  83  84  80  89  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 57.551 -21.088  18.045  24.05    7.421   2.492 -10.877  -5.721  -7.021\n",
            "  -0.721  -0.057   5.712   0.758   1.366  -0.507   0.323  -0.626  -1.443\n",
            "  -1.28    2.939  -0.883   1.985   0.665   1.255  -0.029  -2.282  -1.102\n",
            "  -2.904   0.205  -1.888  -3.293   0.104   2.683   0.233   3.15    2.054\n",
            "   0.478  -0.779   0.526  -2.298  -0.448  -4.343]\n",
            "Quantized input: \n",
            "[[203  41 122 134 100  90  62  73  70  83  84  96  86  87  83  85  83  82\n",
            "   82  91  83  89  86  87  84  80  82  79  85  81  78  85  90  85  91  89\n",
            "   85  83  86  80  84  76]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 57.301   7.579 -33.397   7.148  14.159  -4.303  -7.815  -0.265  -4.768\n",
            "  -1.729   5.542  -0.079  -0.795   1.665  -0.295   1.574   1.653  -0.965\n",
            "  -0.634   1.467   1.453   1.321   1.687  -1.882  -1.374   0.273  -2.405\n",
            "  -1.495   1.586   1.634  -1.936  -1.076   0.33    2.173   2.226   0.297\n",
            "   0.183  -2.644   0.83   -1.294  -0.262   1.716]\n",
            "Quantized input: \n",
            "[[202 100  16  99 114  76  68  84  75  81  96  84  83  88  84  88  88  83\n",
            "   83  88  87  87  88  81  82  85  80  81  88  88  81  82  85  89  89  85\n",
            "   85  79  86  82  84  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 58.03   25.416  -8.68   23.593 -14.326  -4.251   1.362  -5.432  -2.637\n",
            "  -1.838  -3.853  -1.953   1.81   -0.633   4.082  -1.338  -0.015   2.272\n",
            "   1.172   0.476   1.476  -1.603  -1.066   0.506  -1.153   1.736  -1.351\n",
            "   1.001   0.541  -1.022   0.859  -0.725  -3.708  -0.206  -0.017  -3.757\n",
            "   0.056  -2.043  -0.93    1.251   0.404  -2.12 ]\n",
            "Quantized input: \n",
            "[[204 137  67 133  55  76  87  73  79  81  77  80  88  83  93  82  84  89\n",
            "   87  85  88  81  82  86  82  88  82  87  86  82  86  83  77  84  84  77\n",
            "   85  80  83  87  85  80]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 56.923  24.758  17.804  -9.426 -17.86    7.579 -10.897  -0.75   -1.745\n",
            "  -1.666   2.865   2.496  -2.024   1.205  -2.594  -1.366   0.422  -2.246\n",
            "  -0.418   1.708  -0.12    0.509  -2.068  -0.769  -0.294   0.041  -0.202\n",
            "  -0.034  -0.82    2.982   1.903   0.304   0.454  -0.661   1.815   0.477\n",
            "   2.475   2.516  -2.507  -1.171   0.413   0.213]\n",
            "Quantized input: \n",
            "[[202 135 121  65  48 100  62  83  81  81  90  90  80  87  79  82  85  80\n",
            "   84  88  84  86  80  83  84  85  84  84  83  91  88  85  85  83  88  85\n",
            "   90  90  79  82  85  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 58.593  11.741  17.075 -28.89   13.011 -12.497  -4.605  -6.865  -3.945\n",
            "  -1.491  -0.571  -4.28    0.874   0.729   0.599   0.862   0.518   1.916\n",
            "   1.263   0.534  -0.461   1.092   1.402  -0.981  -0.872  -0.399  -2.104\n",
            "   0.078   3.843   0.868  -1.387  -1.75   -0.823  -0.353   0.966   0.302\n",
            "  -1.705   2.26    1.285   1.543   0.067   1.088]\n",
            "Quantized input: \n",
            "[[205 109 120  25 111  59  75  70  76  81  83  76  86  86  86  86  86  88\n",
            "   87  86  84  87  87  82  83  84  80  85  92  86  82  81  83  84  86  85\n",
            "   81  89  87  88  85  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 59.425 -19.091 -22.658 -18.866  -6.634  12.752   2.626 -10.337  -2.881\n",
            "  -0.012   1.898   1.784   1.221  -0.582  -2.817  -1.482  -3.078   3.664\n",
            "   1.301   1.327   0.779  -0.695   1.701  -0.729   1.889  -0.12   -0.616\n",
            "  -0.842  -2.476   1.255   0.321   0.888  -1.726   0.523   0.333   1.726\n",
            "   0.038   0.076   1.275  -0.071   1.64   -0.753]\n",
            "Quantized input: \n",
            "[[207  45  38  46  71 111  90  63  79  84  88  88  87  83  79  81  78  92\n",
            "   87  87  86  83  88  83  88  84  83  83  79  87  85  86  81  86  85  88\n",
            "   85  85  87  84  88  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 59.118 -29.073   6.292   0.496 -20.844 -13.522  -2.289   7.161  -2.738\n",
            "  -1.396  -1.928  -1.836  -0.93    2.718  -1.703  -1.647  -2.085   2.825\n",
            "   2.505   0.921  -1.093   3.285   0.683  -0.98   -1.439  -0.236   1.481\n",
            "   0.734  -0.211  -0.899  -0.491   0.146  -0.97   -0.281   0.938  -1.993\n",
            "  -0.177   0.906  -1.404   0.808  -0.369  -0.897]\n",
            "Quantized input: \n",
            "[[206  25  97  86  42  57  80  99  79  82  81  81  83  90  81  81  80  90\n",
            "   90  86  82  91  86  82  82  84  88  86  84  83  83  85  83  84  86  80\n",
            "   84  86  82  86  84  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 58.508 -19.549  19.101  23.683  10.399   6.742  -8.949  -9.996  -0.217\n",
            "  -1.173   2.801   0.668  -0.802   0.242  -0.103  -1.462  -1.21    0.05\n",
            "   4.329   1.213  -1.356   1.481  -1.251  -0.84    1.094   3.212   0.696\n",
            "  -2.12    1.041   0.764  -1.253  -0.041   0.741   1.152   0.269  -0.542\n",
            "   1.709  -0.474   0.706   0.213  -1.926  -0.603]\n",
            "Quantized input: \n",
            "[[205  44 124 133 106  98  66  64  84  82  90  86  83  85  84  81  82  85\n",
            "   93  87  82  88  82  83  87  91  86  80  87  86  82  84  86  87  85  83\n",
            "   88  84  86  85  81  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 59.793   4.124 -32.65    6.621  13.094  -4.91  -14.54    5.395  -1.569\n",
            "  -0.776  -0.5    -2.836   0.204  -1.674  -3.185   2.596  -3.059   1.806\n",
            "  -0.811   0.577   1.9     0.769  -0.17    1.629  -1.112   1.652   1.163\n",
            "  -0.387   1.597   1.626  -0.328   0.152  -1.105   1.691  -0.092  -0.626\n",
            "  -0.044   0.005   1.042  -0.841   0.171   0.954]\n",
            "Quantized input: \n",
            "[[208  93  17  98 111  74  55  96  81  83  83  79  85  81  78  90  78  88\n",
            "   83  86  88  86  84  88  82  88  87  84  88  88  84  85  82  88  84  83\n",
            "   84  85  87  83  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 59.684  23.798 -12.051  20.607 -15.021  -7.782   8.854  -8.369  -2.082\n",
            "  -0.403   0.786   1.675  -1.637  -2.31    1.706   2.325  -4.768   1.06\n",
            "   0.063   0.55    1.278   0.17   -1.97   -2.282  -1.343  -0.381  -0.506\n",
            "   0.002  -0.253   0.66    1.721   2.189  -1.461  -1.598  -0.235  -0.272\n",
            "  -0.077  -1.439   0.136  -0.084  -0.962  -0.175]\n",
            "Quantized input: \n",
            "[[207 133  60 127  54  68 103  67  80  84  86  88  81  80  88  89  75  87\n",
            "   85  86  87  85  80  80  82  84  83  85  84  86  88  89  81  81  84  84\n",
            "   84  82  85  84  83  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 59.242  26.112  13.81   -7.62  -16.258  14.822 -12.874   3.512  -2.088\n",
            "  -1.14    0.199  -1.264  -0.167   4.084   0.079   3.224  -3.747  -2.424\n",
            "  -0.606   0.81    0.077   0.904   0.89   -0.592  -0.322   1.618  -3.586\n",
            "   0.17    0.843   1.453   0.362  -0.617  -0.173  -1.306   0.077  -0.335\n",
            "   2.437   0.698  -0.304  -1.556  -0.045  -0.852]\n",
            "Quantized input: \n",
            "[[206 138 113  69  51 115  58  92  80  82  85  82  84  93  85  91  77  80\n",
            "   83  86  85  86  86  83  84  88  77  85  86  87  85  83  84  82  85  84\n",
            "   90  86  84  81  84  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[ 58.57   13.949  18.393 -24.331  15.441 -16.419  -3.462  -8.117  -3.492\n",
            "   0.267   0.139  -1.475   0.397   1.633   0.27   -2.399  -4.076  -2.824\n",
            "  -1.369   1.093  -1.394  -2.989   0.318   0.058  -1.857  -0.5     0.3\n",
            "   0.725   1.85   -0.521   0.519  -1.438  -0.423  -1.285   1.325   0.084\n",
            "   0.246   0.621   1.083  -0.171   0.402   0.798]\n",
            "Quantized input: \n",
            "[[205 113 122  34 116  51  77  68  77  85  85  81  85  88  85  80  76  79\n",
            "   82  87  82  78  85  85  81  83  85  86  88  83  86  82  84  82  87  85\n",
            "   85  86  87  84  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.283  0.209  1.11   0.74   2.488  1.807  4.089  3.562  1.497  0.735\n",
            " -0.37   0.083  1.373 -0.822  0.265  0.347  0.11   0.56   1.509  1.134\n",
            " -0.046 -0.75  -0.46   0.333 -1.471 -0.646  0.386 -0.324 -0.364  0.124\n",
            " -0.198  0.674  0.226 -0.953  0.504 -0.219  0.03  -0.087 -0.24   0.033\n",
            "  0.051  0.133]\n",
            "Quantized input: \n",
            "[[254  85  87  86  90  88  93  92  88  86  84  85  87  83  85  85  85  86\n",
            "   88  87  84  83  84  85  81  83  85  84  84  85  84  86  85  83  86  84\n",
            "   85  84  84  85  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.49   0.086  0.897  0.014  2.568  0.491  3.93   1.948  2.02   0.632\n",
            "  0.107  0.587  0.386  0.234 -0.135  0.174  0.431  1.494 -0.285  0.256\n",
            " -0.156  0.61   0.939  0.421  0.038  1.697 -0.02  -0.883  0.986  0.149\n",
            "  0.31   0.047 -0.354 -0.106  0.212 -0.584  0.715  0.505  0.411 -0.731\n",
            " -0.741 -0.014]\n",
            "Quantized input: \n",
            "[[254  85  86  85  90  86  93  89  89  86  85  86  85  85  84  85  85  88\n",
            "   84  85  84  86  86  85  85  88  84  83  87  85  85  85  84  84  85  83\n",
            "   86  86  85  83  83  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.593 -0.062  1.062 -0.041  1.869  1.257  2.415  2.736  1.753  0.65\n",
            " -0.824  0.538  0.221 -0.426  0.296  0.124  0.93   1.     1.801  0.393\n",
            " -0.386  0.394  0.162 -0.492  0.546 -1.005 -0.471 -0.253 -0.171 -0.429\n",
            "  0.865  0.801 -0.468  0.461  0.568  0.049  0.142 -0.197  0.005  0.559\n",
            " -0.761 -0.11 ]\n",
            "Quantized input: \n",
            "[[254  84  87  84  88  87  89  90  88  86  83  86  85  84  85  85  86  87\n",
            "   88  85  84  85  85  83  86  82  84  84  84  84  86  86  84  85  86  85\n",
            "   85  84  85  86  83  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.053 -0.283  2.044  1.309  3.05   1.771  3.898  2.246  2.307  0.681\n",
            " -0.263  0.045 -0.97   0.037 -0.375  0.227  0.086  0.723 -0.772  0.751\n",
            " -0.457  0.544 -0.524 -0.086  0.113  0.355 -1.255  0.098 -0.939  0.649\n",
            " -0.553  0.723  0.118  0.644 -0.116  0.258 -0.026  0.492  0.794 -0.32\n",
            "  0.502  0.315]\n",
            "Quantized input: \n",
            "[[253  84  89  87  91  88  93  89  89  86  84  85  83  85  84  85  85  86\n",
            "   83  86  84  86  83  84  85  85  82  85  83  86  83  86  85  86  84  85\n",
            "   84  86  86  84  86  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[81.714 -1.137  2.222  0.949  2.201  1.124  4.22   3.979  0.722  0.997\n",
            "  0.019  0.644  1.083 -0.152  0.14   1.181  0.147  0.136 -0.399  1.205\n",
            " -0.172 -0.397 -0.34  -0.355  0.075  0.278  0.117  0.191  0.705 -0.179\n",
            " -0.138 -0.627 -1.187  0.528  0.081 -0.537 -0.978  0.207 -0.21   0.131\n",
            "  0.472  0.082]\n",
            "Quantized input: \n",
            "[[253  82  89  86  89  87  93  93  86  87  85  86  87  84  85  87  85  85\n",
            "   84  87  84  84  84  84  85  85  85  85  86  84  84  83  82  86  85  83\n",
            "   82  85  84  85  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[81.777 -2.011  1.681  0.858  2.61   2.407  3.808  1.51   1.186  0.994\n",
            " -0.01  -0.637  0.305 -0.565  0.893  0.306 -0.175 -0.54  -0.579  1.\n",
            " -0.08   0.687 -0.127  0.496  0.404  0.273  0.361 -1.025 -0.253  1.366\n",
            "  0.138  0.202  0.273 -0.393 -0.225  0.385 -0.328 -0.134 -0.952  0.052\n",
            " -0.661  0.447]\n",
            "Quantized input: \n",
            "[[253  80  88  86  90  89  92  88  87  87  84  83  85  83  86  85  84  83\n",
            "   83  87  84  86  84  86  85  85  85  82  84  87  85  85  85  84  84  85\n",
            "   84  84  83  85  83  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[81.434 -2.191  0.709  1.181  2.378  0.831  2.867  3.695  1.285  1.053\n",
            " -0.559  0.848 -0.378 -0.086  0.766 -0.233 -0.014 -1.149  0.174  1.435\n",
            "  0.712  0.206  0.484 -0.214 -1.221 -0.022 -0.266  0.67   0.443  0.076\n",
            " -1.54   0.288 -0.305 -0.34  -0.879  0.662 -0.23  -0.115  0.34   0.176\n",
            " -0.464 -0.083]\n",
            "Quantized input: \n",
            "[[252  80  86  87  89  86  90  92  87  87  83  86  84  84  86  84  84  82\n",
            "   85  87  86  85  85  84  82  84  84  86  85  85  81  85  84  84  83  86\n",
            "   84  84  85  85  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[70.384 -3.143  1.305  0.109  1.306  2.137  3.799  3.501  1.919 -0.027\n",
            "  0.511  2.334  1.106  0.653 -0.338  0.74   0.287 -0.117 -0.276  0.68\n",
            " -0.285  0.063 -0.197  0.448 -1.139 -0.111 -0.202  0.397  0.089  0.545\n",
            "  1.232 -0.279 -0.463  0.748 -0.354 -0.35  -0.269  0.541 -0.204 -0.154\n",
            " -0.38   1.417]\n",
            "Quantized input: \n",
            "[[229  78  87  85  87  89  92  92  88  84  86  89  87  86  84  86  85  84\n",
            "   84  86  84  85  84  85  82  84  84  85  85  86  87  84  84  86  84  84\n",
            "   84  86  84  84  84  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[71.61  -2.618  2.481  2.665  1.321  1.706  3.91   2.694  1.868 -0.435\n",
            " -1.354 -0.683 -1.653  0.36  -0.403  0.494  0.364 -0.444  0.725  0.517\n",
            " -0.063 -0.35  -0.06   0.61  -0.112 -0.133 -0.534 -0.352 -0.802  0.026\n",
            "  1.032 -0.041 -0.868  0.413 -0.274 -0.107  0.391 -0.759  0.037 -1.102\n",
            "  0.818  0.233]\n",
            "Quantized input: \n",
            "[[232  79  90  90  87  88  93  90  88  84  82  83  81  85  84  86  85  84\n",
            "   86  86  84  84  84  86  84  84  83  84  83  85  87  84  83  85  84  84\n",
            "   85  83  85  82  86  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[72.466 -1.717  1.487  3.448  3.28   2.677  4.006  3.737  1.521  0.094\n",
            "  1.174  0.293  1.243 -1.418 -0.356  0.018  0.266 -1.297  0.942  0.803\n",
            " -0.412  0.179  0.628 -0.248 -0.84  -0.034  0.078  0.328 -0.307 -0.362\n",
            "  0.254  0.765 -0.412 -0.652  0.486  0.122  0.022 -0.11   1.362 -0.933\n",
            " -0.01   0.206]\n",
            "Quantized input: \n",
            "[[234  81  88  92  91  90  93  92  88  85  87  85  87  82  84  85  85  82\n",
            "   86  86  84  85  86  84  83  84  85  85  84  84  85  86  84  83  85  85\n",
            "   85  84  87  83  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[69.626 -2.549 -0.548  0.604  3.332  1.823  4.023  2.708  2.089 -0.446\n",
            " -0.249 -1.94   0.958 -0.084 -0.109  0.315  0.332 -0.538 -0.157  1.146\n",
            " -0.425 -0.629  0.071 -0.871  0.075  0.239 -0.407  0.334  0.437  0.92\n",
            "  1.229  0.443  0.296 -0.356 -0.293  0.757 -0.704 -0.472  0.513  0.498\n",
            " -0.308 -0.314]\n",
            "Quantized input: \n",
            "[[228  79  83  86  91  88  93  90  89  84  84  81  86  84  84  85  85  83\n",
            "   84  87  84  83  85  83  85  85  84  85  85  86  87  85  85  84  84  86\n",
            "   83  84  86  86  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[68.905 -1.246 -0.585  2.866  2.454  1.644  4.19   3.58   1.177 -0.083\n",
            " -0.644  1.578 -1.121 -0.938  0.297 -0.97  -0.167 -0.285  0.124  1.218\n",
            " -0.167  0.035 -0.432  0.767  0.051  0.239 -0.374  0.168  1.582  0.314\n",
            " -0.563 -0.249  1.117 -0.009 -0.992 -0.2    0.233  0.534 -0.538 -0.787\n",
            " -0.416  0.13 ]\n",
            "Quantized input: \n",
            "[[226  82  83  90  90  88  93  92  87  84  83  88  82  83  85  83  84  84\n",
            "   85  87  84  85  84  86  85  85  84  85  88  85  83  84  87  84  82  84\n",
            "   85  86  83  83  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[68.634 -0.657  1.138  2.741  0.944  2.572  3.253  2.046  0.484 -0.41\n",
            "  1.076 -0.307 -0.101  1.776 -1.073  0.965  0.656 -0.193 -0.253  1.116\n",
            "  0.243  0.33  -0.572 -0.845  0.105 -0.712  0.527  0.72   0.44  -1.3\n",
            "  0.889 -0.267  0.045 -0.795 -0.862 -0.043 -0.638  0.791 -0.009 -0.201\n",
            "  0.377 -0.222]\n",
            "Quantized input: \n",
            "[[226  83  87  90  86  90  91  89  85  84  87  84  84  88  82  86  86  84\n",
            "   84  87  85  85  83  83  85  83  86  86  85  82  86  84  85  83  83  84\n",
            "   83  86  84  84  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[67.623 -1.029  1.948  0.475  1.261  1.273  2.064  2.623  0.653 -0.154\n",
            " -1.213  0.007  1.508 -1.     0.317  0.037  0.332  0.04  -0.326  0.909\n",
            "  0.603  0.467  0.504  0.323  0.642 -0.21  -0.629 -0.517 -0.426 -0.161\n",
            "  1.049 -0.652  1.182 -0.266 -0.403  0.681 -0.129  0.427  0.398 -1.142\n",
            "  0.42  -0.489]\n",
            "Quantized input: \n",
            "[[224  82  89  85  87  87  89  90  86  84  82  85  88  82  85  85  85  85\n",
            "   84  86  86  85  86  85  86  84  83  83  84  84  87  83  87  84  84  86\n",
            "   84  85  85  82  85  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[68.717 -0.191 -0.166  1.705  1.985  1.207  1.985  0.982  0.586 -0.226\n",
            "  0.391 -0.087  0.711 -0.417  0.69   0.698 -0.177  0.273  0.4    0.77\n",
            " -0.256  1.495  1.277  0.263 -0.784 -0.102  0.525  0.183  0.775  0.554\n",
            " -0.199 -0.611 -0.009 -0.686  0.226  0.978 -0.563 -1.2   -0.179 -1.058\n",
            "  0.127  0.007]\n",
            "Quantized input: \n",
            "[[226  84  84  88  89  87  89  87  86  84  85  84  86  84  86  86  84  85\n",
            "   85  86  84  88  87  85  83  84  86  85  86  86  84  83  84  83  85  87\n",
            "   83  82  84  82  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[68.375 -1.014 -0.243 -0.1    2.344  0.911  3.054  2.09  -0.598 -0.29\n",
            " -0.063  0.877 -0.06  -0.666 -0.202  0.14  -0.125  0.434  0.192  0.525\n",
            " -0.681  0.361 -0.683 -0.601  0.997  0.79   0.47  -0.511 -0.329  0.376\n",
            "  0.376 -0.093 -0.37  -0.463 -0.149 -0.025 -1.396  0.759  0.424 -1.177\n",
            "  0.356 -0.834]\n",
            "Quantized input: \n",
            "[[225  82  84  84  89  86  91  89  83  84  84  86  84  83  84  85  84  85\n",
            "   85  86  83  85  83  83  87  86  85  83  84  85  85  84  84  84  84  84\n",
            "   82  86  85  82  85  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[68.114 -1.648  0.967  0.072  1.128  1.562  2.732  2.473  1.834 -0.535\n",
            " -0.049 -0.879 -0.12   0.078 -0.617 -0.28   0.126  0.275 -0.156  0.23\n",
            " -0.82   0.028 -0.225 -0.036 -0.618  0.465  0.471  0.329  0.313 -0.578\n",
            " -0.425  0.577 -0.089  0.74  -0.336 -0.373 -0.752  1.223 -0.601 -0.324\n",
            " -0.456 -0.298]\n",
            "Quantized input: \n",
            "[[225  81  86  85  87  88  90  90  88  83  84  83  84  85  83  84  85  85\n",
            "   84  85  83  85  84  84  83  85  85  85  85  83  84  86  84  86  84  84\n",
            "   83  87  83  84  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[68.396  0.455  0.819  0.998  0.426  0.317  2.123  2.231  1.665 -0.417\n",
            " -0.798  0.364  0.075 -0.048  0.771 -0.129 -0.322  0.129  0.89   1.143\n",
            "  0.928 -1.463  1.068  0.84  -0.441 -1.103 -0.065  0.641 -1.006  0.575\n",
            " -0.365  0.583 -0.11   0.041 -1.614 -0.535  0.927 -0.967 -0.209  0.102\n",
            "  0.649  0.296]\n",
            "Quantized input: \n",
            "[[225  85  86  87  85  85  89  89  88  84  83  85  85  84  86  84  84  85\n",
            "   86  87  86  81  87  86  84  82  84  86  82  86  84  86  84  85  81  83\n",
            "   86  83  84  85  86  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[67.818  0.8    1.118 -1.109  1.312  1.285  3.523  1.914  2.795 -0.818\n",
            "  0.025  0.856  0.286  0.83  -0.8   -0.089  0.26  -0.059 -1.697  0.267\n",
            "  1.282 -0.257 -1.106  0.411  0.118  0.493  0.104  1.801  0.064  0.442\n",
            "  0.539 -0.051 -0.537 -0.214  0.095  0.489  0.385  0.901  0.777 -0.184\n",
            " -0.136  0.024]\n",
            "Quantized input: \n",
            "[[224  86  87  82  87  87  92  88  90  83  85  86  85  86  83  84  85  84\n",
            "   81  85  87  84  82  85  85  86  85  88  85  85  86  84  83  84  85  86\n",
            "   85  86  86  84  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[67.461  0.898  1.071 -0.587  3.717  1.176  3.895  3.721  0.991 -0.647\n",
            "  0.125 -0.242  1.706 -1.393  0.117  0.424  0.222 -0.636  0.381  0.31\n",
            "  0.763 -0.266 -0.749 -0.738  0.352 -1.729 -0.96  -0.892  1.262  1.314\n",
            " -0.69  -0.49   0.915 -0.452  0.532  0.198  0.343 -0.955 -0.492  0.204\n",
            " -1.071  0.905]\n",
            "Quantized input: \n",
            "[[223  86  87  83  92  87  93  92  87  83  85  84  88  82  85  85  85  83\n",
            "   85  85  86  84  83  83  85  81  83  83  87  87  83  83  86  84  86  85\n",
            "   85  83  83  85  82  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[67.168  0.109  2.275  2.097  3.82   1.536  4.509  1.82   2.705 -0.829\n",
            " -0.56   0.455 -1.07  -1.049  0.687  0.25   0.395 -0.454 -0.462 -0.138\n",
            "  0.403  0.482  0.285  0.826 -0.523  0.377 -1.399  0.694  0.701 -1.738\n",
            " -0.502  0.647 -0.664 -0.111 -0.383 -0.587  0.642 -0.368  0.151  1.178\n",
            " -0.303 -1.072]\n",
            "Quantized input: \n",
            "[[223  85  89  89  92  88  94  88  90  83  83  85  82  82  86  85  85  84\n",
            "   84  84  85  85  85  86  83  85  82  86  86  81  83  86  83  84  84  83\n",
            "   86  84  85  87  84  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.243 -0.805  1.155  0.161  1.555  1.542  3.02   1.503  1.876  0.787\n",
            " -0.426 -0.5    1.368 -1.227  0.922  0.845 -0.23  -0.295  1.086  0.251\n",
            "  0.472 -0.79  -0.031 -0.143  0.154 -0.633  0.294 -0.4    0.304 -0.772\n",
            "  0.694 -1.183  0.091  0.87  -0.029 -0.234 -0.539 -0.372  0.342  1.266\n",
            " -0.612 -0.382]\n",
            "Quantized input: \n",
            "[[243  83  87  85  88  88  91  88  88  86  84  83  87  82  86  86  84  84\n",
            "   87  85  85  83  84  84  85  83  85  84  85  83  86  82  85  86  84  84\n",
            "   83  84  85  87  83  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.444 -0.597  1.475  0.897  1.664  0.481  2.942  2.401  0.17   0.621\n",
            "  0.391  0.925  0.809  0.162 -0.575  0.875  0.243  0.457 -0.373 -0.192\n",
            "  0.541  0.929 -0.052  0.16   0.09   0.489 -0.914  0.593 -0.393 -0.722\n",
            "  0.565 -0.902  0.55   0.984 -0.364  0.737  0.6    0.806 -0.337  0.124\n",
            " -0.197  0.826]\n",
            "Quantized input: \n",
            "[[244  83  88  86  88  85  91  89  85  86  85  86  86  85  83  86  85  85\n",
            "   84  84  86  86  84  85  85  86  83  86  84  83  86  83  86  87  84  86\n",
            "   86  86  84  85  84  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.42  -0.473  0.956  1.555  2.216  2.19   2.929  2.373  1.543  0.309\n",
            " -0.739  0.104 -0.796 -0.073  0.013 -0.455  0.116  0.91   0.463  0.123\n",
            "  0.617 -0.371 -0.759 -0.223 -0.083 -0.788  0.233 -0.519 -0.955 -0.64\n",
            "  0.79  -0.093  0.656 -0.359 -0.964 -0.057  0.135 -1.19  -0.597  0.007\n",
            "  0.718 -0.828]\n",
            "Quantized input: \n",
            "[[244  84  86  88  89  89  91  89  88  85  83  85  83  84  85  84  85  86\n",
            "   85  85  86  84  83  84  84  83  85  83  83  83  86  84  86  84  83  84\n",
            "   85  82  83  85  86  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.221 -0.535  0.432 -0.008  2.259  0.774  2.877  3.058  2.186  0.468\n",
            " -0.502  0.726 -1.204  0.038  0.505 -0.286 -0.559 -0.515 -0.299 -0.139\n",
            "  0.084  0.777  0.162  0.512  0.102  1.038  0.384 -0.242 -0.263  0.216\n",
            "  0.455  0.505  1.628  0.805  0.484 -0.067 -0.122 -0.184 -0.583 -0.061\n",
            " -0.81  -1.081]\n",
            "Quantized input: \n",
            "[[243  83  85  84  89  86  90  91  89  85  83  86  82  85  86  84  83  83\n",
            "   84  84  85  86  85  86  85  87  85  84  84  85  85  86  88  86  85  84\n",
            "   84  84  83  84  83  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.519  0.101  0.279  0.73   2.265  1.193  4.373  2.204 -0.022  0.646\n",
            "  0.523  0.133  0.672  1.158  0.11  -0.079 -0.402 -0.288  0.626  0.032\n",
            "  0.191 -0.254  0.008 -0.996 -0.136  0.25   0.454  0.368  0.41  -0.187\n",
            " -0.288 -0.237  0.733 -0.069  0.136 -0.269 -0.285  0.057 -0.111  0.313\n",
            "  1.148 -0.95 ]\n",
            "Quantized input: \n",
            "[[244  85  85  86  89  87  93  89  84  86  86  85  86  87  85  84  84  84\n",
            "   86  85  85  84  85  82  84  85  85  85  85  84  84  84  86  84  85  84\n",
            "   84  85  84  85  87  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.015  0.082  1.347  1.631  2.156  2.085  1.781  1.639  0.233  1.04\n",
            " -0.564 -0.428  0.843 -0.55   0.098 -1.175 -0.899  0.208  0.367 -0.376\n",
            "  0.183  0.577 -0.231  0.412  0.866 -0.107 -0.032  0.054 -0.089 -0.01\n",
            " -0.014 -0.624  0.428  0.304 -0.228 -0.035 -0.321  0.733 -0.489  0.028\n",
            "  1.045  0.171]\n",
            "Quantized input: \n",
            "[[245  85  87  88  89  89  88  88  85  87  83  84  86  83  85  82  83  85\n",
            "   85  84  85  86  84  85  86  84  84  85  84  84  84  83  85  85  84  84\n",
            "   84  86  83  85  87  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.784 -0.432  1.897  0.521  1.14  -0.433  2.346  1.797  1.716  0.949\n",
            " -0.192  1.072 -0.229 -0.829 -1.355  0.236 -0.387 -0.667  0.228 -0.021\n",
            "  0.653  0.064  0.829 -0.007  0.007 -0.794  0.623  0.023  0.272  0.022\n",
            " -0.7   -0.232  0.773 -0.073 -0.129  0.708 -0.961 -0.347 -0.19   0.631\n",
            "  0.184  0.713]\n",
            "Quantized input: \n",
            "[[245  84  88  86  87  84  89  88  88  86  84  87  84  83  82  85  84  83\n",
            "   85  84  86  85  86  84  85  83  86  85  85  85  83  84  86  84  84  86\n",
            "   83  84  84  86  85  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.648 -0.45   0.24   0.814  1.284  0.452  1.669  2.736  1.243  0.614\n",
            " -0.44   0.729  0.474 -0.52  -0.746 -0.201  0.085  0.154  0.174  0.017\n",
            "  0.26   0.151 -0.959 -0.221  1.47  -0.756  0.279  0.121  0.502 -0.003\n",
            " -0.204 -0.642  0.212  0.101  0.117  0.358  0.281 -0.301 -0.54  -0.46\n",
            " -0.857  0.838]\n",
            "Quantized input: \n",
            "[[244  84  85  86  87  85  88  90  87  86  84  86  85  83  83  84  85  85\n",
            "   85  85  85  85  83  84  88  83  85  85  86  84  84  83  85  85  85  85\n",
            "   85  84  83  84  83  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.177 -1.293 -0.037  1.652  1.257  2.509  2.734  1.197  0.749  0.259\n",
            " -0.174 -0.474  0.296  0.694 -0.219 -0.365  0.285  0.138  0.527  0.055\n",
            "  0.321 -0.576 -0.55  -0.212 -0.124  0.189 -0.321  1.432 -0.457 -0.137\n",
            " -0.215  0.261  0.124  0.427 -0.961  0.11  -0.069 -0.008  0.895  0.741\n",
            "  0.294 -0.51 ]\n",
            "Quantized input: \n",
            "[[245  82  84  88  87  90  90  87  86  85  84  84  85  86  84  84  85  85\n",
            "   86  85  85  83  83  84  84  85  84  87  84  84  84  85  85  85  83  85\n",
            "   84  84  86  86  85  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.636 -1.92   0.371  0.96   2.342  0.143  3.865  2.763  0.205  0.407\n",
            "  0.144  1.111  0.408 -0.264  1.135 -0.865 -0.168  0.412  0.387  0.106\n",
            "  0.32  -0.201  0.837  0.403 -1.377  0.419  0.418 -0.244 -0.004  0.841\n",
            " -0.376 -0.712  1.088 -0.193 -0.338  0.426  0.113 -0.75  -0.711  0.365\n",
            "  0.117  0.507]\n",
            "Quantized input: \n",
            "[[244  81  85  86  89  85  92  90  85  85  85  87  85  84  87  83  84  85\n",
            "   85  85  85  84  86  85  82  85  85  84  84  86  84  83  87  84  84  85\n",
            "   85  83  83  85  85  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.645  0.4    0.209 -0.065  1.117  0.78   3.762  1.276  1.023  0.455\n",
            "  0.357 -0.29   0.106 -0.746 -0.142  0.735  0.234  0.155 -0.434 -0.445\n",
            "  0.563  1.253  1.135  0.17   0.01  -0.534 -0.012  0.175  0.778 -0.635\n",
            "  0.265 -0.394 -0.504 -0.579  0.114  0.027  0.705 -1.351  0.048  0.578\n",
            " -0.189 -0.801]\n",
            "Quantized input: \n",
            "[[244  85  85  84  87  86  92  87  87  85  85  84  85  83  84  86  85  85\n",
            "   84  84  86  87  87  85  85  83  84  85  86  83  85  84  83  83  85  85\n",
            "   86  82  85  86  84  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.556  0.995  0.618  0.095  2.342  1.87   2.328  2.876  1.346  0.48\n",
            " -0.731  0.646  0.283 -0.554  1.002  0.942  0.215 -0.253  0.63  -0.225\n",
            "  1.096 -1.216  0.584  0.314 -0.262  0.126  0.102  0.094 -0.438 -0.283\n",
            "  0.398  0.498  0.222  0.133 -0.072 -0.828  1.198  1.121  0.185 -0.582\n",
            "  0.713 -0.468]\n",
            "Quantized input: \n",
            "[[244  87  86  85  89  88  89  90  87  85  83  86  85  83  87  86  85  84\n",
            "   86  84  87  82  86  85  84  85  85  85  84  84  85  86  85  85  84  83\n",
            "   87  87  85  83  86  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.625  0.865  1.623  0.755  2.682  0.101  3.525  1.674  1.635  0.364\n",
            "  0.308  0.219  0.117  0.758  0.622  0.012  0.204 -0.275 -0.612 -0.889\n",
            "  0.161 -0.035 -1.05   0.266 -0.395  0.009 -0.095  0.2    0.008  0.611\n",
            "  0.661 -0.352  1.113 -0.474  1.331  0.477 -0.382  0.169  0.463  0.411\n",
            "  0.335  1.28 ]\n",
            "Quantized input: \n",
            "[[244  86  88  86  90  85  92  88  88  85  85  85  85  86  86  85  85  84\n",
            "   83  83  85  84  82  85  84  85  84  85  85  86  86  84  87  84  87  85\n",
            "   84  85  85  85  85  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.719  0.187  2.208  0.393  1.636  1.582  3.711  3.029  1.17   0.332\n",
            " -0.653  0.391  1.091 -0.65   0.559 -0.559  0.44  -0.551 -0.236 -0.427\n",
            " -0.87  -0.123  0.498 -1.201 -0.029 -0.266 -0.985 -0.261  0.792  0.328\n",
            " -1.178  0.286  1.501  0.604  0.186 -0.92  -0.803  0.386  0.586  0.315\n",
            "  0.287 -0.362]\n",
            "Quantized input: \n",
            "[[244  85  89  85  88  88  92  91  87  85  83  85  87  83  86  83  85  83\n",
            "   84  84  83  84  86  82  84  84  82  84  86  85  82  85  88  86  85  83\n",
            "   83  85  86  85  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.184 -0.185  1.218  1.573  1.526  1.338  2.34   1.851  2.879 -0.28\n",
            " -0.503 -0.308  0.512 -0.554  0.533  0.178 -0.218 -0.712  0.964 -0.125\n",
            "  0.525  0.705  0.147 -0.304 -0.058 -0.275 -0.665  0.695 -0.849 -0.583\n",
            "  0.399 -0.08  -0.457 -1.112 -0.718  0.208  0.946 -0.085  0.802  0.605\n",
            "  0.1    0.105]\n",
            "Quantized input: \n",
            "[[254  84  87  88  88  87  89  88  90  84  83  84  86  83  86  85  84  83\n",
            "   86  84  86  86  85  84  84  84  83  86  83  83  85  84  84  82  83  85\n",
            "   86  84  86  86  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.566 -0.311  0.343  1.188  2.453  1.094  3.229  1.758 -0.325 -0.055\n",
            "  0.234  1.088  1.531 -0.565  0.52   0.625 -0.292  0.107  0.538 -0.496\n",
            "  0.293  0.424 -0.338 -0.228 -0.387  0.151  1.078 -0.161  0.625 -0.129\n",
            " -0.003  1.224  0.204 -0.564 -0.902  0.589  0.819 -0.459 -0.056 -0.401\n",
            "  0.379  0.801]\n",
            "Quantized input: \n",
            "[[254  84  85  87  90  87  91  88  84  84  85  87  88  83  86  86  84  85\n",
            "   86  83  85  85  84  84  84  85  87  84  86  84  84  87  85  83  83  86\n",
            "   86  84  84  84  85  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Evaluated on 100 results so far.\n",
            "Non-quantized input:\n",
            "[82.387 -0.811  0.364  0.146  2.233  1.465  3.141  2.821  0.811 -0.662\n",
            " -0.209 -0.151 -0.864  0.439 -0.322  0.343  0.004  0.811 -0.597 -1.059\n",
            "  0.008 -0.018  0.255  0.668  0.672  0.247  1.157 -0.565  0.883  0.646\n",
            "  0.688  0.656 -0.449  0.247 -0.251  0.507 -0.14   0.211  0.59   0.214\n",
            "  0.428 -1.149]\n",
            "Quantized input: \n",
            "[[254  83  85  85  89  88  91  90  86  83  84  84  83  85  84  85  85  86\n",
            "   83  82  85  84  85  86  86  85  87  83  86  86  86  86  84  85  84  86\n",
            "   84  85  86  85  85  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.021 -0.094  1.246 -0.358  1.12   0.508  3.737  3.245  1.319 -0.366\n",
            " -0.251  0.851 -0.419  0.328 -0.177 -0.302 -0.181 -0.209  0.181 -0.539\n",
            "  0.416 -0.698  0.532  0.122 -0.409 -0.372 -0.385 -0.135 -0.312  0.158\n",
            "  0.245 -1.374  0.162  0.942  0.357  0.125  0.507  0.651 -0.146 -0.142\n",
            " -0.484  0.49 ]\n",
            "Quantized input: \n",
            "[[253  84  87  84  87  86  92  91  87  84  84  86  84  85  84  84  84  84\n",
            "   85  83  85  83  86  85  84  84  84  84  84  85  85  82  85  86  85  85\n",
            "   86  86  84  84  84  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[81.668 -0.05   0.913 -0.219  3.181  2.326  3.979  1.952  1.757 -0.388\n",
            "  0.034  0.363  1.419 -0.132 -0.223  0.453  0.26  -0.153  0.101 -0.443\n",
            "  0.087 -0.749 -0.077  0.666 -0.491  0.092 -0.477  0.754  0.435  0.44\n",
            "  0.02  -0.135  0.608  0.714  0.445 -0.287 -1.325 -0.267 -0.267  0.6\n",
            " -0.581 -0.259]\n",
            "Quantized input: \n",
            "[[253  84  86  84  91  89  93  89  88  84  85  85  87  84  84  85  85  84\n",
            "   85  84  85  83  84  86  83  85  84  86  85  85  85  84  86  86  85  84\n",
            "   82  84  84  86  83  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[80.826 -0.263  1.474  1.722  3.61   0.838  3.707  3.869  3.348 -0.649\n",
            " -0.596  0.278  0.506 -1.085  0.144  0.356 -0.04  -0.552  0.553  0.215\n",
            " -0.169 -0.566 -1.949  0.503  0.699 -0.486 -0.808 -0.532  0.333 -0.679\n",
            "  0.181  1.238 -0.495  1.194 -1.403  0.423 -0.69   0.098 -0.31  -0.393\n",
            "  0.682 -0.932]\n",
            "Quantized input: \n",
            "[[251  84  88  88  92  86  92  92  91  83  83  85  86  82  85  85  84  83\n",
            "   86  85  84  83  80  86  86  84  83  83  85  83  85  87  83  87  82  85\n",
            "   83  85  84  84  86  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[80.072 -0.786  2.208  1.555  1.838  1.738  6.156  3.521  3.015 -1.047\n",
            " -0.278  1.353  0.342 -0.325 -0.282  0.87   0.089 -0.774 -0.338  0.438\n",
            " -0.419  1.326 -0.773 -0.073  0.244  0.083  0.329 -0.207 -0.031 -0.923\n",
            "  1.151 -1.124 -0.81  -0.783 -1.014  1.404  1.268 -2.28  -1.431  1.754\n",
            "  1.176 -0.019]\n",
            "Quantized input: \n",
            "[[249  83  89  88  88  88  97  92  91  82  84  87  85  84  84  86  85  83\n",
            "   84  85  84  87  83  84  85  85  85  84  84  83  87  82  83  83  82  87\n",
            "   87  80  82  88  87  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.54  -0.134  1.168  1.202  2.711  1.495  2.922  2.397  2.332  0.418\n",
            "  0.495 -0.102  1.438  0.494  0.714 -0.294  0.767 -0.577  1.074  0.92\n",
            "  0.345  0.019  1.593  1.02   1.267 -0.907  0.069  0.885  0.212  0.727\n",
            "  0.472  0.743  0.191  0.237 -0.628 -0.265  0.475  0.05   0.26   0.009\n",
            " -0.298 -0.449]\n",
            "Quantized input: \n",
            "[[244  84  87  87  90  88  91  89  89  85  86  84  87  86  86  84  86  83\n",
            "   87  86  85  85  88  87  87  83  85  86  85  86  85  86  85  85  83  84\n",
            "   85  85  85  85  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.484 -0.361  0.295  0.937  1.607  2.259  3.586  2.703  0.452  0.321\n",
            " -0.783  1.274  0.16   0.615  0.994  0.857  0.933  0.306  0.311  0.543\n",
            " -0.752  0.798  0.099 -1.347 -0.059 -0.53   0.852  1.264  0.965  0.609\n",
            " -0.013 -1.58  -0.202 -0.161 -0.512  0.135  0.26  -0.371 -0.072 -0.249\n",
            " -0.172  0.429]\n",
            "Quantized input: \n",
            "[[246  84  85  86  88  89  92  90  85  85  83  87  85  86  87  86  86  85\n",
            "   85  86  83  86  85  82  84  83  86  87  86  86  84  81  84  84  83  85\n",
            "   85  84  84  84  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.195 -0.948  0.531  0.561  2.283  0.123  3.928  2.224  1.62   0.32\n",
            "  0.393 -0.391 -1.26  -1.051  0.589  0.104  0.824  0.025  0.191  0.193\n",
            " -0.69   0.34  -1.408  0.572 -0.141  0.915 -0.313  0.279 -0.413  0.632\n",
            "  0.128  0.12  -0.222  0.118 -1.441  1.015  0.025  0.116  0.02  -0.761\n",
            "  0.217 -0.456]\n",
            "Quantized input: \n",
            "[[245  83  86  86  89  85  93  89  88  85  85  84  82  82  86  85  86  85\n",
            "   85  85  83  85  82  86  84  86  84  85  84  86  85  85  84  85  82  87\n",
            "   85  85  85  83  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[77.93  -0.395  1.587  0.893  1.637  0.641  3.65   2.632  0.724  0.296\n",
            " -0.915 -0.419 -0.561 -0.017 -0.065 -1.772  0.607  0.707 -0.739 -0.242\n",
            "  1.548 -0.427 -0.421  0.698 -0.199 -0.177  0.797 -0.298  0.994 -0.277\n",
            "  0.941  0.64  -0.093  0.231 -0.32  -0.012 -0.133  1.127 -0.131 -0.115\n",
            "  0.294  0.442]\n",
            "Quantized input: \n",
            "[[245  84  88  86  88  86  92  90  86  85  83  84  83  84  84  81  86  86\n",
            "   83  84  88  84  84  86  84  84  86  84  87  84  86  86  84  85  84  84\n",
            "   84  87  84  84  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.224 -0.77   1.427  0.146  1.853  1.926  2.94   1.712  0.005  0.8\n",
            "  0.606  1.587  0.775 -0.894 -0.979  0.58   1.274  0.012  0.267 -0.275\n",
            "  1.104  0.543 -0.972 -0.596 -0.444  0.229 -0.691 -1.178  0.037  0.108\n",
            "  0.253  0.232  0.899 -0.505 -0.891  0.554 -0.944  0.52   0.172  0.35\n",
            " -0.222 -0.047]\n",
            "Quantized input: \n",
            "[[245  83  87  85  88  88  91  88  85  86  86  88  86  83  82  86  87  85\n",
            "   85  84  87  86  82  83  84  85  83  82  85  85  85  85  86  83  83  86\n",
            "   83  86  85  85  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.463 -1.018  0.879  0.508  2.452  0.467  1.827  1.673  1.34   0.758\n",
            " -0.199 -1.555  0.216 -0.696  1.     1.354  0.559  0.768  1.096 -0.473\n",
            "  0.072  0.408 -0.419 -0.809 -0.254  1.044 -0.518 -0.577  0.381 -1.239\n",
            " -1.08  -0.73   0.711  0.334 -0.75   0.608 -0.344 -0.062 -0.415 -0.717\n",
            "  0.381  0.517]\n",
            "Quantized input: \n",
            "[[246  82  86  86  90  85  88  88  87  86  84  81  85  83  87  87  86  86\n",
            "   87  84  85  85  84  83  84  87  83  83  85  82  82  83  86  85  83  86\n",
            "   84  84  84  83  85  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[78.066 -0.44   0.165  0.347  1.18   0.276  3.939  2.508  2.256  0.892\n",
            " -1.244  2.174  0.111 -0.225  1.43  -0.074 -0.109  0.209  0.554  0.131\n",
            " -0.704  0.371  0.614  0.204 -0.098  0.155  0.648  1.339 -0.775 -0.402\n",
            " -0.609 -1.911  0.992 -1.374  0.095  0.312 -0.666 -1.337  0.148  0.293\n",
            "  0.435 -0.679]\n",
            "Quantized input: \n",
            "[[245  84  85  85  87  85  93  90  89  86  82  89  85  84  87  84  84  85\n",
            "   86  85  83  85  86  85  84  85  86  87  83  84  83  81  87  82  85  85\n",
            "   83  82  85  85  85  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[81.462  0.747  0.601 -0.317  0.039 -0.089  0.82   0.55   1.192  0.967\n",
            "  2.614  1.427  0.183  0.779 -0.352  0.066  0.676 -0.028  0.51  -0.067\n",
            "  1.378  1.09   0.084 -0.468 -0.172  0.254  0.011 -0.608  0.814  0.143\n",
            " -1.136 -1.363  0.483  0.203  0.752  0.084 -0.228 -0.675 -0.9    0.535\n",
            " -1.242  0.192]\n",
            "Quantized input: \n",
            "[[252  86  86  84  85  84  86  86  87  86  90  87  85  86  84  85  86  84\n",
            "   86  84  87  87  85  84  84  85  85  83  86  85  82  82  85  85  86  85\n",
            "   84  83  83  86  82  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[80.616  0.382  0.547  0.754  1.381  0.022  1.198  0.253 -1.146  0.409\n",
            " -2.944  0.74  -0.669 -1.261  1.342 -1.582  0.029  0.211  0.39   0.595\n",
            "  1.532  0.239 -0.839 -0.87   1.253  0.684 -0.73   0.046 -0.633  0.011\n",
            " -0.225  0.119  0.002  0.157 -0.558  0.175 -0.438  0.083  0.715  0.606\n",
            " -0.166  0.599]\n",
            "Quantized input: \n",
            "[[250  85  86  86  87  85  87  85  82  85  78  86  83  82  87  81  85  85\n",
            "   85  86  88  85  83  83  87  86  83  85  83  85  84  85  85  85  83  85\n",
            "   84  85  86  86  84  86]]\n",
            "Quantized output:\n",
            "[96]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[79.235  0.27   0.658 -0.465  2.474  0.371  1.787  0.973 -1.049 -0.003\n",
            "  2.639 -2.271  0.768  0.353  0.761 -0.911 -0.206 -1.276 -0.772  1.188\n",
            "  0.461  0.312 -0.11   0.224 -1.295 -0.344  1.48   0.15   0.114 -0.035\n",
            "  0.033 -0.358  0.455  0.994 -0.048  0.783  0.209  0.557 -0.533  1.224\n",
            "  1.184 -0.073]\n",
            "Quantized input: \n",
            "[[248  85  86  84  90  85  88  87  82  84  90  80  86  85  86  83  84  82\n",
            "   83  87  85  85  84  85  82  84  88  85  85  84  85  84  85  87  84  86\n",
            "   85  86  83  87  87  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.074  1.618 -0.048 -2.007  0.972 -0.016  2.18   1.485  0.173  0.647\n",
            " -2.232 -2.14   1.443 -0.184  0.979 -0.015  0.084  1.125  1.085 -0.834\n",
            "  0.989  0.027  0.111 -0.392 -0.522  0.28  -0.595  0.082 -0.155 -0.823\n",
            " -0.53  -0.168 -0.862  0.292 -0.663 -0.916 -0.207  0.099 -0.48  -0.114\n",
            "  0.082 -1.334]\n",
            "Quantized input: \n",
            "[[253  88  84  80  86  84  89  88  85  86  80  80  87  84  87  84  85  87\n",
            "   87  83  87  85  85  84  83  85  83  85  84  83  83  84  83  85  83  83\n",
            "   84  85  84  84  85  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[82.7    2.476  0.13  -0.792  2.96   0.61   2.17   0.833  1.854  1.174\n",
            "  0.556  2.985  0.01  -1.761  0.282 -0.981 -0.352  0.591  0.089 -0.73\n",
            "  0.2   -0.251 -0.559  0.066  0.055  0.867  0.02   0.213 -0.561  0.641\n",
            "  0.236 -0.641  0.098 -0.658  0.981  0.856  0.295 -0.155 -0.59  -0.495\n",
            "  0.094 -0.071]\n",
            "Quantized input: \n",
            "[[255  90  85  83  91  86  89  86  88  87  86  91  85  81  85  82  84  86\n",
            "   85  83  85  84  83  85  85  86  85  85  83  86  85  83  85  83  87  86\n",
            "   85  84  83  83  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[83.847  2.596  1.538  0.569  1.994 -0.276  2.899  2.562  0.754  0.347\n",
            "  0.064 -1.846  0.547  0.457 -0.245  1.741  0.419  0.085  0.706 -0.625\n",
            " -0.502  0.473  0.538 -0.117 -0.801 -0.603 -0.435  0.113  0.794  0.966\n",
            " -0.523 -0.722  0.137  0.353  0.126 -0.421  0.243 -0.215  0.825  0.159\n",
            "  0.375 -0.19 ]\n",
            "Quantized input: \n",
            "[[ 1 90 88 86 89 84 90 90 86 85 85 81 86 85 84 88 85 85 86 83 83 85 86 84\n",
            "  83 83 84 85 86 86 83 83 85 85 85 84 85 84 86 85 85 84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[83.821  1.588  3.057 -0.32   1.915  1.605  3.851  1.542  1.755  0.836\n",
            " -1.151  1.665  0.242 -1.191  1.854 -0.7   -0.765  1.134  0.05  -0.472\n",
            " -0.679 -1.282  0.243  0.309  0.277  0.038 -0.021  0.817  0.276 -0.064\n",
            " -0.071  0.855 -0.204 -0.153 -0.215 -0.489  0.118 -0.128 -0.577 -0.15\n",
            "  0.033  0.522]\n",
            "Quantized input: \n",
            "[[ 1 88 91 84 88 88 92 88 88 86 82 88 85 82 88 83 83 87 85 84 83 82 85 85\n",
            "  85 85 84 86 85 84 84 86 84 84 84 83 85 84 83 84 85 86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.425 -16.873  11.434  -1.621  -8.99  -10.489  -2.621   8.066   3.955\n",
            "   4.076  10.635  -3.247   2.736  -2.678  -2.13    0.406  -0.312   1.637\n",
            "  -0.187   2.095  -2.623  -3.8    -4.096  -1.525  -0.015   0.335  -0.738\n",
            "  -2.684   0.563   1.037   1.938  -0.475   1.673   0.407   1.011   3.279\n",
            "  -1.526  -1.291  -0.764   0.19    0.543   2.873]\n",
            "Quantized input: \n",
            "[[230  50 108  81  66  63  79 101  93  93 106  78  90  79  80  85  84  88\n",
            "   84  89  79  77  76  81  84  85  83  79  86  87  88  84  88  85  87  91\n",
            "   81  82  83  85  86  90]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.775  -5.608  17.053  12.522   8.666   6.403  -2.686  -8.346   1.183\n",
            "   3.235 -10.206   6.082   2.698   2.518  -4.003   0.295   0.761   2.169\n",
            "  -1.914   0.954  -0.189  -1.714   0.422   1.889  -3.251   2.407  -3.095\n",
            "   1.189  -0.827  -1.013   2.812   0.786  -0.703   0.649  -0.785  -1.866\n",
            "  -1.127   4.162   1.483  -0.844   1.203  -0.718]\n",
            "Quantized input: \n",
            "[[229  73 120 110 102  98  79  67  87  91  64  97  90  90  76  85  86  89\n",
            "   81  86  84  81  85  88  78  89  78  87  83  82  90  86  83  86  83  81\n",
            "   82  93  88  83  87  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.997   5.853   5.96    4.208  10.404   6.65   13.151  11.859 -11.835\n",
            "   2.099   7.684  -6.058  -3.389   2.611   2.957  -2.561  -1.214   2.89\n",
            "   6.561  -0.105   0.288  -0.693   2.1     0.969  -3.004  -3.48   -3.59\n",
            "  -2.665   1.108  -1.567   2.677   1.611   0.065  -1.16    0.705  -1.925\n",
            "   1.699  -1.803  -0.336  -2.148   0.629   2.193]\n",
            "Quantized input: \n",
            "[[230  97  97  93 106  98 112 109  60  89 100  72  78  90  91  79  82  90\n",
            "   98  84  85  83  89  86  78  77  77  79  87  81  90  88  85  82  86  81\n",
            "   88  81  84  80  86  89]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.562 -15.396  -9.396 -14.114  -2.021   6.599   9.629  -5.305  -0.756\n",
            "   1.934 -10.959  -2.741  -3.896  -2.863  -0.402   3.047  -0.441   0.049\n",
            "  -1.427   3.122  -4.192   3.397   0.497   2.052  -1.16    0.976  -1.791\n",
            "   0.913   1.175   0.359  -2.714  -0.915   3.529   0.615  -2.632  -1.229\n",
            "  -2.137  -1.217  -0.243   2.028  -1.387  -0.943]\n",
            "Quantized input: \n",
            "[[229  53  65  55  80  98 104  74  83  88  62  79  76  79  84  91  84  85\n",
            "   82  91  76  91  86  89  82  87  81  86  87  85  79  83  92  86  79  82\n",
            "   80  82  84  89  82  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.57   -3.403 -19.283   4.7    12.239   2.287  -7.783   6.993   1.797\n",
            "   4.131   9.222   6.07   -2.468   2.324   3.342   0.854  -1.519   1.846\n",
            "  -1.241   4.321  -1.279  -3.39    1.511  -1.975   2.106  -0.939   3.014\n",
            "   2.123  -0.694  -2.899  -1.152  -0.422  -0.341  -1.551  -0.898   1.797\n",
            "   0.693  -0.565  -3.059   0.231   0.102  -2.336]\n",
            "Quantized input: \n",
            "[[229  77  45  94 110  89  68  99  88  93 103  97  79  89  91  86  81  88\n",
            "   82  93  82  78  88  80  89  83  91  89  83  79  82  84  84  81  83  88\n",
            "   86  83  78  85  85  80]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.878   8.463  -8.213  17.345  -7.126  -6.838   7.252  -4.304  -3.824\n",
            "   1.944  -6.629  -7.954   3.92    5.502  -1.123   1.633   0.386  -2.639\n",
            "  -3.133   3.009   1.789   2.97   -0.182   4.324   3.511  -0.969   1.998\n",
            "   0.337  -1.604   1.083   0.241  -1.236   1.991  -0.628   0.189   1.101\n",
            "   1.803   0.718   2.001  -1.693  -0.069  -0.019]\n",
            "Quantized input: \n",
            "[[229 102  68 120  70  70  99  76  77  89  71  68  93  96  82  88  85  79\n",
            "   78  91  88  91  84  93  92  83  89  85  81  87  85  82  89  83  85  87\n",
            "   88  86  89  81  84  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-57.026  11.056   7.078  -1.273 -14.332  12.024  -7.493   2.111   0.575\n",
            "   5.003   4.09    4.581  -2.365  -5.902   1.27   -6.207  -0.858  -2.157\n",
            "  -3.466   2.033   4.216   0.021   0.648  -2.956  -1.55   -3.43    1.355\n",
            "  -0.666  -0.02   -2.11    1.473   2.006   0.66    2.75    0.901  -0.349\n",
            "   0.055  -1.488   3.212  -0.211  -1.037   1.935]\n",
            "Quantized input: \n",
            "[[224 107  99  82  55 109  69  89  86  95  93  94  80  72  87  72  83  80\n",
            "   77  89  93  85  86  78  81  77  87  83  84  80  88  89  86  90  86  84\n",
            "   85  81  91  84  82  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.498 -19.325  -5.098  -2.582  -9.239  -7.78   -8.935   4.117   0.869\n",
            "   4.206   9.603  -3.305   2.565  -1.872  -3.424   1.214  -1.212  -1.788\n",
            "   0.934  -3.654  -0.102   0.991  -1.753   3.266  -0.684   1.874   0.389\n",
            "  -1.751  -0.06   -0.572  -0.189   0.353   0.293  -0.746   2.037  -0.237\n",
            "   0.337   0.299   1.031  -1.747   0.748  -0.282]\n",
            "Quantized input: \n",
            "[[231  45  74  79  65  68  66  93  86  93 104  78  90  81  77  87  82  81\n",
            "   86  77  84  87  81  91  83  88  85  81  84  83  84  85  85  83  89  84\n",
            "   85  85  87  81  86  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.138 -18.381   9.143   7.724   0.783   5.087  -3.715 -10.389  -3.953\n",
            "   3.525  -7.994   5.514   0.857   1.788  -3.549  -1.52   -1.828   0.218\n",
            "  -1.458  -3.304   1.358  -3.319   2.52   -2.791   2.312   0.72    0.577\n",
            "  -1.269  -0.194  -2.273   0.472   1.336  -0.749  -0.697   1.735  -1.011\n",
            "  -0.188   1.464   0.413   0.054  -0.338  -1.668]\n",
            "Quantized input: \n",
            "[[230  47 103 100  86  95  77  63  76  92  68  96  86  88  77  81  81  85\n",
            "   82  78  87  78  90  79  89  86  86  82  84  80  85  87  83  83  88  82\n",
            "   84  88  85  85  84  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.7    -8.499  11.363   7.74    7.498   3.307   8.288   9.721 -12.198\n",
            "   2.219   6.401  -6.744  -2.977   2.392   3.379  -3.757  -3.674   1.281\n",
            "   3.316  -4.046   0.291   3.539   1.798   1.381  -0.624  -3.362  -1.67\n",
            "   1.626   0.394  -1.316  -0.628   2.1     1.658   0.237   0.323  -1.793\n",
            "   0.269  -0.143   1.373   1.595  -1.314   1.514]\n",
            "Quantized input: \n",
            "[[227  67 108 100 100  91 102 105  59  89  98  71  78  89  91  77  77  87\n",
            "   91  76  85  92  88  87  83  78  81  88  85  82  83  89  88  85  85  81\n",
            "   85  84  87  88  82  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.089  -6.334 -20.949  -7.001  -6.117   1.175   3.588  -8.32   -2.682\n",
            "   2.219  -9.389  -3.405  -4.744  -2.101  -0.113   1.957  -1.831  -0.265\n",
            "  -0.263  -3.441  -3.314   0.261   0.884  -3.08   -3.33   -1.809  -0.067\n",
            "  -0.742  -1.395  -0.762   0.324   0.617  -0.5    -0.333   0.271  -1.403\n",
            "  -0.389  -0.816  -0.164  -0.623   0.174   1.607]\n",
            "Quantized input: \n",
            "[[232  71  41  70  72  87  92  67  79  89  65  77  75  80  84  89  81  84\n",
            "   84  77  78  85  86  78  78  81  84  83  82  83  85  86  83  84  85  82\n",
            "   84  83  84  83  85  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.794  11.334 -19.14    4.508   1.347   2.597 -10.801   3.304   1.281\n",
            "   4.168   7.276   7.465  -0.636   2.025   0.482   1.949  -1.147  -0.843\n",
            "   0.663  -1.046  -3.882  -2.679   2.539   1.804   1.175   2.3    -2.746\n",
            "   1.86   -2.665   0.618  -0.908   0.316  -1.189  -0.707   0.964   0.088\n",
            "  -0.327  -0.444   1.028  -0.317  -0.484  -0.347]\n",
            "Quantized input: \n",
            "[[231 108  45  94  87  90  62  91  87  93  99 100  83  89  85  89  82  83\n",
            "   86  82  77  79  90  88  87  89  79  88  79  86  83  85  82  83  86  85\n",
            "   84  84  87  84  84  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-56.039  21.089  -1.085   6.26   -5.931  -7.861   2.527  -5.1     0.726\n",
            "   1.65   -5.081  -9.307   2.747   4.807  -2.006   2.033   0.307  -3.182\n",
            "  -1.659   0.035  -4.39   -2.181  -0.623   2.565  -1.715  -2.339   2.509\n",
            "   0.055   1.438   0.769   0.814   0.119  -3.093   1.309   0.855  -0.885\n",
            "   0.095   0.155   0.108  -2.109  -1.475  -0.53 ]\n",
            "Quantized input: \n",
            "[[226 128  82  97  72  68  90  74  86  88  74  65  90  94  80  89  85  78\n",
            "   81  85  75  80  83  90  81  80  90  85  87  86  86  85  78  87  86  83\n",
            "   85  85  85  80  81  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.678  17.902  13.313  -9.059  -6.559   4.912  -4.02    1.234  -1.174\n",
            "   4.545   1.778   5.947  -2.195  -6.272   2.856  -6.51   -1.459   1.435\n",
            "  -3.874   0.238  -3.86   -0.112  -0.425  -0.496   0.59    0.85    0.128\n",
            "   0.555  -0.208   3.492   1.238   2.826  -0.16   -1.971   0.765  -0.252\n",
            "   1.11   -0.283  -1.071  -2.049  -1.715   0.841]\n",
            "Quantized input: \n",
            "[[229 121 112  66  71  95  76  87  82  94  88  97  80  72  90  71  81  87\n",
            "   77  85  77  84  84  83  86  86  85  86  84  92  87  90  84  80  86  84\n",
            "   87  84  82  80  81  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-59.808 -27.783   2.803   3.915 -15.912  -9.842 -10.474  -3.062   0.399\n",
            "   3.484   2.676  -1.46   -2.421  -2.294   1.008   0.526  -0.65   -3.438\n",
            "   2.124  -3.762   0.017   2.723   4.469  -1.492   2.535   0.58    1.368\n",
            "  -0.781   3.16    4.291  -0.196  -0.108   2.209  -2.745   1.338   0.948\n",
            "  -0.58   -1.791   1.486  -0.981  -0.269  -0.665]\n",
            "Quantized input: \n",
            "[[218  27  90  93  52  64  63  78  85  92  90  81  80  80  87  86  83  77\n",
            "   89  77  85  90  94  81  90  86  87  83  91  93  84  84  89  79  87  86\n",
            "   83  81  88  82  84  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-60.865 -20.48   14.12   17.946   7.817   6.664  -5.73   -6.744 -12.141\n",
            "   3.259  -2.184   1.302   3.668  -2.626   0.571   0.114  -0.76   -0.461\n",
            "   0.557  -4.789   0.627   1.347  -1.488  -2.805   1.389   0.343   2.948\n",
            "   2.87   -0.36    0.402   2.568  -0.324  -1.864   1.47    0.554   1.138\n",
            "  -0.989   3.16    0.085   0.59    0.208  -1.31 ]\n",
            "Quantized input: \n",
            "[[216  42 114 121 101  98  73  71  60  91  80  87  92  79  86  85  83  84\n",
            "   86  75  86  87  81  79  87  85  91  90  84  85  90  84  81  88  86  87\n",
            "   82  91  85  86  85  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-63.385  -7.51   10.33    8.067  14.626  12.308  18.009  12.935 -12.932\n",
            "  -0.468   4.928  -4.247  -3.848   4.576  -1.72   -2.054  -0.05    1.027\n",
            "  -2.087  -4.994   0.172   0.178   0.321   0.203   0.873   1.643   2.294\n",
            "   0.656  -2.236   0.522  -0.473  -0.289   0.554   1.698  -2.473  -1.517\n",
            "   0.841  -0.217  -1.649   0.229  -0.621   0.263]\n",
            "Quantized input: \n",
            "[[211  69 106 101 115 110 122 111  58  84  95  76  77  94  81  80  84  87\n",
            "   80  74  85  85  85  85  86  88  89  86  80  86  84  84  86  88  79  81\n",
            "   86  84  81  85  83  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-59.837 -16.413 -22.631 -15.666 -13.882   1.844  -1.046  -4.853  -2.872\n",
            "   2.051  -2.615  -0.826  -4.197   4.682   0.393  -1.352  -0.403  -0.084\n",
            "   2.273  -4.646   0.445  -1.685   4.458   1.441  -3.173  -1.221   0.803\n",
            "   1.114   2.629  -1.755  -2.603   2.865   0.642  -4.168   0.163  -0.964\n",
            "   1.743  -1.82    0.418  -1.674  -0.419   0.775]\n",
            "Quantized input: \n",
            "[[218  51  38  52  56  88  82  75  79  89  79  83  76  94  85  82  84  84\n",
            "   89  75  85  81  94  87  78  82  86  87  90  81  79  90  86  76  85  83\n",
            "   88  81  85  81  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-61.198   7.614 -31.912  -0.803   6.832   1.428 -10.062  -2.057   0.585\n",
            "   2.293   1.293   2.6     3.255   3.274  -4.137  -1.975   1.284   0.867\n",
            "  -1.245  -4.382  -1.034  -3.942  -1.473   0.862  -0.83   -1.54   -1.743\n",
            "   2.344  -3.234  -1.364  -0.216   1.634  -0.606  -4.618  -0.429  -1.75\n",
            "   3.29    0.798   2.755  -3.227  -0.058  -0.736]\n",
            "Quantized input: \n",
            "[[216 100  19  83  99  87  64  80  86  89  87  90  91  91  76  80  87  86\n",
            "   82  75  82  76  81  86  83  81  81  89  78  82  84  88  83  75  84  81\n",
            "   91  86  90  78  84  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-61.567  26.638 -11.864  15.696  -6.586  -9.115  -0.77   -2.829  -0.78\n",
            "   0.878  -0.338  -4.676   1.69   -4.81   -4.04    0.657   2.773   0.17\n",
            "  -2.97   -4.047  -4.129  -1.037  -1.342   0.515  -0.357  -3.035  -3.583\n",
            "   1.023  -0.426  -1.15    2.648  -1.454  -2.075   0.164   4.463   0.617\n",
            "   2.754   0.034   0.837  -2.949  -0.996   0.878]\n",
            "Quantized input: \n",
            "[[215 139  60 117  71  66  83  79  83  86  84  75  88  75  76  86  90  85\n",
            "   78  76  76  82  82  86  84  78  77  87  84  82  90  82  80  85  94  86\n",
            "   90  85  86  78  82  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-62.231  27.713  15.061  -4.05  -15.164   6.123  -4.269  -2.465  -0.84\n",
            "   1.096  -0.262   2.658  -4.933  -5.37    0.884   5.277   3.489  -0.793\n",
            "  -1.884  -1.496  -5.802  -2.339   2.156   1.231  -0.191   0.343  -1.393\n",
            "  -1.878   2.089   0.855   3.346   0.511  -0.7     1.448   3.282  -4.428\n",
            "   0.284   0.721   1.494  -1.033  -0.673   1.093]\n",
            "Quantized input: \n",
            "[[213 142 115  76  53  97  76  79  83  87  84  90  74  73  86  95  92  83\n",
            "   81  81  73  80  89  87  84  85  82  81  89  86  91  86  83  87  91  75\n",
            "   85  86  88  82  83  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.215 -30.903   0.702   2.339 -15.357 -11.751  -7.422  -1.318  -1.447\n",
            "   3.683   0.647  -0.317  -0.921  -0.739  -0.934  -0.737   1.359  -0.288\n",
            "   0.96   -0.746   0.436  -1.772  -0.107   0.063   1.915  -2.099  -3.345\n",
            "   0.317  -0.924  -2.119   0.387   0.422  -1.243   1.689  -2.05   -1.311\n",
            "   0.452   0.862   0.905  -1.301   1.191   0.872]\n",
            "Quantized input: \n",
            "[[228  21  86  89  53  60  69  82  82  92  86  84  83  83  83  83  87  84\n",
            "   86  83  85  81  84  85  88  80  78  85  83  80  85  85  82  88  80  82\n",
            "   85  86  86  82  87  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.218 -23.943  16.093  17.066   7.662   7.421  -7.915  -6.565  -9.13\n",
            "   2.759  -0.244  -1.332  -0.24    1.408  -1.467  -2.167   0.478  -2.159\n",
            "  -0.822  -1.165  -1.146   1.531   0.628  -0.055  -1.67   -0.782   0.601\n",
            "   2.5    -0.828  -1.13    0.541   2.155   2.517  -2.025   0.759   0.677\n",
            "   0.831  -2.379   1.521  -1.535   0.034   1.217]\n",
            "Quantized input: \n",
            "[[228  35 118 120 100 100  68  71  66  90  84  82  84  87  81  80  85  80\n",
            "   83  82  82  88  86  84  81  83  86  90  83  82  86  89  90  80  86  86\n",
            "   86  80  88  81  85  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-56.278  -9.439  12.306  12.05   11.743  15.887  19.003  13.591 -14.02\n",
            "   0.317   2.727  -1.177  -0.475   2.335   2.427  -1.76   -1.043   0.949\n",
            "  -0.175  -1.912  -1.274  -1.775   1.762  -0.638   0.115   2.183   2.583\n",
            "  -1.858   1.7     0.987   0.362  -1.092   2.808  -2.472  -0.206   0.189\n",
            "   0.877   0.422   1.873  -2.023  -1.09   -0.283]\n",
            "Quantized input: \n",
            "[[226  65 110 109 109 117 124 112  56  85  90  82  84  89  89  81  82  86\n",
            "   84  81  82  81  88  83  85  89  90  81  88  87  85  82  90  79  84  85\n",
            "   86  85  88  80  82  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.559 -17.535 -25.913 -13.374 -13.161   5.099  -1.92   -6.947  -1.068\n",
            "   2.311  -0.757  -0.361  -0.512   0.162  -1.327   1.366   1.961   0.829\n",
            "  -2.717  -2.079  -0.527   0.438  -0.717   1.193  -0.468   2.035   0.191\n",
            "  -0.595   1.585  -1.14    3.348  -2.388  -1.714  -0.298   1.277   0.827\n",
            "  -0.463   0.077  -1.408  -0.383  -1.384   1.008]\n",
            "Quantized input: \n",
            "[[229  48  31  57  57  95  81  70  82  89  83  84  83  85  82  87  89  86\n",
            "   79  80  83  85  83  87  84  89  85  83  88  82  91  80  81  84  87  86\n",
            "   84  85  82  84  82  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-57.088   8.801 -33.461   1.262   5.186  -0.902 -10.323   1.966  -0.106\n",
            "   1.721   0.768   0.211   0.567  -1.338  -0.288   2.722   2.173  -3.202\n",
            "   1.773   1.153  -2.642  -0.595   1.369  -0.655  -1.992  -1.15    0.236\n",
            "  -1.07   -0.286   1.319   0.951   1.728   2.225  -0.718  -1.06    0.701\n",
            "   0.422  -1.276   1.631  -0.315  -2.002  -0.777]\n",
            "Quantized input: \n",
            "[[224 103  16  87  95  83  63  89  84  88  86  85  86  82  84  90  89  78\n",
            "   88  87  79  83  87  83  80  82  85  82  84  87  86  88  89  83  82  86\n",
            "   85  82  88  84  80  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-59.081  28.501 -11.254  15.454  -7.619  -6.014   2.272  -5.477   2.164\n",
            "   1.257  -0.952  -2.135  -3.711   2.684   1.674  -0.582   1.088   0.429\n",
            "   0.616   0.927  -2.061  -0.895  -0.137   1.547   0.048   1.196  -1.133\n",
            "   1.918   0.127  -1.436  -0.553   1.439   0.436   0.302  -1.358  -0.354\n",
            "   1.186  -0.005  -0.01   -1.419   0.299   0.56 ]\n",
            "Quantized input: \n",
            "[[220 143  61 116  69  72  89  73  89  87  83  80  77  90  88  83  87  85\n",
            "   86  86  80  83  84  88  85  87  82  88  85  82  83  87  85  85  82  84\n",
            "   87  84  84  82  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-59.743  28.733  16.732  -5.063 -12.916   6.117  -6.321   2.266  -2.538\n",
            "   2.443  -0.22    2.136   1.319   3.593  -1.303  -1.464   1.608   1.475\n",
            "  -1.528   1.021  -1.987  -0.672  -0.739   0.416   0.527  -2.76    1.06\n",
            "  -0.3    -1.003  -0.074   1.403  -1.666  -0.014  -1.137  -0.451   0.182\n",
            "  -0.2    -0.111  -0.732   1.653   0.204   0.879]\n",
            "Quantized input: \n",
            "[[219 144 119  74  58  97  71  89  79  90  84  89  87  92  82  81  88  88\n",
            "   81  87  80  83  83  85  86  79  87  84  82  84  87  81  84  82  84  85\n",
            "   84  84  83  88  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.26  -19.077   5.869   4.137 -11.516  -8.812  -8.267  -0.569   2.208\n",
            "   2.603   3.734  -4.873  -4.083  -2.252   3.449   2.015   3.416  -0.671\n",
            "  -0.374   0.141  -1.155   3.169   0.326  -1.3    -3.301  -1.456   0.324\n",
            "  -2.986  -0.717  -0.135   2.493   0.752  -0.716   2.477   2.23   -4.387\n",
            "   3.079   3.595  -2.115  -1.754  -2.624   3.61 ]\n",
            "Quantized input: \n",
            "[[230  45  97  93  61  66  67  83  89  90  92  74  76  80  92  89  92  83\n",
            "   84  85  82  91  85  82  78  82  85  78  83  84  90  86  83  90  89  75\n",
            "   91  92  80  81  79  92]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.641 -11.682  11.281  14.057   8.385   6.342  -3.511  -6.372  -7.948\n",
            "   2.989  -1.814   3.777   4.51   -4.368  -1.116   1.895   3.982  -2.156\n",
            "   0.353   1.218  -0.592   0.488  -2.429   2.602   1.889   0.127  -0.382\n",
            "  -1.24    0.478  -1.732   0.779  -0.177  -2.895  -2.259   1.263   0.187\n",
            "   2.716  -1.424   2.648  -1.473  -0.297  -1.679]\n",
            "Quantized input: \n",
            "[[227  60 108 113 102  98  77  71  68  91  81  92  94  76  82  88  93  80\n",
            "   85  87  83  86  80  90  88  85  84  82  85  81  86  84  79  80  87  85\n",
            "   90  82  90  81  84  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.731  -2.789   5.427   4.47   10.206   9.161  14.132  11.634 -12.03\n",
            "  -0.094   3.201  -4.869  -3.531   4.096  -0.535  -3.276   2.426   3.166\n",
            "  -5.781  -1.881  -1.505  -2.197   0.395  -3.735   0.883   0.447  -0.843\n",
            "   0.277   1.306  -1.212  -0.245   1.618  -1.609  -1.82    1.31    0.449\n",
            "   0.991   0.517  -0.377  -1.208  -0.317  -1.145]\n",
            "Quantized input: \n",
            "[[227  79  96  94 106 103 114 108  60  84  91  74  77  93  83  78  89  91\n",
            "   73  81  81  80  85  77  86  85  83  85  87  82  84  88  81  81  87  85\n",
            "   87  86  84  82  84  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.853 -12.953 -13.62  -14.465 -10.904   1.342   2.595  -3.266  -2.204\n",
            "   1.939  -5.537   4.158  -1.521   3.433   3.455   0.813   4.107   0.406\n",
            "   1.613   2.127  -1.123  -1.684   2.789  -0.143  -2.507   2.284  -4.548\n",
            "  -0.048  -2.003   0.122   2.194   1.655  -2.546  -3.047   4.391   1.016\n",
            "  -3.903  -0.835   2.743   3.195  -2.028  -1.415]\n",
            "Quantized input: \n",
            "[[229  58  56  55  62  87  90  78  80  88  73  93  81  92  92  86  93  85\n",
            "   88  89  82  81  90  84  79  89  75  84  80  85  89  88  79  78  94  87\n",
            "   76  83  90  91  80  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.109   3.346 -23.175  -2.362   8.726   2.748  -8.273  -1.09    2.735\n",
            "   2.26    5.379  -1.195   2.866   3.989  -0.167  -3.465   3.549   2.223\n",
            "  -0.935   2.738  -3.55   -4.148  -1.269   1.322  -2.097   2.396  -0.738\n",
            "  -4.9     1.574   3.927  -4.476   0.466   2.084   2.957   1.038   0.317\n",
            "   2.66   -1.909  -0.999  -2.536   0.136  -2.186]\n",
            "Quantized input: \n",
            "[[228  91  37  80 102  90  67  82  90  89  96  82  90  93  84  77  92  89\n",
            "   83  90  77  76  82  87  80  89  83  74  88  93  75  85  89  91  87  85\n",
            "   90  81  82  79  85  80]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.838  17.859  -9.974  13.622  -3.003  -9.191   2.706   0.042  -1.242\n",
            "   0.993  -5.363  -3.378  -0.301  -2.765  -3.056  -3.074   3.676   1.543\n",
            "  -1.051   4.822  -4.019   2.363  -4.409  -0.672  -1.149   1.618   1.224\n",
            "   5.694   0.361  -0.488  -3.329   2.602  -2.016  -2.285  -0.355  -4.578\n",
            "  -0.911  -0.107   0.646   0.038  -0.379   2.307]\n",
            "Quantized input: \n",
            "[[227 121  64 113  78  66  90  85  82  87  73  78  84  79  78  78  92  88\n",
            "   82  94  76  89  75  83  82  88  87  96  85  83  78  90  80  80  84  75\n",
            "   83  84  86  85  84  89]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-57.242  19.441  11.03   -0.333 -12.237   8.671  -0.765  -1.316   1.554\n",
            "   2.083   4.455   3.313  -4.031  -2.11   -1.278   1.931   3.335   3.25\n",
            "   0.453   5.498  -0.91    1.521   1.589  -2.324   4.791  -5.037  -1.471\n",
            "  -1.096  -1.385   2.792  -0.988  -0.829  -3.883  -2.356   1.375   1.395\n",
            "  -0.065   1.27   -2.107  -1.282   0.42   -2.704]\n",
            "Quantized input: \n",
            "[[224 125 107  84  59 102  83  82  88  89  94  91  76  80  82  88  91  91\n",
            "   85  96  83  88  88  80  94  74  81  82  82  90  82  83  77  80  87  87\n",
            "   84  87  80  82  85  79]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-45.632   5.203   3.362  -1.162   0.695  -1.252   1.294   1.136   1.677\n",
            "   4.162  -1.115   0.172   1.521  -0.126  -0.3     0.557   2.229  -0.371\n",
            "   1.667   0.527  -1.472   0.1     0.463  -0.104  -1.121  -0.72   -0.723\n",
            "  -0.408  -0.059   0.721  -1.192   0.982   0.965  -0.639   1.129   0.256\n",
            "   1.847  -0.588  -1.034   0.217  -0.727  -2.032]\n",
            "Quantized input: \n",
            "[[248  95  91  82  86  82  87  87  88  93  82  85  88  84  84  86  89  84\n",
            "   88  86  81  85  85  84  82  83  83  84  84  86  82  87  86  83  87  85\n",
            "   88  83  82  85  83  80]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-45.424   5.391  -0.562  -0.362   1.311  -0.331   0.258  -0.066   1.372\n",
            "   4.282  -0.031   0.559  -0.415   0.659   0.8    -0.599   2.133   1.368\n",
            "  -0.64   -1.19   -0.857  -0.274  -0.842   0.511  -0.013   1.478   0.271\n",
            "  -0.593  -0.88    0.142   1.595   1.102  -0.498  -0.427  -0.354  -0.975\n",
            "   0.067  -1.26    0.345   0.967  -2.499  -0.076]\n",
            "Quantized input: \n",
            "[[248  96  83  84  87  84  85  84  87  93  84  86  84  86  86  83  89  87\n",
            "   83  82  83  84  83  86  84  88  85  83  83  85  88  87  83  84  84  82\n",
            "   85  82  85  86  79  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-45.977   2.942  -3.254  -2.028   0.25   -0.501   0.679   0.389   1.39\n",
            "   4.061  -1.381  -0.482   0.087   0.031   0.845   0.618   2.605  -0.305\n",
            "   1.133  -0.461  -0.609  -1.04    1.223   0.987  -0.971  -1.205   0.354\n",
            "  -0.471   0.29   -0.385  -0.025   0.577  -0.341   2.308   0.077   0.477\n",
            "   0.574   0.085   0.12   -0.216  -1.108   1.17 ]\n",
            "Quantized input: \n",
            "[[247  91  78  80  85  83  86  85  87  93  82  84  85  85  86  86  90  84\n",
            "   87  84  83  82  87  87  83  82  85  84  85  84  84  86  84  89  85  85\n",
            "   86  85  85  84  82  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-45.271   2.651   4.95   -3.32    2.084   0.074   2.264   0.51    3.117\n",
            "   3.778   0.002  -1.049  -1.481  -0.012   0.759  -1.089   0.877   2.311\n",
            "   0.271  -0.07   -1.637  -0.521  -0.05   -0.042   1.328   0.826   0.279\n",
            "   0.424   0.444  -0.634  -0.447   0.085  -1.211  -0.079   1.372  -0.527\n",
            "   2.437   0.067   1.181   0.488   0.539  -0.093]\n",
            "Quantized input: \n",
            "[[248  90  95  78  89  85  89  86  91  92  85  82  81  84  86  82  86  89\n",
            "   85  84  81  83  84  84  87  86  85  85  85  83  84  85  82  84  87  83\n",
            "   90  85  87  86  86  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-44.966  -0.086   3.591  -2.929   4.184  -0.78    3.022   2.859   1.708\n",
            "   3.546  -1.559   2.067   1.31    0.212  -0.938   1.037   1.057   0.833\n",
            "   1.089   1.161  -0.535   0.74   -0.313   1.093   0.473  -1.127   2.155\n",
            "  -0.189  -0.214  -0.018  -0.042   0.636  -0.637   0.074   2.707  -1.705\n",
            "   1.446   0.866   0.861   0.663   2.126  -0.533]\n",
            "Quantized input: \n",
            "[[249  84  92  78  93  83  91  90  88  92  81  89  87  85  83  87  87  86\n",
            "   87  87  83  86  84  87  85  82  89  84  84  84  84  86  83  85  90  81\n",
            "   87  86  86  86  89  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-44.815  -1.939   2.083   0.377   3.872   0.592   4.367   1.165   4.356\n",
            "   3.335   0.818  -1.222   0.904  -0.864   0.794   0.293   0.173   1.784\n",
            "  -0.458   0.611   0.535   1.29    1.095   0.019   0.215  -0.613  -0.288\n",
            "  -1.331  -1.038   0.202   0.738   1.781  -0.711   0.222   1.677  -0.575\n",
            "  -0.454   0.95    0.258   0.028   0.407   2.463]\n",
            "Quantized input: \n",
            "[[249  81  89  85  92  86  93  87  93  91  86  82  86  83  86  85  85  88\n",
            "   84  86  86  87  87  85  85  83  84  82  82  85  86  88  83  85  88  83\n",
            "   84  86  85  85  85  90]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-44.898  -2.52    1.404   1.315   1.659   1.217   3.352   4.423   0.764\n",
            "   3.036  -2.113   0.62   -0.825  -0.457  -0.257   0.684   0.385   0.493\n",
            "  -0.982   0.397   0.868  -0.997   0.975  -0.328  -1.449   1.737  -0.648\n",
            "  -1.798  -0.908   0.82   -0.048   0.499  -0.538  -0.671   1.577   0.401\n",
            "  -0.267  -0.823   0.511   1.406  -0.318   0.354]\n",
            "Quantized input: \n",
            "[[249  79  87  87  88  87  91  94  86  91  80  86  83  84  84  86  85  86\n",
            "   82  85  86  82  87  84  82  88  83  81  83  86  84  86  83  83  88  85\n",
            "   84  83  86  87  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-47.282   1.401   0.183  -0.981  -0.356  -0.986   0.914   0.417   0.314\n",
            "   4.877   1.81   -0.95   -0.657   0.1    -1.05   -1.165   0.171   0.689\n",
            "   1.51   -0.93   -1.154  -0.056  -0.17    1.572   0.062  -1.07    0.182\n",
            "  -1.764   0.529  -0.499  -0.273  -0.19    0.374   0.232   0.418  -1.465\n",
            "   0.14    1.37    1.21   -2.043  -1.075   0.215]\n",
            "Quantized input: \n",
            "[[244  87  85  82  84  82  86  85  85  95  88  83  83  85  82  82  85  86\n",
            "   88  83  82  84  84  88  85  82  85  81  86  83  84  84  85  85  85  81\n",
            "   85  87  87  80  82  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-47.159   0.24   -0.631   0.715   0.2     0.449  -0.245   0.655   1.221\n",
            "   4.114  -2.199  -0.018   0.016   1.357  -1.703  -0.624   0.955  -1.026\n",
            "   0.672  -0.395  -1.137  -0.285   1.775  -1.134   0.92    2.011   0.542\n",
            "   1.341   0.055  -0.484   0.444   1.114   1.498  -1.681   1.628   0.36\n",
            "   0.329  -0.036   0.92    0.447  -0.065  -0.105]\n",
            "Quantized input: \n",
            "[[244  85  83  86  85  85  84  86  87  93  80  84  85  87  81  83  86  82\n",
            "   86  84  82  84  88  82  86  89  86  87  85  84  85  87  88  81  88  85\n",
            "   85  84  86  85  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-46.356  -1.54   -1.632   1.387   0.045   1.737   1.699   1.094   0.302\n",
            "   4.387   1.038  -0.495  -0.672   1.342   0.352  -0.888   0.042  -1.263\n",
            "   0.775  -1.123  -1.381   0.82    0.58   -0.162  -0.867  -0.84    0.668\n",
            "  -0.34   -0.147  -0.784   1.448   0.239   1.393   1.36   -0.79   -0.826\n",
            "   0.492  -0.401  -0.198   0.452   0.331   0.11 ]\n",
            "Quantized input: \n",
            "[[246  81  81  87  85  88  88  87  85  94  87  83  83  87  85  83  85  82\n",
            "   86  82  82  86  86  84  83  83  86  84  84  83  87  85  87  87  83  83\n",
            "   86  84  84  85  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-46.401   1.919   0.522  -1.847   0.546   0.345  -0.247  -0.741   0.153\n",
            "   4.584  -2.61   -0.024  -0.066  -0.174  -1.595   1.177   0.618   1.624\n",
            "  -0.634  -0.701  -1.331   0.445   1.029  -0.437  -1.575   0.397   0.026\n",
            "  -0.083  -0.671  -1.857   0.951   1.607  -0.974  -0.912   1.347  -0.412\n",
            "  -1.309  -0.402  -0.087  -1.848  -0.397   1.042]\n",
            "Quantized input: \n",
            "[[246  88  86  81  86  85  84  83  85  94  79  84  84  84  81  87  86  88\n",
            "   83  83  82  85  87  84  81  85  85  84  83  81  86  88  82  83  87  84\n",
            "   82  84  84  81  84  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-46.332   2.196   0.809  -1.936   1.388  -2.069   0.003   0.532   1.953\n",
            "   5.252   1.661   0.449   0.543  -1.243   0.518   1.797   0.224   1.063\n",
            "   0.378   0.278  -1.709  -1.676   0.92    1.358  -0.766   0.124  -1.559\n",
            "  -1.449  -2.279   0.907   0.923  -0.264  -0.175  -0.729  -0.344   0.579\n",
            "  -0.127  -1.421   0.454  -0.551   0.479  -0.499]\n",
            "Quantized input: \n",
            "[[246  89  86  81  87  80  85  86  89  95  88  85  86  82  86  88  85  87\n",
            "   85  85  81  81  86  87  83  85  81  82  80  86  86  84  84  83  84  86\n",
            "   84  82  85  83  85  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-45.361   2.463   0.901  -2.003   0.672  -0.721   2.212   0.125   1.606\n",
            "   4.626  -2.46   -1.462  -0.522   0.324   1.585   1.46   -0.046   0.824\n",
            "  -1.36    0.529  -2.235  -0.904  -0.698   1.018  -1.812   1.213   0.687\n",
            "   1.514   0.598   0.674  -0.24   -0.211  -0.23    1.952   0.289   0.508\n",
            "   1.224   0.449  -0.592  -0.155   0.753   0.865]\n",
            "Quantized input: \n",
            "[[248  90  86  80  86  83  89  85  88  94  79  81  83  85  88  88  84  86\n",
            "   82  86  80  83  83  87  81  87  86  88  86  86  84  84  84  89  85  86\n",
            "   87  85  83  84  86  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-44.192   2.389   1.396  -1.976   1.908   0.271   0.673   1.263  -0.038\n",
            "   5.428   1.091   1.552  -0.003   0.372   1.961  -0.531  -0.137  -0.41\n",
            "  -0.712   1.475  -2.133  -0.077  -1.101  -0.839   1.368  -0.635   0.267\n",
            "   0.826  -0.187   1.369   1.358   0.326  -0.296  -0.256  -0.352  -1.135\n",
            "  -1.321   1.018   0.713   0.745  -1.059   1.086]\n",
            "Quantized input: \n",
            "[[251  89  87  80  88  85  86  87  84  96  87  88  84  85  89  83  84  84\n",
            "   83  88  80  84  82  83  87  83  85  86  84  87  87  85  84  84  84  82\n",
            "   82  87  86  86  82  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.98    1.236   1.765  -1.262   1.567  -1.457   1.734  -0.583   0.233\n",
            "   6.104  -0.69    1.376  -0.058  -0.314   1.127  -0.651  -0.76    0.134\n",
            "   0.677   1.667  -2.066  -0.588  -0.004  -0.586   0.265  -1.014  -0.651\n",
            "   0.544   1.241   1.142   1.955   1.778  -0.911  -1.133  -0.318   1.149\n",
            "   0.297   1.644  -1.054  -0.546  -1.139  -1.142]\n",
            "Quantized input: \n",
            "[[230  87  88  82  88  82  88  83  85  97  83  87  84  84  87  83  83  85\n",
            "   86  88  80  83  84  83  85  82  83  86  87  87  89  88  83  82  84  87\n",
            "   85  88  82  83  82  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-54.406   1.723   1.273  -1.188   1.424   0.297  -0.056   1.774   0.678\n",
            "   5.461  -0.971  -2.396   0.161   1.068   0.468   1.503  -0.115   1.196\n",
            "   0.048   0.188  -2.085   0.925  -0.966   0.533  -1.358   1.475   0.233\n",
            "   1.568   2.098   1.741   1.279  -0.306  -1.477   0.416  -0.644  -2.184\n",
            "   0.412   0.514  -1.15   -0.048  -1.684  -1.188]\n",
            "Quantized input: \n",
            "[[230  88  87  82  87  85  84  88  86  96  83  80  85  87  85  88  84  87\n",
            "   85  85  80  86  83  86  82  88  85  88  89  88  87  84  81  85  83  80\n",
            "   85  86  82  84  81  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-55.558   1.89    0.902  -1.783   1.292  -0.73    2.388  -0.921   1.579\n",
            "   5.937   0.905   1.509  -0.797  -0.44   -1.123   0.673   0.197   0.091\n",
            "   0.807   0.7    -1.156  -1.6    -0.347  -0.805  -0.577   1.078  -1.394\n",
            "  -0.532   0.898   2.195   0.295  -1.127  -2.164   1.423  -0.317   1.046\n",
            "   0.569  -1.46    0.252  -0.324  -1.073   0.827]\n",
            "Quantized input: \n",
            "[[227  88  86  81  87  83  89  83  88  97  86  88  83  84  82  86  85  85\n",
            "   86  86  82  81  84  83  83  87  82  83  86  89  85  82  80  87  84  87\n",
            "   86  81  85  84  82  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.539   0.601   1.55   -1.266   0.79    0.206   0.479   0.916   3.526\n",
            "   5.417  -0.526  -0.623   0.293   1.35   -1.014   0.483   0.452  -0.279\n",
            "  -0.21    2.351  -0.699   0.275   0.655   1.371   0.03    0.789  -0.598\n",
            "  -1.028   2.662   0.319   0.378  -0.142  -0.553  -1.009  -0.755   0.961\n",
            "   1.067   0.627   1.759  -1.295   0.584  -0.527]\n",
            "Quantized input: \n",
            "[[231  86  88  82  86  85  85  86  92  96  83  83  85  87  82  85  85  84\n",
            "   84  89  83  85  86  87  85  86  83  82  90  85  85  84  83  82  83  86\n",
            "   87  86  88  82  86  83]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.965   0.706   0.781  -2.192   1.698  -0.959   3.221   2.422   1.825\n",
            "   5.047  -0.67   -0.631  -0.214   0.066  -0.475  -0.391   0.146   0.763\n",
            "  -0.215   2.168   0.067  -0.643  -1.269   0.344  -2.03   -0.682   0.73\n",
            "  -0.725   0.11   -0.928   1.392   0.443   0.483  -0.099  -1.126  -0.3\n",
            "   0.899  -0.314  -0.043   0.147   1.257   0.483]\n",
            "Quantized input: \n",
            "[[230  86  86  80  88  83  91  89  88  95  83  83  84  85  84  84  85  86\n",
            "   84  89  85  83  82  85  80  83  86  83  85  83  87  85  85  84  82  84\n",
            "   86  84  84  85  87  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.479   0.757   0.822  -0.236   2.803   1.724   2.436   0.714   1.29\n",
            "   5.079  -0.539   0.302  -0.095   0.034  -0.553   0.943   0.681   0.809\n",
            "  -0.443   1.843   0.148   0.822  -0.88    0.013   0.248   0.792  -2.01\n",
            "   0.201  -1.698  -0.779   0.8     0.142   1.359   0.278  -0.499   1.342\n",
            "   0.516  -0.208  -0.102   1.533  -0.812   0.807]\n",
            "Quantized input: \n",
            "[[231  86  86  84  90  88  90  86  87  95  83  85  84  85  83  86  86  86\n",
            "   84  88  85  86  83  85  85  86  80  85  81  83  86  85  87  85  83  87\n",
            "   86  84  84  88  83  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-53.977   0.227   2.19    0.912   2.042  -0.657   1.267   2.248   2.232\n",
            "   5.16   -0.438  -0.635   0.23    0.111   0.525   1.628   0.301   0.089\n",
            "  -0.24    1.874   0.17    0.191  -0.119  -0.815   0.595   0.817   0.068\n",
            "  -0.452   1.38   -1.556  -0.037  -2.272  -0.543   1.809   0.226  -0.471\n",
            "   0.619   0.437   0.75   -0.395  -1.318  -1.48 ]\n",
            "Quantized input: \n",
            "[[230  85  89  86  89  83  87  89  89  95  84  83  85  85  86  88  85  85\n",
            "   84  88  85  85  84  83  86  86  85  84  87  81  84  80  83  88  85  84\n",
            "   86  85  86  84  82  81]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-40.191   1.942   3.877   1.271   2.65   -1.836   2.49    4.161   3.967\n",
            "   5.083  -0.282   0.571   1.356  -1.665  -0.988   1.994   0.51   -3.037\n",
            "  -0.994   1.872  -3.119   1.512   0.537  -0.025   0.393   0.516  -0.512\n",
            "   0.057  -2.373  -0.026   2.15   -0.019   0.088  -0.57   -0.511  -0.93\n",
            "   0.826  -0.11    0.22    2.089  -0.145  -1.565]\n",
            "Quantized input: \n",
            "[[ 2 88 92 87 90 81 90 93 93 95 84 86 87 81 82 89 86 78 82 88 78 88 86 84\n",
            "  85 86 83 85 80 84 89 84 85 83 83 83 86 84 85 89 84 81]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-39.832   3.474   1.303  -1.054   4.602   1.951   2.354  -0.107   2.705\n",
            "   5.789  -1.386   0.607   1.224   0.16    0.235   0.591  -0.804   0.81\n",
            "  -1.603  -0.577  -2.299  -0.807  -3.234   0.629  -0.761  -0.818  -0.606\n",
            "  -1.129   1.303  -2.448  -0.128  -0.544  -1.476   1.154  -0.005   0.465\n",
            "   0.622  -1.769  -1.967  -0.772   0.832   1.941]\n",
            "Quantized input: \n",
            "[[ 3 92 87 82 94 89 89 84 90 96 82 86 87 85 85 86 83 86 81 83 80 83 78 86\n",
            "  83 83 83 82 87 79 84 83 81 87 84 85 86 81 80 83 86 88]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-40.938   3.776   0.703  -5.26    1.781  -2.614   3.462   1.939   1.326\n",
            "   5.875  -0.297  -0.445  -0.639   0.718  -0.202   0.697  -0.445  -0.366\n",
            "   0.521  -0.152  -0.672  -2.689  -0.954   1.554  -0.284  -0.162  -2.752\n",
            "  -0.429  -0.521   0.766  -0.701   1.219   0.471  -0.745   0.442   0.712\n",
            "  -0.059   1.607   2.278  -1.107   0.178  -1.703]\n",
            "Quantized input: \n",
            "[[ 0 92 86 74 88 79 92 88 87 97 84 84 83 86 84 86 84 84 86 84 83 79 83 88\n",
            "  84 84 79 84 83 86 83 87 85 83 85 86 84 88 89 82 85 81]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-40.409  -0.541   4.515  -1.262   0.785   1.928   6.276   1.742   2.581\n",
            "   4.512  -0.342  -1.874  -2.587   0.208   2.173   0.254  -0.272  -1.312\n",
            "   1.409   1.952  -1.121  -1.178   1.263   0.875   0.296   0.293   0.667\n",
            "  -0.628   0.326  -0.718   0.594  -0.766  -0.378   2.049   1.658   2.022\n",
            "   1.888   0.865  -0.562   1.168   0.025   0.59 ]\n",
            "Quantized input: \n",
            "[[ 1 83 94 82 86 88 97 88 90 94 84 81 79 85 89 85 84 82 87 89 82 82 87 86\n",
            "  85 85 86 83 85 83 86 83 84 89 88 89 88 86 83 87 85 86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-40.335  -2.967   2.886  -1.328   4.748   2.616   1.124   3.49    1.097\n",
            "   4.511  -1.527   2.293  -0.593   3.531  -0.069  -0.667   0.256  -0.133\n",
            "  -0.536   1.311  -0.232   0.928  -0.021   0.613   0.053   0.776   2.627\n",
            "  -0.636  -0.115   0.922   0.582  -0.273   0.845   0.354   0.829  -1.887\n",
            "  -0.161  -0.563   2.466   0.949  -0.979  -0.168]\n",
            "Quantized input: \n",
            "[[ 2 78 90 82 94 90 87 92 87 94 81 89 83 92 84 83 85 84 83 87 84 86 84 86\n",
            "  85 86 90 83 84 86 86 84 86 85 86 81 84 83 90 86 82 84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-39.824  -4.114   0.786   1.957   3.172  -2.131   3.476   0.285  -0.278\n",
            "   4.844   1.798  -2.405   2.028   0.99   -0.659  -2.131  -0.073   0.408\n",
            "   0.676   0.771   0.951   0.982  -0.474  -1.86    0.248   0.164   0.218\n",
            "   0.379  -0.564  -0.128   0.669   2.703   0.448   1.651   1.418  -0.329\n",
            "  -0.197   0.762  -1.37   -0.723   1.113  -0.131]\n",
            "Quantized input: \n",
            "[[ 3 76 86 89 91 80 92 85 84 94 88 80 89 87 83 80 84 85 86 86 86 87 84 81\n",
            "  85 85 85 85 83 84 86 90 85 88 87 84 84 86 82 83 87 84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-39.906  -3.436  -0.586   1.949  -1.766   2.561   0.923   1.251   1.937\n",
            "   4.973  -3.464   0.559  -0.275  -2.5    -1.517  -0.929  -0.237   0.305\n",
            "   1.752   0.538   1.798   0.758  -0.269   0.685   0.695   0.669  -2.466\n",
            "  -1.384   0.174   0.233  -0.502  -0.999  -1.09   -0.34   -0.457  -0.85\n",
            "  -2.156   0.616  -0.339   1.298   1.722   1.307]\n",
            "Quantized input: \n",
            "[[ 2 77 83 89 81 90 86 87 88 95 77 86 84 79 81 83 84 85 88 86 88 86 84 86\n",
            "  86 86 79 82 85 85 83 82 82 84 84 83 80 86 84 87 88 87]]\n",
            "Quantized output:\n",
            "[160]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-48.753   1.326  -1.346  -0.406  -0.514   2.38    0.461   0.009  -0.814\n",
            "   4.523  -1.159   0.023   0.062   0.905   1.53   -1.125   0.11    3.15\n",
            "  -2.289   0.373   0.879  -1.357   1.076   0.446  -0.514   2.129   2.298\n",
            "   0.612   0.894   0.717  -0.756   0.186   2.328  -0.178   0.765  -1.827\n",
            "   1.005  -0.832  -1.491   0.922  -1.388   0.939]\n",
            "Quantized input: \n",
            "[[241  87  82  84  83  89  85  85  83  94  82  85  85  86  88  82  85  91\n",
            "   80  85  86  82  87  85  83  89  89  86  86  86  83  85  89  84  86  81\n",
            "   87  83  81  86  82  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-50.67   -0.442  -0.895  -1.252   1.281  -3.63    2.699   0.144   1.182\n",
            "   4.225   0.22   -0.789  -0.645   0.436   1.295   0.893   0.901   2.196\n",
            "   0.45    1.955   0.023   0.066   2.601   0.534   0.408  -1.082  -0.889\n",
            "  -0.543  -0.021   0.322  -0.397  -0.506   0.741   1.246   0.078   1.252\n",
            "   0.113  -1.686   1.514   0.74    0.02   -1.053]\n",
            "Quantized input: \n",
            "[[237  84  83  82  87  77  90  85  87  93  85  83  83  85  87  86  86  89\n",
            "   85  89  85  85  90  86  85  82  83  83  84  85  84  83  86  87  85  87\n",
            "   85  81  88  86  85  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-50.962  -1.248   1.204  -2.21    2.191   0.472  -2.003   0.813   1.784\n",
            "   4.15   -0.943   0.748   0.155  -1.194   0.506   0.543   1.574   2.354\n",
            "   0.114   1.953  -1.582   1.313   0.337  -0.047   0.167   0.072   1.819\n",
            "   0.672  -2.386  -0.473   1.969   0.389   1.852  -0.883   1.404   1.095\n",
            "  -0.959   0.767  -0.653  -0.431  -1.661   1.087]\n",
            "Quantized input: \n",
            "[[237  82  87  80  89  85  80  86  88  93  83  86  85  82  86  86  88  89\n",
            "   85  89  81  87  85  84  85  85  88  86  80  84  89  85  88  83  87  87\n",
            "   83  86  83  84  81  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-48.703   2.538   0.492  -0.845   1.441  -1.598  -1.658  -0.149  -0.943\n",
            "   4.803   0.722  -0.532   0.551   0.047  -0.766   0.118   0.924  -0.816\n",
            "  -0.043   1.13    1.326   0.152  -1.869  -0.184   0.558  -1.505  -0.593\n",
            "   1.756  -0.812   0.347  -1.125   2.204   1.015  -1.857   0.867  -0.466\n",
            "   0.153   0.924   1.598   0.568  -0.288  -2.108]\n",
            "Quantized input: \n",
            "[[241  90  86  83  87  81  81  84  83  94  86  83  86  85  83  85  86  83\n",
            "   84  87  87  85  81  84  86  81  83  88  83  85  82  89  87  81  86  84\n",
            "   85  86  88  86  84  80]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-48.557   2.516   1.509  -1.468  -0.426  -1.966   0.87   -1.334   3.068\n",
            "   4.566  -1.792  -0.47   -0.351  -0.941   0.63    0.491   0.7     0.654\n",
            "  -1.148  -0.167   1.097   0.555  -0.376  -0.908   1.692  -1.085  -2.167\n",
            "  -1.405   1.758  -1.451  -2.185  -0.515  -1.47   -0.534   1.51   -0.029\n",
            "   1.317   0.115  -0.238  -2.547  -0.003   1.677]\n",
            "Quantized input: \n",
            "[[242  90  88  81  84  80  86  82  91  94  81  84  84  83  86  86  86  86\n",
            "   82  84  87  86  84  83  88  82  80  82  88  82  80  83  81  83  88  84\n",
            "   87  85  84  79  84  88]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-48.619   2.689   1.159  -3.949   0.549  -0.009   1.785   3.008   2.704\n",
            "   4.286   0.264   1.317   0.5    -0.164   0.045   1.985   1.696  -0.934\n",
            "  -1.195  -0.173   0.37   -0.053   1.052   1.877  -1.609  -0.71   -0.68\n",
            "  -1.114  -1.197  -1.064   0.32   -1.102  -1.733  -1.775   0.121  -1.962\n",
            "  -0.392  -1.426   0.274   0.616   0.169   0.814]\n",
            "Quantized input: \n",
            "[[241  90  87  76  86  84  88  91  90  93  85  87  86  84  85  89  88  83\n",
            "   82  84  85  84  87  88  81  83  83  82  82  82  85  82  81  81  85  80\n",
            "   84  82  85  86  85  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-49.275   2.46    1.388  -1.936   4.929   0.381   3.894   0.961   4.115\n",
            "   3.389  -1.165  -1.313   0.481   0.318   1.159   1.463   1.963  -1.137\n",
            "  -0.904   0.131  -0.826  -2.371  -0.302   0.042  -1.847   0.233  -0.882\n",
            "  -0.382  -2.513   0.323   0.007   1.369   0.108   0.942   0.93    2.154\n",
            "   1.151  -0.001  -0.381  -0.162   0.537   0.273]\n",
            "Quantized input: \n",
            "[[240  90  87  81  95  85  93  86  93  91  82  82  85  85  87  88  89  82\n",
            "   83  85  83  80  84  85  81  85  83  84  79  85  85  87  85  86  86  89\n",
            "   87  84  84  84  86  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.84   -7.584   4.617  -0.88   -1.703  -0.496   0.753  -7.014   1.071\n",
            "  -4.556   1.222  -3.05   -0.609  -0.591   1.889  -2.971  -0.718   0.48\n",
            "  -0.818  -2.809   1.169   0.342  -0.35   -0.263   0.126  -1.809   1.271\n",
            "  -0.751  -0.342   0.191  -1.321   0.951  -0.643  -1.69   -0.349  -0.014\n",
            "  -0.664  -2.065   0.783  -0.678   0.534  -0.776]\n",
            "Quantized input: \n",
            "[[179  69  94  83  81  83  86  70  87  75  87  78  83  83  88  78  83  85\n",
            "   83  79  87  85  84  84  85  81  87  83  84  85  82  86  83  81  84  84\n",
            "   83  80  86  83  86  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-80.183  -2.702   7.49    2.76    2.874  -1.84   -1.106   4.533  -4.475\n",
            "  -4.457   0.455   2.486   3.014   0.616   0.181   0.366  -0.095  -2.948\n",
            "   1.125  -0.463   1.372  -0.543   1.364   1.206   0.679   0.06    1.47\n",
            "   1.319   1.283   0.601  -3.25   -0.323   0.548  -0.613   1.017  -0.197\n",
            "  -0.922   0.693   0.849  -0.576  -0.433   1.387]\n",
            "Quantized input: \n",
            "[[177  79 100  90  90  81  82  94  75  75  85  90  91  86  85  85  84  78\n",
            "   87  84  87  83  87  87  86  85  88  87  87  86  78  84  86  83  87  84\n",
            "   83  86  86  83  84  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-80.06    2.023   4.246   2.34    5.221   5.983   4.76   -0.467   1.405\n",
            "  -5.626   0.818  -0.937  -3.934   1.42    1.165  -1.444  -1.236  -0.15\n",
            "  -3.937  -1.568  -0.143   0.452   1.581  -2.078   2.026   0.348   0.926\n",
            "  -0.25   -1.047   1.315  -1.18   -1.173   0.376  -0.462   0.154   0.717\n",
            "   0.856   0.398  -1.082  -0.326  -0.464  -1.205]\n",
            "Quantized input: \n",
            "[[177  89  93  89  95  97  94  84  87  73  86  83  76  87  87  82  82  84\n",
            "   76  81  84  85  88  80  89  85  86  84  82  87  82  82  85  84  85  86\n",
            "   86  85  82  84  84  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.59   -6.798  -3.927  -4.882  -1.716  -1.515  -4.907   3.666  -1.908\n",
            "  -4.243  -1.954   4.11    0.392  -1.54   -0.617  -0.531   0.994  -0.191\n",
            "   0.211  -2.566   1.609  -0.059   1.164  -0.092  -0.143  -0.107  -1.544\n",
            "   1.671  -1.443  -0.242  -1.544   0.061  -1.27   -0.929   0.149   0.704\n",
            "   2.625  -1.922  -1.211  -0.218   1.738   0.635]\n",
            "Quantized input: \n",
            "[[182  71  76  74  81  81  74  92  81  76  80  93  85  81  83  83  87  84\n",
            "   85  79  88  84  87  84  84  84  81  88  82  84  81  85  82  83  85  86\n",
            "   90  81  82  84  88  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.852  -0.588  -9.023  -1.019   1.163  -2.328   1.414  -6.538   2.617\n",
            "  -3.889   3.32   -1.206   1.448  -0.297  -0.291   2.141   1.782  -0.908\n",
            "   1.117  -2.719   0.714  -1.654  -0.111   1.106  -0.094  -1.541  -0.184\n",
            "   1.387  -0.381   1.498  -0.005  -1.885  -1.541   0.696   0.235  -1.394\n",
            "   1.367   2.001   0.537   1.412   0.626   1.113]\n",
            "Quantized input: \n",
            "[[181  83  66  82  87  80  87  71  90  76  91  82  87  84  84  89  88  83\n",
            "   87  79  86  81  84  87  84  81  84  87  84  88  84  81  81  86  85  82\n",
            "   87  89  86  87  86  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.097   6.762  -5.916   2.651  -2.874   0.431  -2.866   4.883   2.657\n",
            "  -4.448  -3.982   0.883  -0.025  -1.9     2.506  -0.52    0.563   0.847\n",
            "  -1.206  -3.105  -1.558  -0.165  -0.095  -1.07    0.39    0.634   1.22\n",
            "   0.873   2.345   2.289   0.155   1.483   0.573  -1.197   2.795   0.763\n",
            "  -0.518   0.652   1.859  -0.589  -1.427  -0.712]\n",
            "Quantized input: \n",
            "[[181  98  72  90  79  85  79  95  90  75  76  86  84  81  90  83  86  86\n",
            "   82  78  81  84  84  82  85  86  87  86  89  89  85  88  86  82  90  86\n",
            "   83  86  88  83  82  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-80.243   9.81    2.494  -1.623  -0.652  -0.639   3.361  -2.493   0.463\n",
            "  -3.738   4.02    2.015  -0.864   1.676  -1.324   0.687   2.056  -3.456\n",
            "   2.13   -0.596  -1.951  -0.167   0.72    0.838   0.835  -1.673  -1.167\n",
            "  -1.143   1.068  -0.12   -1.427   3.982  -0.039  -1.989  -0.214  -2.011\n",
            "   0.569  -1.355   0.995  -2.176  -0.274   0.858]\n",
            "Quantized input: \n",
            "[[176 105  90  81  83  83  91  79  85  77  93  89  83  88  82  86  89  77\n",
            "   89  83  80  84  86  86  86  81  82  82  87  84  82  93  84  80  84  80\n",
            "   86  82  87  80  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.538 -10.842  -2.963   0.272 -13.002  -1.152  -0.219  -5.488   0.082\n",
            "  -1.707  -5.792   2.207  -4.043   3.276   1.824  -0.128   0.095  -1.089\n",
            "  -2.265   1.671  -0.179  -0.413  -0.379   1.616  -0.886  -1.929  -1.667\n",
            "   0.47   -0.929  -0.64    0.283  -0.234  -1.531   0.225  -0.937  -0.956\n",
            "  -1.849   0.5    -0.477  -0.115   1.633   1.641]\n",
            "Quantized input: \n",
            "[[192  62  78  85  58  82  84  73  85  81  73  89  76  91  88  84  85  82\n",
            "   80  88  84  84  84  88  83  81  81  85  83  83  85  84  81  85  83  83\n",
            "   81  86  84  84  88  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.201 -12.163   2.536  11.098   1.292  -3.887  -5.71    0.682  -3.723\n",
            "  -0.786   5.022  -3.825  -0.788  -3.218   3.67    1.026  -0.9    -2.091\n",
            "   1.575   2.248  -1.223  -0.851   0.407   1.278  -0.67   -1.272  -0.798\n",
            "   0.147   0.169   0.347  -1.029   1.532   1.435  -0.999   1.139  -1.013\n",
            "  -0.247  -0.852   1.026  -2.698   1.679   1.964]\n",
            "Quantized input: \n",
            "[[193  59  90 107  87  77  73  86  77  83  95  77  83  78  92  87  83  80\n",
            "   88  89  82  83  85  87  83  82  83  85  85  85  82  88  87  82  87  82\n",
            "   84  83  87  79  88  89]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-73.574  -8.686   3.169   3.775  10.704   9.359   5.05   -0.363  -6.324\n",
            "  -2.17   -1.659   2.999   2.741  -1.33   -1.228   1.916  -0.065   1.166\n",
            "  -5.436   0.142  -2.081  -0.48    1.585  -1.95   -0.277   2.864   1.652\n",
            "  -0.71    0.151   1.567   1.2     0.052   1.376  -2.522   2.59   -0.562\n",
            "  -0.642  -0.296   2.098  -1.706   0.199  -0.927]\n",
            "Quantized input: \n",
            "[[190  67  91  92 107 104  95  84  71  80  81  91  90  82  82  88  84  87\n",
            "   73  85  80  84  88  80  84  90  88  83  85  88  87  85  87  79  90  83\n",
            "   83  84  89  81  85  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-74.172  -2.508 -12.031  -9.49   -3.224   2.813  -6.916   1.709   1.161\n",
            "  -1.175   5.863   1.946   4.292   2.813  -1.359  -0.195   1.961  -2.865\n",
            "  -2.618   2.458   0.016  -1.891  -0.21   -0.572   0.206   0.384  -0.189\n",
            "  -0.532  -0.657   0.603  -0.046  -0.205  -0.134   1.608  -1.544   1.123\n",
            "  -2.373   2.2    -1.046  -0.605  -0.474   1.924]\n",
            "Quantized input: \n",
            "[[189  79  60  65  78  90  70  88  87  82  97  89  93  90  82  84  89  79\n",
            "   79  90  85  81  84  83  85  85  84  83  83  86  84  84  84  88  81  87\n",
            "   80  89  82  83  84  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Evaluated on 200 results so far.\n",
            "Non-quantized input:\n",
            "[-75.544   8.781 -12.641   0.122   5.184  -5.907   1.123  -3.339   0.622\n",
            "  -2.412  -4.066  -5.866   0.242  -3.305  -0.057  -1.343   2.138  -2.774\n",
            "   0.214   2.079   0.444   0.683  -2.681   1.539   0.34    0.538   0.613\n",
            "  -0.095   0.04    0.542   0.037  -0.523  -0.504   1.531  -2.713   1.67\n",
            "  -3.267  -0.115  -0.282   0.15   -0.827  -1.203]\n",
            "Quantized input: \n",
            "[[186 103  58  85  95  72  87  78  86  80  76  72  85  78  84  82  89  79\n",
            "   85  89  85  86  79  88  85  86  86  84  85  86  85  83  83  88  79  88\n",
            "   78  84  84  85  83  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.466  15.173  -0.461   6.165  -4.208   1.543   0.417   1.835   3.686\n",
            "  -1.413   1.669   6.953  -4.068  -2.142   2.18   -0.718   1.488   0.49\n",
            "   1.143   1.388   1.061   2.372  -0.569  -2.411   0.176   0.396   1.642\n",
            "   0.463   1.117   1.071  -0.355  -1.278   0.451   1.077  -2.66    0.597\n",
            "  -1.197   0.334  -2.377  -0.949   0.104  -1.757]\n",
            "Quantized input: \n",
            "[[186 116  84  97  76  88  85  88  92  82  88  99  76  80  89  83  88  86\n",
            "   87  87  87  89  83  80  85  85  88  85  87  87  84  82  85  87  79  86\n",
            "   82  85  80  83  85  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-76.834  11.871  12.163  -5.299  -1.134   0.45    0.345   0.839   0.159\n",
            "  -2.453   0.402  -4.224   1.832   7.403  -1.142   3.504   3.065  -1.048\n",
            "   2.947   0.818   1.605   2.021   2.812  -0.505   1.17    0.524   1.382\n",
            "   1.844   0.88    0.942  -0.446  -0.072   1.613  -2.622  -1.424   0.611\n",
            "  -1.483   0.294  -0.142  -0.484  -0.416   0.956]\n",
            "Quantized input: \n",
            "[[183 109 110  74  82  85  85  86  85  79  85  76  88 100  82  92  91  82\n",
            "   91  86  88  89  90  83  87  86  87  88  86  86  84  84  88  79  82  86\n",
            "   81  85  84  84  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.192 -10.36    5.899  -1.843   1.233   0.031  -1.55   -0.558   3.714\n",
            "  -2.636   3.292  -2.092  -2.033   0.016   3.441  -4.53   -2.64    1.437\n",
            "   2.679   0.431   2.246   1.552   0.669   0.125   0.358  -0.808   0.563\n",
            "   1.211   1.469  -0.69    0.018   1.357   0.093  -1.672   3.257  -0.51\n",
            "   2.264  -2.892   2.465  -3.147   0.466   0.703]\n",
            "Quantized input: \n",
            "[[183  63  97  81  87  85  81  83  92  79  91  80  80  85  92  75  79  87\n",
            "   90  85  89  88  86  85  85  83  86  87  88  83  85  87  85  81  91  83\n",
            "   89  79  90  78  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.069  -3.406  11.026   1.199   3.227   0.395   3.212  -0.24   -3.114\n",
            "  -3.477  -2.755   2.043   3.88   -0.239   0.473   1.819  -0.676  -3.228\n",
            "   2.457   2.609   0.4     1.255   2.305  -1.536   0.926   1.752   3.264\n",
            "   1.193   0.785   1.533  -0.34   -1.097   2.104  -0.931   1.851   0.533\n",
            "   1.726   0.274   2.657  -0.527  -0.661  -1.91 ]\n",
            "Quantized input: \n",
            "[[181  77 107  87  91  85  91  84  78  77  79  89  92  84  85  88  83  78\n",
            "   90  90  85  87  89  81  86  88  91  87  86  88  84  82  89  83  88  86\n",
            "   88  85  90  83  83  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.848   4.205   7.696   2.576   5.548   2.465   4.187   5.136  -0.146\n",
            "  -4.659   3.62   -0.688  -2.765   1.123   1.835  -0.854  -1.557   2.7\n",
            "  -3.783  -0.487  -0.354   1.95   -0.351  -2.205   1.088   2.851   1.021\n",
            "  -0.838  -0.405  -1.069   1.648  -2.292  -1.041   0.058  -0.94    0.926\n",
            "   1.358  -0.972  -1.281   1.577   0.315  -0.092]\n",
            "Quantized input: \n",
            "[[181  93 100  90  96  90  93  95  84  75  92  83  79  87  88  83  81  90\n",
            "   77  83  84  89  84  80  87  90  87  83  84  82  88  80  82  85  83  86\n",
            "   87  83  82  88  85  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-74.405  -9.986  -4.726  -3.051  -0.381  -3.167   0.303  -0.231  -5.495\n",
            "  -2.64   -3.598   2.12   -0.178   3.145  -3.191  -1.849  -0.671   2.245\n",
            "  -0.747  -1.578   2.912   0.28    0.818   0.568  -1.865  -1.841  -4.073\n",
            "  -1.634  -1.341  -0.708   3.19   -0.37   -1.665   0.723   0.04   -5.495\n",
            "   0.805   1.858  -1.109  -0.969   1.01    2.175]\n",
            "Quantized input: \n",
            "[[188  64  75  78  84  78  85  84  73  79  77  89  84  91  78  81  83  89\n",
            "   83  81  90  85  86  86  81  81  76  81  82  83  91  84  81  86  85  73\n",
            "   86  88  82  83  87  89]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-73.806  -3.402 -10.125   2.134  -1.129   1.711  -3.711  -4.93    1.366\n",
            "  -1.477   4.474   0.943   5.281  -3.889  -2.161   1.521  -0.212   1.386\n",
            "  -0.306  -0.7     0.758  -3.097  -1.491  -0.291  -2.604   2.483  -4.382\n",
            "  -0.832   0.583  -1.176   4.259  -0.065  -2.289   1.233   1.942   2.486\n",
            "  -3.059   3.176  -0.476   0.79   -1.34   -0.084]\n",
            "Quantized input: \n",
            "[[190  77  64  89  82  88  77  74  87  81  94  86  95  76  80  88  84  87\n",
            "   84  83  86  78  81  84  79  90  75  83  86  82  93  84  80  87  88  90\n",
            "   78  91  84  86  82  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.245   5.288  -7.44    2.073  -4.936  -3.823  -3.903   1.787   3.081\n",
            "  -3.494  -4.1    -3.013  -2.756  -3.345  -0.581   4.473  -0.998  -2.044\n",
            "   1.974   1.812  -4.144  -0.893  -3.463   0.3    -1.794   2.01   -1.415\n",
            "  -4.043   2.355   2.14   -0.225  -1.09    2.54    0.741  -1.322   0.043\n",
            "  -1.395  -1.717   0.276  -0.952  -1.742   0.23 ]\n",
            "Quantized input: \n",
            "[[187  95  69  89  74  77  76  88  91  77  76  78  79  78  83  94  82  80\n",
            "   89  88  76  83  77  85  81  89  82  76  89  89  84  82  90  86  82  85\n",
            "   82  81  85  83  81  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-76.639  10.902  -1.5    -3.855  -3.826   0.725   2.569  -1.977   0.862\n",
            "  -3.019   3.112   4.246  -3.836   3.362   2.793   0.788  -2.529   0.817\n",
            "  -0.555   3.291  -6.264   1.894   1.023  -0.928  -2.611   0.721  -0.2\n",
            "   3.926   0.231  -2.03   -1.056   0.758   1.369  -1.882  -1.764  -0.171\n",
            "  -1.025  -1.277  -1.977   0.559   0.42   -1.597]\n",
            "Quantized input: \n",
            "[[184 107  81  77  77  86  90  80  86  78  91  93  77  91  90  86  79  86\n",
            "   83  91  72  88  87  83  79  86  84  93  85  80  82  86  87  81  81  84\n",
            "   82  82  80  86  85  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-80.438  -1.032  -3.9    -6.648  -5.399  -3.06   -0.875  -0.55    2.932\n",
            "  -4.807  -1.727  -1.577  -1.306  -0.28   -1.089  -0.128  -2.174   0.473\n",
            "   3.12    0.602   2.163   2.694   1.212   0.591  -0.888  -1.756  -0.956\n",
            "  -2.263   1.035  -0.131   0.273   2.129   0.595   2.052  -0.209   2.103\n",
            "  -0.145   1.012   0.103   2.513   0.136   0.135]\n",
            "Quantized input: \n",
            "[[176  82  76  71  73  78  83  83  91  75  81  81  82  84  82  84  80  85\n",
            "   91  86  89  90  87  86  83  81  83  80  87  84  85  89  86  89  84  89\n",
            "   84  87  85  90  85  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.962  -4.333   0.314   2.699  -5.132  -1.695  -6.032  -2.955   1.413\n",
            "  -3.731   2.078   0.973   2.192  -0.847  -1.782  -0.322  -1.14   -1.498\n",
            "   3.114   1.768   1.333   1.462  -1.184  -0.062   2.356   0.39    2.246\n",
            "   0.199   0.683   0.985   3.09   -1.962  -0.022   0.088   0.703  -1.324\n",
            "  -0.232   0.534  -1.585   2.462   1.066   0.441]\n",
            "Quantized input: \n",
            "[[179  76  85  90  74  81  72  78  87  77  89  87  89  83  81  84  82  81\n",
            "   91  88  87  88  82  84  89  85  89  85  86  87  91  80  84  85  86  82\n",
            "   84  86  81  90  87  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-79.098  -4.42    0.116   5.117   2.274   0.9    -0.383  -2.512  -6.393\n",
            "  -4.205  -1.32    1.628   0.42    0.428   0.462  -2.829  -1.579  -0.141\n",
            "   0.283   0.323   0.064   0.371  -1.102  -1.513   0.901   1.581  -1.067\n",
            "   1.087   1.508  -2.178   0.416  -1.089  -1.949   4.596  -1.642  -0.727\n",
            "   2.379  -0.192   0.105   0.568   1.438  -2.431]\n",
            "Quantized input: \n",
            "[[179  75  85  95  89  86  84  79  71  76  82  88  85  85  85  79  81  84\n",
            "   85  85  85  85  82  81  86  88  82  87  88  80  85  82  80  94  81  83\n",
            "   89  84  85  86  87  79]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-79.495   4.976  -7.149  -5.626   2.681   0.563  -0.552  -1.374  -0.054\n",
            "  -4.812   1.03    3.212  -0.115   0.91   -0.582  -0.543  -3.455   4.053\n",
            "   0.264  -0.982   2.165  -0.983   0.081  -1.982  -0.543   4.084   0.164\n",
            "  -1.614   0.385  -1.843  -0.177  -0.674  -0.766  -0.312  -1.501  -0.53\n",
            "   0.309  -0.959   0.718   1.146  -0.967  -0.15 ]\n",
            "Quantized input: \n",
            "[[178  95  70  73  90  86  83  82  84  75  87  91  84  86  83  83  77  93\n",
            "   85  82  89  82  85  80  83  93  85  81  85  81  84  83  83  84  81  83\n",
            "   85  83  86  87  83  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-79.095   9.185  -1.788   2.765   3.396  -2.991  -1.336  -0.543   0.408\n",
            "  -4.756   0.272  -0.988   4.761  -0.831  -1.312   2.004  -3.962   0.306\n",
            "   3.504   0.293   0.755   0.605  -2.288   1.03   -1.082   0.196   0.157\n",
            "   0.904  -2.276   0.718  -0.832  -1.262   2.441   0.313   0.68   -0.058\n",
            "   0.87    0.874  -1.916   0.191   0.377  -2.659]\n",
            "Quantized input: \n",
            "[[179 103  81  90  91  78  82  83  85  75  85  82  94  83  82  89  76  85\n",
            "   92  85  86  86  80  87  82  85  85  86  80  86  83  82  90  85  86  84\n",
            "   86  86  81  85  85  79]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-79.427   8.036   6.797   0.448  -3.134  -1.416   1.078  -0.941   5.189\n",
            "  -3.933  -1.434   0.445  -1.472  -2.529   0.735  -0.729  -5.631   0.908\n",
            "   0.328  -0.041  -0.337   0.589   0.124  -1.132   1.14   -1.929  -1.136\n",
            "   1.549   0.02   -0.569  -1.419  -0.853   0.543   1.416   1.204   0.693\n",
            "  -0.764  -0.203   0.992   0.62    1.489   0.96 ]\n",
            "Quantized input: \n",
            "[[178 101  98  85  78  82  87  83  95  76  82  85  81  79  86  83  73  86\n",
            "   85  84  84  86  85  82  87  81  82  88  85  83  82  83  86  87  87  86\n",
            "   83  84  87  86  88  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-79.255   3.236   8.201  -8.784   1.118   0.131   1.249   2.747   1.15\n",
            "  -3.306   1.079   2.255  -0.248   2.327  -2.474   2.228  -3.972  -1.505\n",
            "  -1.072  -0.035   0.75   -1.156   2.238   1.706   2.104  -0.344   0.54\n",
            "  -0.961  -0.162   0.173  -0.613  -0.562   0.898   1.253   0.107  -1.881\n",
            "  -1.974  -1.194  -0.93    2.328   0.343  -0.577]\n",
            "Quantized input: \n",
            "[[178  91 101  66  87  85  87  90  87  78  87  89  84  89  79  89  76  81\n",
            "   82  84  86  82  89  88  89  84  86  83  84  85  83  83  86  87  85  81\n",
            "   80  82  83  89  85  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.715 -10.679  -2.039  -2.637   0.306   1.941   3.742   4.48    4.288\n",
            "  -5.238   3.431  -0.891  -2.292   3.682   1.205   0.625  -0.758   0.514\n",
            "  -2.037   1.638   1.343   1.107   1.874  -0.268   0.761  -1.372  -0.033\n",
            "  -0.681  -1.613  -1.001   2.659   0.373  -1.61    1.679  -1.454   1.048\n",
            "   2.074  -0.291  -0.364  -0.783  -1.045   2.501]\n",
            "Quantized input: \n",
            "[[182  63  80  79  85  88  92  94  93  74  92  83  80  92  87  86  83  86\n",
            "   80  88  87  87  88  84  86  82  84  83  81  82  90  85  81  88  82  87\n",
            "   89  84  84  83  82  90]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-76.513 -10.205   6.21    1.957  -0.131   0.658   3.185   1.682   5.937\n",
            "  -4.356  -3.164   3.081   0.594  -2.881   0.417   2.673  -0.309  -2.328\n",
            "   2.103   4.238  -0.601   1.399  -0.467   1.947   0.048  -2.713   1.594\n",
            "   1.135  -0.335  -0.684  -0.539  -0.957  -0.349  -1.335  -1.48   -0.015\n",
            "   0.688  -0.456   1.627  -0.363   0.57    0.558]\n",
            "Quantized input: \n",
            "[[184  64  97  89  84  86  91  88  97  76  78  91  86  79  85  90  84  80\n",
            "   89  93  83  87  84  89  85  79  88  87  84  83  83  83  84  82  81  84\n",
            "   86  84  88  84  86  86]]\n",
            "Quantized output:\n",
            "[188]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-73.043  -4.628   8.229   5.146   5.244   1.823   3.06    2.672  -1.771\n",
            "  -4.502   2.421  -2.253   0.762  -1.084  -1.608   0.901   0.151  -0.111\n",
            "  -0.527   1.36   -1.787  -0.523  -1.079  -0.191   1.39    1.838  -0.5\n",
            "   0.189   1.012  -1.114  -0.04    0.213  -0.167  -0.247  -1.346   0.996\n",
            "   0.356  -1.226  -0.498  -1.008   1.037  -0.191]\n",
            "Quantized input: \n",
            "[[191  75 101  95  95  88  91  90  81  75  89  80  86  82  81  86  85  84\n",
            "   83  87  81  83  82  84  87  88  83  85  87  82  84  85  84  84  82  87\n",
            "   85  82  83  82  87  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.551  -5.76   -7.834   2.088   3.723   2.362   2.513   0.953  -5.868\n",
            "  -4.349  -2.19    0.364   3.315   1.085   0.133  -1.522  -0.554  -0.513\n",
            "  -1.625   0.841   2.288  -1.215   1.032   0.328  -0.99    2.307   0.264\n",
            "  -0.428  -0.579   2.331   1.181  -1.13    1.782   0.847   0.37   -0.614\n",
            "  -0.063   0.522   0.194   2.818  -2.17    1.57 ]\n",
            "Quantized input: \n",
            "[[182  73  68  89  92  89  90  86  72  76  80  85  91  87  85  81  83  83\n",
            "   81  86  89  82  87  85  82  89  85  84  83  89  87  82  88  86  85  83\n",
            "   84  86  85  90  80  88]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.921  -0.701  -4.385   8.809  -1.219   0.867  -4.541  -3.107   0.027\n",
            "  -2.807   3.314  -0.792  -0.973  -2.26   -0.297  -2.493  -0.56   -1.608\n",
            "   1.56    1.258   1.739  -0.693  -2.716  -0.942  -0.933   0.244  -0.897\n",
            "   1.088   1.054   0.062  -1.004   1.77    1.084   1.411   0.164   1.739\n",
            "  -1.023  -0.969   1.516   2.158  -0.641  -4.242]\n",
            "Quantized input: \n",
            "[[181  83  75 103  82  86  75  78  85  79  91  83  82  80  84  79  83  81\n",
            "   88  87  88  83  79  83  83  85  83  87  87  85  82  88  87  87  85  88\n",
            "   82  83  88  89  83  76]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.219   2.48   -0.006  -0.003  -9.247  -3.318  -3.94   -0.963  -1.861\n",
            "  -3.222  -2.69   -0.528  -3.219   2.492  -2.4     1.147  -0.178   0.549\n",
            "   0.935   0.126   0.629   2.027  -2.243  -2.399   3.102   0.905  -0.097\n",
            "  -0.623   1.931  -0.958  -1.898  -0.779   0.462  -0.782  -0.308  -1.281\n",
            "   2.669  -2.145  -2.412  -1.183   0.361   0.814]\n",
            "Quantized input: \n",
            "[[181  90  84  84  65  78  76  83  81  78  79  83  78  90  80  87  84  86\n",
            "   86  85  86  89  80  80  91  86  84  83  88  83  81  83  85  83  84  82\n",
            "   90  80  80  82  85  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.405   4.61   -1.15  -10.088  -2.289  -0.059  -3.123  -4.466  -1.835\n",
            "  -1.145   3.131   1.884   3.091  -0.284   0.643  -0.278  -1.65    2.131\n",
            "   0.167  -0.22    0.343   2.846   2.913   0.236  -0.732  -1.675  -0.151\n",
            "   1.148  -0.221  -1.176  -0.546  -2.582  -0.712  -3.023   1.089  -1.294\n",
            "   1.147   1.058   0.264   1.481   1.038  -0.371]\n",
            "Quantized input: \n",
            "[[180  94  82  64  80  84  78  75  81  82  91  88  91  84  86  84  81  89\n",
            "   85  84  85  90  90  85  83  81  84  87  84  82  83  79  83  78  87  82\n",
            "   87  87  85  88  87  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.655   0.042   1.436  -2.773   1.706  -2.086  -0.62   -1.5     1.949\n",
            "  -2.946  -0.451  -0.089   1.274   0.396   1.015   0.563  -1.795   0.365\n",
            "  -0.591  -1.288   1.024  -0.965   0.061   1.522  -0.7    -0.827   0.47\n",
            "  -0.568   0.115  -0.488  -0.323   0.01    0.443   0.687  -0.425   0.733\n",
            "  -0.698  -1.362  -0.124  -0.698  -0.667  -0.6  ]\n",
            "Quantized input: \n",
            "[[198  85  87  79  88  80  83  81  89  78  84  84  87  85  87  86  81  85\n",
            "   83  82  87  83  85  88  83  83  85  83  85  83  84  85  85  86  84  86\n",
            "   83  82  84  83  83  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.098   1.4     2.968  -2.757   0.462  -0.652  -1.067  -0.686  -1.209\n",
            "  -3.08    1.036   0.849  -1.114   0.231  -1.403   1.224  -1.724  -2.137\n",
            "   0.433   0.047   0.697  -0.543   1.292   0.33    0.915  -1.604   0.002\n",
            "   0.553  -0.46   -0.037  -0.368  -0.007   0.011  -1.053  -0.778   0.898\n",
            "  -0.959   0.888   1.975  -0.412  -0.475   0.173]\n",
            "Quantized input: \n",
            "[[197  87  91  79  85  83  82  83  82  78  87  86  82  85  82  87  81  80\n",
            "   85  85  86  83  87  85  86  81  85  86  84  84  84  84  85  82  83  86\n",
            "   83  86  89  84  84  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.447   2.039   0.804   2.216  -2.504   2.513   0.053   1.442   0.411\n",
            "  -3.452  -0.439  -0.713  -0.106  -0.928  -0.74   -0.163  -2.498  -0.502\n",
            "  -0.866  -0.306   0.107   1.042  -0.173  -1.173  -0.134  -0.54    0.838\n",
            "   1.207   0.455   0.757  -0.983  -0.265  -0.45   -1.91   -1.587   0.843\n",
            "   0.989   1.638  -0.646  -0.863  -0.477   0.524]\n",
            "Quantized input: \n",
            "[[199  89  86  89  79  90  85  87  85  77  84  83  84  83  83  84  79  83\n",
            "   83  84  85  87  84  82  84  83  86  87  85  86  82  84  84  81  81  86\n",
            "   87  88  83  83  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.19    0.557  -0.443  -2.081   0.603  -1.724   1.32    1.028   2.737\n",
            "  -3.29   -0.299   1.47    1.346  -1.266   0.463  -0.571  -1.682  -0.87\n",
            "  -1.252  -0.965   0.186  -0.773  -0.345  -0.042  -0.461   0.209  -0.261\n",
            "   0.744  -1.007   0.649  -1.249   0.542   1.472  -0.153  -1.695  -1.43\n",
            "  -0.063  -0.214  -0.849   0.027  -0.12    1.126]\n",
            "Quantized input: \n",
            "[[199  86  84  80  86  81  87  87  90  78  84  88  87  82  85  83  81  83\n",
            "   82  83  85  83  84  84  84  85  84  86  82  86  82  86  88  84  81  82\n",
            "   84  84  83  85  84  87]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.74    1.886  -0.315  -1.157   1.746   1.009   2.75    1.274   3.002\n",
            "  -3.756   0.587  -0.238  -0.318  -0.146  -0.764   0.568  -0.651  -2.235\n",
            "   0.235  -0.336  -0.079  -0.609  -0.928  -0.15    1.098  -0.923  -1.119\n",
            "   1.66   -0.309  -1.224  -0.603  -0.076  -1.213  -0.276  -0.12    1.49\n",
            "  -1.098   0.521   0.291   0.685  -0.232  -0.416]\n",
            "Quantized input: \n",
            "[[198  88  84  82  88  87  90  87  91  77  86  84  84  84  83  86  83  80\n",
            "   85  84  84  83  83  84  87  83  82  88  84  82  83  84  82  84  84  88\n",
            "   82  86  85  86  84  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.256   1.964   1.853   0.493   2.81    0.418   1.661   2.114   1.532\n",
            "  -3.556  -0.635   0.905   0.198   0.087  -0.139   0.274  -0.765  -0.714\n",
            "  -0.883  -1.013  -0.802   0.032   0.062   0.434   1.148  -0.487   1.178\n",
            "  -1.468   0.892   0.699  -0.758  -1.787  -0.839   0.58   -0.88    0.412\n",
            "   1.138  -1.471  -0.612  -0.6    -0.233   0.145]\n",
            "Quantized input: \n",
            "[[197  89  88  86  90  85  88  89  88  77  83  86  85  85  84  85  83  83\n",
            "   83  82  83  85  85  85  87  83  87  81  86  86  83  81  83  86  83  85\n",
            "   87  81  83  83  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.127   0.235   3.803   0.428   1.272  -0.204   2.013   0.365   0.208\n",
            "  -3.094   1.267   0.218   1.005  -0.719   0.369   0.166  -0.213  -1.207\n",
            "  -0.021  -0.455  -0.136   0.657   0.656   0.844  -0.9    -0.164   1.116\n",
            "   2.079  -1.281  -0.543   1.273   1.158  -1.245   0.585  -1.169  -0.314\n",
            "   0.614   0.033   0.274   0.507   0.422  -0.272]\n",
            "Quantized input: \n",
            "[[197  85  92  85  87  84  89  85  85  78  87  85  87  83  85  85  84  82\n",
            "   84  84  84  86  86  86  83  84  87  89  82  83  87  87  82  86  82  84\n",
            "   86  85  85  86  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.635   0.792  -1.49   -0.774  -1.064  -0.026  -0.181   0.242   1.176\n",
            "  -4.14   -0.646  -0.257  -1.91    0.367   0.545   0.004  -1.727  -0.102\n",
            "  -1.1    -0.108  -2.694   0.457  -0.349  -1.248   0.623  -0.783   0.232\n",
            "  -1.481   0.498  -0.292   0.612  -0.118  -0.961   1.647   0.492  -0.402\n",
            "  -0.299   1.1     0.328  -0.505  -0.478  -0.508]\n",
            "Quantized input: \n",
            "[[180  86  81  83  82  84  84  85  87  76  83  84  81  85  86  85  81  84\n",
            "   82  84  79  85  84  82  86  83  85  81  86  84  86  84  83  88  86  84\n",
            "   84  87  85  83  84  83]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.787  -1.523  -1.604   0.637  -0.294  -0.544   0.264  -0.112   2.267\n",
            "  -3.493   0.75    0.143   1.633  -1.527   0.269   1.206  -1.1    -0.872\n",
            "   1.673  -0.157  -1.451  -0.462  -1.699   1.397  -0.342   0.439  -1.095\n",
            "   1.32   -0.268  -0.211  -0.579  -0.221  -0.073  -0.453   1.253  -0.372\n",
            "   0.248   0.623   1.163   1.393   0.211   0.099]\n",
            "Quantized input: \n",
            "[[179  81  81  86  84  83  85  84  89  77  86  85  88  81  85  87  82  83\n",
            "   88  84  82  84  81  87  84  85  82  87  84  84  83  84  84  84  87  84\n",
            "   85  86  87  87  85  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.237  -3.471  -1.027   0.912   0.638   0.777   0.025   0.751  -0.132\n",
            "  -3.857  -0.309   0.971   0.87    0.259  -1.273   1.572  -0.383   0.836\n",
            "  -0.269  -1.531   0.047  -1.502   0.148  -1.696  -0.791   1.496  -0.868\n",
            "  -0.682  -1.118   0.455   0.363   1.246   0.797   0.407  -0.21   -0.227\n",
            "   1.97   -0.143  -0.719  -0.034  -0.381   0.079]\n",
            "Quantized input: \n",
            "[[183  77  82  86  86  86  85  86  84  77  84  86  86  85  82  88  84  86\n",
            "   84  81  85  81  85  81  83  88  83  83  82  85  85  87  86  85  84  84\n",
            "   89  84  83  84  84  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.764   2.604  -0.55   -1.669   0.474  -0.595  -0.297  -0.299  -0.935\n",
            "  -3.567   0.916   2.19    1.859   0.53   -0.146   0.663  -0.951  -1.35\n",
            "   0.585   1.552  -1.539   0.579   1.732   1.24    0.416   1.588   0.574\n",
            "  -0.445  -0.004   0.27    0.086   1.217  -0.487  -0.968   0.342  -0.299\n",
            "  -0.666  -1.087  -0.685  -0.465  -1.311   1.141]\n",
            "Quantized input: \n",
            "[[179  90  83  81  85  83  84  84  83  77  86  89  88  86  84  86  83  82\n",
            "   86  88  81  86  88  87  85  88  86  84  84  85  85  87  83  83  85  84\n",
            "   83  82  83  84  82  87]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-78.635   2.942   1.254  -0.987   1.118  -1.146  -1.007  -1.557   2.028\n",
            "  -3.59   -0.072  -2.075   0.51   -0.254   1.171  -1.338  -1.323   0.406\n",
            "   0.605   1.556  -0.161  -0.468   0.498   0.942  -1.963  -1.178   0.234\n",
            "  -0.419  -0.697   0.469  -0.454   0.453  -1.45   -0.773  -0.901   0.797\n",
            "   0.69   -0.266   0.014   0.578   0.576  -1.199]\n",
            "Quantized input: \n",
            "[[180  91  87  82  87  82  82  81  89  77  84  80  86  84  87  82  82  85\n",
            "   86  88  84  84  86  86  80  82  85  84  83  85  84  85  82  83  83  86\n",
            "   86  84  85  86  86  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-77.893   2.467   2.151  -2.43   -0.195  -2.228   0.16    0.516   0.806\n",
            "  -3.62   -1.052   2.698  -0.752  -0.069  -0.752  -0.949  -0.705   1.245\n",
            "  -0.715   1.329   0.71   -0.551  -0.991  -0.281   0.382   1.168  -0.616\n",
            "   0.815  -0.421   0.378  -0.189   0.311  -1.444   0.23   -0.496   1.242\n",
            "   2.438  -0.764   1.205  -0.849  -0.324   0.73 ]\n",
            "Quantized input: \n",
            "[[181  90  89  79  84  80  85  86  86  77  82  90  83  84  83  83  83  87\n",
            "   83  87  86  83  82  84  85  87  83  86  84  85  84  85  82  85  83  87\n",
            "   90  83  87  83  84  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-76.99    1.68    1.794  -3.427   1.512  -0.174   0.702  -1.231   0.301\n",
            "  -3.331   2.085  -0.648   2.17   -0.015  -0.259   1.148  -0.358   1.113\n",
            "   0.357   1.043   0.748   0.71   -0.817  -0.244  -0.36   -0.761  -0.348\n",
            "   0.402   1.121   0.989   0.222  -1.745  -0.871  -0.305   0.867  -1.995\n",
            "   1.525  -0.678  -0.115   0.481  -1.512   0.713]\n",
            "Quantized input: \n",
            "[[183  88  88  77  88  84  86  82  85  78  89  83  89  84  84  87  84  87\n",
            "   85  87  86  86  83  84  84  83  84  85  87  87  85  81  83  84  86  80\n",
            "   88  83  84  85  81  86]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.399   0.737   1.185  -1.938   0.746  -0.87   -1.437  -0.277  -0.474\n",
            "  -2.39   -0.02   -0.367   0.54   -0.375   1.708   0.096   0.104   2.733\n",
            "   1.352  -0.361   1.013  -0.232   1.588   1.434  -0.242  -0.428  -0.454\n",
            "   1.055   0.836  -1.271  -0.74   -0.345   0.667  -1.255  -0.003   0.558\n",
            "  -0.548   0.014  -1.562  -0.271  -0.164  -0.624]\n",
            "Quantized input: \n",
            "[[197  86  87  81  86  83  82  84  84  80  84  84  86  84  88  85  85  90\n",
            "   87  84  87  84  88  87  84  84  84  87  86  82  83  84  86  82  84  86\n",
            "   83  85  81  84  84  83]]\n",
            "Quantized output:\n",
            "[128]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.408   1.558   2.208  -2.886   1.772  -2.437   0.818  -2.354  -0.343\n",
            "  -2.166   0.758   1.266   0.147  -1.592   0.602  -0.88    0.656   1.832\n",
            "   0.141  -0.036  -0.125   0.806   1.212  -0.954   0.815   0.301   0.42\n",
            "  -0.238   0.404   0.587  -0.093  -0.276   2.369  -0.464  -0.681  -1.057\n",
            "  -0.443  -0.438   0.265  -0.583  -0.887  -0.412]\n",
            "Quantized input: \n",
            "[[193  88  89  79  88  79  86  80  84  80  86  87  85  81  86  83  86  88\n",
            "   85  84  84  86  87  83  86  85  85  84  85  86  84  84  89  84  83  82\n",
            "   84  84  85  83  83  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-74.061   2.629   2.312  -2.48    0.982  -0.743  -1.12    1.336   0.964\n",
            "  -2.002  -1.366  -0.192   2.326  -0.828   2.411  -0.552   1.547   4.366\n",
            "   1.022  -0.703   0.665   0.366  -0.831  -0.139  -0.178   0.327   0.227\n",
            "   0.901   0.847  -0.316   0.392  -0.78    0.988   0.37   -0.153   1.138\n",
            "  -1.13    0.211   0.271   0.649  -0.056   0.203]\n",
            "Quantized input: \n",
            "[[189  90  89  79  87  83  82  87  86  80  82  84  89  83  89  83  88  93\n",
            "   87  83  86  85  83  84  84  85  85  86  86  84  85  83  87  85  84  87\n",
            "   82  85  85  86  84  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.201   0.801   0.431  -2.073   0.506  -2.406  -0.871  -2.331   0.375\n",
            "  -2.176  -0.486   1.397   0.904  -0.056   0.83   -0.314  -0.322   2.193\n",
            "  -0.5    -0.801   0.974  -1.441  -0.803   0.943  -0.061  -1.419  -0.109\n",
            "  -0.705  -0.868  -1.324   0.578  -0.637  -0.268  -0.122  -1.067   0.99\n",
            "   1.192  -0.711   0.626   0.212   0.204  -0.351]\n",
            "Quantized input: \n",
            "[[199  86  85  80  86  80  83  80  85  80  83  87  86  84  86  84  84  89\n",
            "   83  83  87  82  83  86  84  82  84  83  83  82  86  83  84  84  82  87\n",
            "   87  83  86  85  85  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.272   1.829  -0.46   -2.552  -0.286  -1.393  -0.034   0.548   4.01\n",
            "  -2.925  -0.193   1.097   2.574  -1.607  -0.217   1.358   0.576  -0.64\n",
            "  -0.868   0.174   0.679   0.535  -0.342   0.035  -0.411   0.642  -0.071\n",
            "   1.641  -0.144  -0.514   0.22   -0.595   1.064   0.854   0.276  -0.38\n",
            "   0.968  -0.005   0.825  -0.406  -0.723  -0.3  ]\n",
            "Quantized input: \n",
            "[[199  88  84  79  84  82  84  86  93  78  84  87  90  81  84  87  86  83\n",
            "   83  85  86  86  84  85  84  86  84  88  84  83  85  83  87  86  85  84\n",
            "   86  84  86  84  83  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.011   3.372  -0.182  -2.761   1.919  -0.37    3.203   1.818   3.254\n",
            "  -3.469   0.085  -0.686  -1.046  -1.157   1.56    0.166   0.064  -0.717\n",
            "   0.757   0.239   0.147  -0.848   0.434  -0.5     0.915  -1.188  -0.506\n",
            "   0.439   0.32    0.337  -0.055  -0.622   1.473  -0.628   1.184  -0.828\n",
            "  -0.167  -0.946  -0.675  -0.067  -0.483   0.535]\n",
            "Quantized input: \n",
            "[[197  91  84  79  88  84  91  88  91  77  85  83  82  82  88  85  85  83\n",
            "   86  85  85  83  85  83  86  82  83  85  85  85  84  83  88  83  87  83\n",
            "   84  83  83  84  84  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.079   3.258   2.242   0.038   3.894   1.168   2.55    1.637   1.06\n",
            "  -3.284  -0.609   2.036  -0.289   1.608   0.7    -0.301   0.44    0.179\n",
            "  -0.972  -0.603  -0.046  -0.186   0.752   1.817  -0.089  -0.457  -1.137\n",
            "   0.259   0.594  -0.345  -0.274  -0.095   0.448  -0.033   0.517  -0.448\n",
            "   0.197  -0.345  -0.017   0.643   0.732  -1.049]\n",
            "Quantized input: \n",
            "[[197  91  89  85  93  87  90  88  87  78  83  89  84  88  86  84  85  85\n",
            "   83  83  84  84  86  88  84  84  82  85  86  84  84  84  85  84  86  84\n",
            "   85  84  84  86  86  82]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-71.117   1.673   0.392  -0.887   1.692   0.627   1.944   1.085   2.347\n",
            "  -3.553  -0.355   0.859   1.354  -0.855   0.528   0.557   1.134  -1.653\n",
            "  -0.345   0.984  -0.025  -0.213   0.613  -0.442   0.444   0.596   0.98\n",
            "   0.8     0.257   0.354  -0.133  -0.083  -0.215  -0.057  -0.327  -1.262\n",
            "  -0.001   0.654  -0.293  -0.39   -0.552   0.305]\n",
            "Quantized input: \n",
            "[[195  88  85  83  88  86  89  87  89  77  84  86  87  83  86  86  87  81\n",
            "   84  87  84  84  86  84  85  86  87  86  85  85  84  84  84  84  84  82\n",
            "   84  86  84  84  83  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-70.154   0.566  -0.56   -0.904   0.337  -0.311   1.54    1.342   2.118\n",
            "  -3.218   0.076   0.714   0.926  -0.19    1.537   0.504   0.941   1.162\n",
            "  -1.066  -0.362  -0.047   0.324  -0.609   0.114   0.186  -0.643  -0.796\n",
            "  -0.036  -0.168   0.086  -0.707  -0.028  -0.62    0.975  -0.738   0.322\n",
            "   0.15   -0.611   0.213   0.078   0.343  -0.05 ]\n",
            "Quantized input: \n",
            "[[197  86  83  83  85  84  88  87  89  78  85  86  86  84  88  86  86  87\n",
            "   82  84  84  85  83  85  85  83  83  84  84  85  83  84  83  87  83  85\n",
            "   85  83  85  85  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-69.701  -0.79   -0.182  -0.719   1.027  -0.964   0.572  -0.064   2.302\n",
            "  -3.129  -0.361   0.56    0.186  -0.273   0.399   1.299   1.5     0.842\n",
            "   0.396   0.113   0.213  -0.811  -0.526   0.831  -0.083  -0.837   0.196\n",
            "   0.548  -1.763   0.345  -0.467   1.5     0.531  -0.658   0.043   0.69\n",
            "  -0.775  -0.457   0.532  -0.609   0.246   0.316]\n",
            "Quantized input: \n",
            "[[198  83  84  83  87  83  86  84  89  78  84  86  85  84  85  87  88  86\n",
            "   85  85  85  83  83  86  84  83  85  86  81  85  84  88  86  83  85  86\n",
            "   83  84  86  83  85  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.078   1.424   2.145  -0.021   2.448   0.107   2.113   1.953   2.684\n",
            "  -3.61    0.497  -0.133  -0.797   0.008   1.09   -0.758   0.938  -1.026\n",
            "   0.722   0.67    0.536   0.367   0.293   0.648   0.246  -0.753   0.212\n",
            "   0.065   0.162   0.795  -0.027   0.057   0.081  -1.39    0.734   0.588\n",
            "  -0.798   0.467   0.086  -0.766  -0.554  -0.1  ]\n",
            "Quantized input: \n",
            "[[193  87  89  84  90  85  89  89  90  77  86  84  83  85  87  83  86  82\n",
            "   86  86  86  85  85  86  85  83  85  85  85  86  84  85  85  82  86  86\n",
            "   83  85  85  83  83  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.542  -0.079   3.366  -0.168   1.86    0.309   2.857   1.419   1.298\n",
            "  -3.589  -0.675   1.401   0.259   1.039  -0.149  -0.832   1.412   0.474\n",
            "  -0.005   0.128   0.726  -0.16    0.172  -0.129  -0.752  -0.695  -0.314\n",
            "  -0.279   0.378   0.28   -0.784   0.228  -0.098  -0.059  -0.231   0.32\n",
            "   0.264  -0.842   0.988  -1.256   0.546  -0.075]\n",
            "Quantized input: \n",
            "[[192  84  91  84  88  85  90  87  87  77  83  87  85  87  84  83  87  85\n",
            "   84  85  86  84  85  84  83  83  84  84  85  85  83  85  84  84  84  85\n",
            "   85  83  87  82  86  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.297  -2.077   2.97   -0.706   1.988   0.64    0.964   0.686   0.413\n",
            "  -3.199   1.148  -0.225   1.531  -0.608  -0.11   -0.542   1.494   1.355\n",
            "  -0.561  -0.45    0.438  -0.24   -0.335  -0.329  -0.171   0.794  -0.228\n",
            "   0.915  -0.056   0.026   0.62   -0.512  -0.235  -0.273  -0.577  -0.942\n",
            "   0.503  -0.288  -0.361  -0.265   1.22    0.12 ]\n",
            "Quantized input: \n",
            "[[193  80  91  83  89  86  86  86  85  78  87  84  88  83  84  83  88  87\n",
            "   83  84  85  84  84  84  84  86  84  86  84  85  86  83  84  84  83  83\n",
            "   86  84  84  84  87  85]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.92   -3.048   1.007  -0.568   1.612  -0.935   1.069   0.539   1.923\n",
            "  -3.067  -1.224   0.839   0.577  -2.271   0.106   0.743   1.435  -0.277\n",
            "   1.3     0.65    0.112  -0.112  -0.196   0.215   0.094  -1.036  -0.304\n",
            "   0.639   0.21    1.173  -0.1    -0.327   1.48   -0.899   1.381   0.371\n",
            "  -0.124   1.042  -0.135   0.101  -0.18    0.73 ]\n",
            "Quantized input: \n",
            "[[191  78  87  83  88  83  87  86  88  78  82  86  86  80  85  86  87  84\n",
            "   87  86  85  84  84  85  85  82  84  86  85  87  84  84  88  83  87  85\n",
            "   84  87  84  85  84  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.814   1.989  -0.003  -1.531  -0.081  -0.049   1.612   1.873   1.133\n",
            "  -2.65   -1.682   1.254  -1.858   1.625   0.302  -0.223   0.594  -1.757\n",
            "   1.252  -1.431   2.92   -0.019  -0.442   0.236  -0.997  -0.461  -1.856\n",
            "  -0.004   0.15    1.296  -0.857   0.193   2.385  -0.344   0.363  -1.083\n",
            "   0.217  -0.583   0.861  -0.324  -0.634  -1.521]\n",
            "Quantized input: \n",
            "[[186  89  84  81  84  84  88  88  87  79  81  87  81  88  85  84  86  81\n",
            "   87  82  91  84  84  85  82  84  81  84  85  87  83  85  89  84  85  82\n",
            "   85  83  86  84  83  81]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.069   0.341  -0.95    0.308  -0.145  -0.501   0.464   0.042   3.366\n",
            "  -2.524   1.075  -0.965   0.495  -2.131   2.583  -0.174   0.208  -0.193\n",
            "   0.071  -0.338   4.156  -0.292   1.067  -1.23    0.008   0.518   1.385\n",
            "   0.987  -0.077   0.944   1.046   0.404   0.73   -0.926   0.615   0.414\n",
            "   0.272  -1.343   0.665  -0.115  -1.969   1.045]\n",
            "Quantized input: \n",
            "[[187  85  83  85  84  83  85  85  91  79  87  83  86  80  90  84  85  84\n",
            "   85  84  93  84  87  82  85  86  87  87  84  86  87  85  86  83  86  85\n",
            "   85  82  86  84  80  87]]\n",
            "Quantized output:\n",
            "[210]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-74.935  -1.121  -0.656  -1.021   1.982  -1.473   0.374  -0.437  -0.182\n",
            "  -2.83   -0.917   1.763   2.79   -0.554  -0.127   1.425   0.762  -0.163\n",
            "  -0.852   0.692   2.17    2.002   1.679   1.768   0.872  -1.046   2.62\n",
            "   0.361  -2.045   0.219   0.488   0.271   0.02    2.104  -1.454   1.14\n",
            "   0.479   0.544   0.165  -0.518  -0.763  -0.708]\n",
            "Quantized input: \n",
            "[[187  82  83  82  89  81  85  84  84  79  83  88  90  83  84  87  86  84\n",
            "   83  86  89  89  88  88  86  82  90  85  80  85  86  85  85  89  82  87\n",
            "   85  86  85  83  83  83]]\n",
            "Quantized output:\n",
            "[7]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-76.91    2.144   1.601  -1.164   2.797   0.673   1.311  -0.193   2.956\n",
            "  -2.181   1.423   1.17    2.403   1.707  -0.496  -0.892   0.421   0.28\n",
            "   0.82   -2.094   0.697  -1.581  -1.464  -0.888  -0.667  -1.086  -2.278\n",
            "   1.019   0.949  -1.642  -1.824  -0.369   0.349  -0.049   0.566  -0.394\n",
            "  -0.031  -0.653  -1.85    0.7     1.495   0.937]\n",
            "Quantized input: \n",
            "[[183  89  88  82  90  86  87  84  91  80  87  87  89  88  83  83  85  85\n",
            "   86  80  86  81  81  83  83  82  80  87  86  81  81  84  85  84  86  84\n",
            "   84  83  81  86  88  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.874   0.93    2.731  -0.256   1.601  -1.587   0.544   1.425   0.294\n",
            "  -2.641  -1.009  -1.566   1.464  -2.231  -1.49   -1.086   0.23    0.714\n",
            "  -1.486  -2.496  -2.128  -0.144  -0.355   0.394   1.431   0.399  -0.839\n",
            "  -1.057  -0.63    0.203  -0.689  -1.817   1.023  -0.193  -0.009  -0.698\n",
            "  -0.465   1.711  -0.979   2.403   0.412  -0.17 ]\n",
            "Quantized input: \n",
            "[[185  86  90  84  88  81  86  87  85  79  82  81  88  80  81  82  85  86\n",
            "   81  79  80  84  84  85  87  85  83  82  83  85  83  81  87  84  84  83\n",
            "   84  88  82  89  85  84]]\n",
            "Quantized output:\n",
            "[96]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.109  -0.546   2.534  -1.471   0.252  -0.133   1.292  -1.189  -0.185\n",
            "  -2.766   1.267   1.888  -1.423  -3.607   0.185   1.425  -0.034  -0.305\n",
            "   1.143  -0.809  -2.043  -0.3     0.335   0.733   0.339  -0.481   2.046\n",
            "   0.093   0.007  -1.716  -1.052   0.507  -0.539   1.239   0.587   1.971\n",
            "  -0.998   0.676   0.097   1.156   0.106  -0.976]\n",
            "Quantized input: \n",
            "[[187  83  90  81  85  84  87  82  84  79  87  88  82  77  85  87  84  84\n",
            "   87  83  80  84  85  86  85  84  89  85  85  81  82  86  83  87  86  89\n",
            "   82  86  85  87  85  82]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.073  -1.263   0.818  -2.041   1.375  -0.434  -1.255   0.435   3.031\n",
            "  -3.668  -0.863   0.385   0.213   1.645   2.139   3.301  -0.132   0.393\n",
            "   0.934  -0.638  -0.82    1.302   0.517   0.741   1.611   0.356   2.367\n",
            "  -1.076  -1.227   0.409  -0.611   0.661  -0.454  -0.146  -1.031  -0.409\n",
            "  -0.228  -0.665   0.148  -0.708  -0.25    0.074]\n",
            "Quantized input: \n",
            "[[187  82  86  80  87  84  82  85  91  77  83  85  85  88  89  91  84  85\n",
            "   86  83  83  87  86  86  88  85  89  82  82  85  83  86  84  84  82  84\n",
            "   84  83  85  83  84  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-73.317   0.792  -2.308   0.206   1.767   2.647   0.332  -1.056   1.879\n",
            "  -3.325  -0.812   1.214   0.133   0.137  -0.503   0.923   0.896  -0.788\n",
            "   0.011   0.144   1.494   0.576  -0.255  -0.134   0.273   0.081   0.901\n",
            "  -0.121  -0.604   0.05   -0.941  -0.198   0.142  -0.546  -0.3     0.623\n",
            "   0.108   0.026   1.153  -0.686  -0.589  -0.05 ]\n",
            "Quantized input: \n",
            "[[191  86  80  85  88  90  85  82  88  78  83  87  85  85  83  86  86  83\n",
            "   85  85  88  86  84  84  85  85  86  84  83  85  83  84  85  83  84  86\n",
            "   85  85  87  83  83  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.267  -1.996  -2.443  -1.158  -1.073  -0.64    2.419   2.317  -0.264\n",
            "  -3.197   0.909  -0.218  -0.707  -0.606  -0.237  -0.727   0.212  -1.294\n",
            "  -0.067   0.443   0.529  -0.245   0.365   0.849   0.576  -0.812   0.871\n",
            "  -0.227  -0.645  -0.762  -0.326   0.931   0.015  -0.052  -0.675   0.839\n",
            "  -0.46    0.578  -0.524   0.899   0.125  -0.101]\n",
            "Quantized input: \n",
            "[[193  80  79  82  82  83  89  89  84  78  86  84  83  83  84  83  85  82\n",
            "   84  85  86  84  85  86  86  83  86  84  83  83  84  86  85  84  83  86\n",
            "   84  86  83  86  85  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-72.062  -4.154   0.194   0.626   0.563  -0.351  -0.191   0.214   4.125\n",
            "  -2.907  -0.43    0.366   1.143  -0.279   0.127  -0.522  -0.209  -0.875\n",
            "  -1.646   0.453  -0.878   0.255   0.74    0.432   1.135  -0.393   0.883\n",
            "   0.266   0.71    0.264  -0.739  -0.451   0.799  -0.276  -0.383  -0.401\n",
            "  -0.628  -0.04   -0.237   0.55    0.464   0.65 ]\n",
            "Quantized input: \n",
            "[[193  76  85  86  86  84  84  85  93  79  84  85  87  84  85  83  84  83\n",
            "   81  85  83  85  86  85  87  84  86  85  86  85  83  84  86  84  84  84\n",
            "   83  84  84  86  85  86]]\n",
            "Quantized output:\n",
            "[46]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-74.816   2.4     0.477   1.317   0.972  -2.219   0.174   2.854   2.673\n",
            "  -3.246   1.021  -0.033   0.765  -0.009   0.496   0.388   0.497  -0.577\n",
            "   0.656  -0.304   0.971   0.452   0.184  -0.455  -0.159  -0.689   0.111\n",
            "   0.319  -0.637  -0.274  -0.736   0.481  -0.055   0.012  -0.025  -0.512\n",
            "   0.231  -0.837   0.395  -1.2    -0.53    0.217]\n",
            "Quantized input: \n",
            "[[188  89  85  87  86  80  85  90  90  78  87  84  86  84  86  85  86  83\n",
            "   86  84  86  85  85  84  84  83  85  85  83  84  83  85  84  85  84  83\n",
            "   85  83  85  82  83  85]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.576   2.16    2.812  -1.285  -0.598   0.852   3.774  -0.489   2.026\n",
            "  -3.279  -1.318   0.439   0.271   0.115   0.139  -0.351   0.289  -0.149\n",
            "   0.24   -0.552   0.607  -0.413   0.365   0.422  -0.054  -0.334  -1.01\n",
            "  -0.116  -0.931   0.011  -0.229   0.045  -0.478  -0.368   0.342  -0.458\n",
            "  -0.199  -0.803  -0.118  -0.266   0.23    0.957]\n",
            "Quantized input: \n",
            "[[186  89  90  82  83  86  92  83  89  78  82  85  85  85  85  84  85  84\n",
            "   85  83  86  84  85  85  84  84  82  84  83  85  84  85  84  84  85  84\n",
            "   84  83  84  84  85  86]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-75.199   0.431   3.18   -2.762   3.107   0.978  -1.177   1.969   0.425\n",
            "  -2.885   1.447   0.639   0.373  -0.176  -0.12   -0.521   0.274   0.545\n",
            "  -0.921  -1.295  -0.373  -0.797   0.144   0.092  -0.972   0.395  -0.378\n",
            "  -0.101   0.013   1.002  -0.155  -0.715  -0.032   0.506   0.357   0.186\n",
            "  -0.012   0.111  -0.872   1.049   0.147  -0.016]\n",
            "Quantized input: \n",
            "[[187  85  91  79  91  87  82  89  85  79  87  86  85  84  84  83  85  86\n",
            "   83  82  84  83  85  85  82  85  84  84  85  87  84  83  84  86  85  85\n",
            "   84  85  83  87  85  84]]\n",
            "Quantized output:\n",
            "[0]\n",
            "De-quantized output:\n",
            "[0.]\n",
            "-----------------------------\n",
            "Non-quantized input:\n",
            "[-74.27   -1.296   1.872  -0.604   2.74   -3.311   1.684  -1.813  -0.423\n",
            "  -3.046  -0.74   -0.164   0.999  -1.249   0.411   0.684   0.241   0.153\n",
            "   0.835  -0.484  -1.075  -0.645  -1.102  -0.095   0.553   0.578   0.618\n",
            "   0.504   0.34    0.185  -0.27    0.787  -0.662   0.336  -0.32    0.745\n",
            "   0.537  -0.519   1.039   0.879   0.279  -0.425]\n",
            "Quantized input: \n",
            "[[189  82  88  83  90  78  88  81  84  78  83  84  87  82  85  86  85  85\n",
            "   86  84  82  83  82  84  86  86  86  86  85  85  84  86  83  85  84  86\n",
            "   86  83  87  86  85  84]]\n",
            "Quantized output:\n",
            "[249]\n",
            "De-quantized output:\n",
            "[1.]\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Quantized TFLite Model Test Accuracy: 91.72932330827066\n"
          ]
        }
      ],
      "source": [
        "# Passing the Int8 TF Lite model to the interpreter.\n",
        "interpreter = tf.lite.Interpreter('int8_quant_mlp.tflite')\n",
        "\n",
        "# Allocating tensors.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Evaluating the model on the test dataset.\n",
        "test_accuracy_quant = evaluate(interpreter)\n",
        "\n",
        "# Printing the test accuracy for the FP-16 quantized TFLite model and the baseline Keras model.\n",
        "print('Quantized TFLite Model Test Accuracy:', test_accuracy_quant*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over 90% accuracy! It's gone down from our non-quantized model, but it's still good enough for our application. Let's take a final look at our model before converting it for the ESP32:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNW-CMIyEPfA",
        "outputId": "6918eee7-3305-4020-9ce8-bd39e146d1bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== int8_quant_mlp.tflite ===\n",
            "\n",
            "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
            "T# represents the Tensor numbers. For example, in Subgraph#0, the QUANTIZE op takes\n",
            "tensor #0 as input and produces tensor #15 as output.\n",
            "\n",
            "Subgraph#0 main(T#0) -> [T#24]\n",
            "  Op#0 QUANTIZE(T#0) -> [T#15]\n",
            "  Op#1 FULLY_CONNECTED(T#15, T#14, T#13[13, 16, -64, -171, -8, ...]) -> [T#16]\n",
            "  Op#2 FULLY_CONNECTED(T#16, T#12, T#11[13, -418, -280, 502, 436, ...]) -> [T#17]\n",
            "  Op#3 FULLY_CONNECTED(T#17, T#10, T#9[509, 127, -312, -224, 246, ...]) -> [T#18]\n",
            "  Op#4 FULLY_CONNECTED(T#18, T#8, T#7[-117, -124, 230, 292, -140, ...]) -> [T#19]\n",
            "  Op#5 FULLY_CONNECTED(T#19, T#6, T#5[-26, -1, 1, 3]) -> [T#20]\n",
            "  Op#6 FULLY_CONNECTED(T#20, T#4, T#3[-1, 0]) -> [T#21]\n",
            "  Op#7 FULLY_CONNECTED(T#21, T#2, T#1[1466]) -> [T#22]\n",
            "  Op#8 LOGISTIC(T#22) -> [T#23]\n",
            "  Op#9 QUANTIZE(T#23) -> [T#24]\n",
            "\n",
            "Tensors of Subgraph#0\n",
            "  T#0(serving_default_input_layer:0) shape_signature:[-1, 42], type:UINT8\n",
            "  T#1(tfl.pseudo_qconst) shape:[1], type:INT32 RO 4 bytes, buffer: 2, data:[1466]\n",
            "  T#2(tfl.pseudo_qconst1) shape:[1, 2], type:INT8 RO 2 bytes, buffer: 3, data:[., .]\n",
            "  T#3(tfl.pseudo_qconst2) shape:[2], type:INT32 RO 8 bytes, buffer: 4, data:[-1, 0]\n",
            "  T#4(tfl.pseudo_qconst3) shape:[2, 4], type:INT8 RO 8 bytes, buffer: 5, data:[., ., ., 1, ., ...]\n",
            "  T#5(tfl.pseudo_qconst4) shape:[4], type:INT32 RO 16 bytes, buffer: 6, data:[-26, -1, 1, 3]\n",
            "  T#6(tfl.pseudo_qconst5) shape:[4, 8], type:INT8 RO 32 bytes, buffer: 7, data:[{, ., ., ., O, ...]\n",
            "  T#7(tfl.pseudo_qconst6) shape:[8], type:INT32 RO 32 bytes, buffer: 8, data:[-117, -124, 230, 292, -140, ...]\n",
            "  T#8(tfl.pseudo_qconst7) shape:[8, 16], type:INT8 RO 128 bytes, buffer: 9, data:[., ., ., ., ., ...]\n",
            "  T#9(tfl.pseudo_qconst8) shape:[16], type:INT32 RO 64 bytes, buffer: 10, data:[509, 127, -312, -224, 246, ...]\n",
            "  T#10(tfl.pseudo_qconst9) shape:[16, 32], type:INT8 RO 512 bytes, buffer: 11, data:[F, \n",
            ", ., g, ., ...]\n",
            "  T#11(tfl.pseudo_qconst10) shape:[32], type:INT32 RO 128 bytes, buffer: 12, data:[13, -418, -280, 502, 436, ...]\n",
            "  T#12(tfl.pseudo_qconst11) shape:[32, 42], type:INT8 RO 1344 bytes, buffer: 13, data:[., L, ., ., ., ...]\n",
            "  T#13(tfl.pseudo_qconst12) shape:[42], type:INT32 RO 168 bytes, buffer: 14, data:[13, 16, -64, -171, -8, ...]\n",
            "  T#14(tfl.pseudo_qconst13) shape:[42, 42], type:INT8 RO 1764 bytes, buffer: 15, data:[., (, ., ., ., ...]\n",
            "  T#15(tfl.quantize) shape_signature:[-1, 42], type:INT8\n",
            "  T#16(sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd) shape_signature:[-1, 42], type:INT8\n",
            "  T#17(sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd) shape_signature:[-1, 32], type:INT8\n",
            "  T#18(sequential_1/dense_2_1/MatMul;sequential_1/dense_2_1/Relu;sequential_1/dense_2_1/BiasAdd) shape_signature:[-1, 16], type:INT8\n",
            "  T#19(sequential_1/dense_3_1/MatMul;sequential_1/dense_3_1/Relu;sequential_1/dense_3_1/BiasAdd) shape_signature:[-1, 8], type:INT8\n",
            "  T#20(sequential_1/dense_4_1/MatMul;sequential_1/dense_4_1/Relu;sequential_1/dense_4_1/BiasAdd) shape_signature:[-1, 4], type:INT8\n",
            "  T#21(sequential_1/dense_5_1/MatMul;sequential_1/dense_5_1/Relu;sequential_1/dense_5_1/BiasAdd) shape_signature:[-1, 2], type:INT8\n",
            "  T#22(sequential_1/dense_6_1/MatMul;sequential_1/dense_6_1/Add) shape_signature:[-1, 1], type:INT8\n",
            "  T#23(StatefulPartitionedCall_1:01) shape_signature:[-1, 1], type:INT8\n",
            "  T#24(StatefulPartitionedCall_1:0) shape_signature:[-1, 1], type:UINT8\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Your TFLite model has '1' signature_def(s).\n",
            "\n",
            "Signature#0 key: 'serving_default'\n",
            "- Subgraph: Subgraph#0\n",
            "- Inputs: \n",
            "    'input_layer' : T#0\n",
            "- Outputs: \n",
            "    'output_0' : T#24\n",
            "\n",
            "---------------------------------------------------------------\n",
            "              Model size:      11368 bytes\n",
            "    Non-data buffer size:       7030 bytes (61.84 %)\n",
            "  Total data buffer size:       4338 bytes (38.16 %)\n",
            "    (Zero value buffers):         16 bytes (00.14 %)\n",
            "\n",
            "* Buffers of TFLite model are mostly used for constant tensors.\n",
            "  And zero value buffers are buffers filled with zeros.\n",
            "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
            "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tf.lite.experimental.Analyzer.analyze(model_path=\"int8_quant_mlp.tflite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting the model to C data array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to convert our quantized .tflite model to a C data array format that can be easily loaded and interpreted in the ESP32. We use this helper function to convert it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1BDEGFsuEm9-"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def convert_bytes_to_c_source(data,\n",
        "                              array_name,\n",
        "                              max_line_width=80,\n",
        "                              include_guard=None,\n",
        "                              include_path=None,\n",
        "                              use_tensorflow_license=False):\n",
        "  \"\"\"Returns strings representing a C constant array containing `data`.\n",
        "\n",
        "  Args:\n",
        "    data: Byte array that will be converted into a C constant.\n",
        "    array_name: String to use as the variable name for the constant array.\n",
        "    max_line_width: The longest line length, for formatting purposes.\n",
        "    include_guard: Name to use for the include guard macro definition.\n",
        "    include_path: Optional path to include in the source file.\n",
        "    use_tensorflow_license: Whether to include the standard TensorFlow Apache2\n",
        "      license in the generated files.\n",
        "\n",
        "  Returns:\n",
        "    Text that can be compiled as a C source file to link in the data as a\n",
        "    literal array of values.\n",
        "    Text that can be used as a C header file to reference the literal array.\n",
        "  \"\"\"\n",
        "\n",
        "  starting_pad = \"   \"\n",
        "  array_lines = []\n",
        "  array_line = starting_pad\n",
        "  for value in bytearray(data):\n",
        "    if (len(array_line) + 4) > max_line_width:\n",
        "      array_lines.append(array_line + \"\\n\")\n",
        "      array_line = starting_pad\n",
        "    array_line += \" 0x%02x,\" % (value,)\n",
        "  if len(array_line) > len(starting_pad):\n",
        "    array_lines.append(array_line + \"\\n\")\n",
        "  array_values = \"\".join(array_lines)\n",
        "\n",
        "  if include_guard is None:\n",
        "    include_guard = \"TENSORFLOW_LITE_UTIL_\" + array_name.upper() + \"_DATA_H_\"\n",
        "\n",
        "  if include_path is not None:\n",
        "    include_line = \"#include \\\"{include_path}\\\"\\n\".format(\n",
        "        include_path=include_path)\n",
        "  else:\n",
        "    include_line = \"\"\n",
        "\n",
        "  if use_tensorflow_license:\n",
        "    license_text = \"\"\"\n",
        "/* Copyright {year} The TensorFlow Authors. All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "==============================================================================*/\n",
        "\"\"\".format(year=datetime.date.today().year)\n",
        "  else:\n",
        "    license_text = \"\"\n",
        "\n",
        "  source_template = \"\"\"{license_text}\n",
        "// This is a TensorFlow Lite model file that has been converted into a C data\n",
        "// array using the tensorflow.lite.util.convert_bytes_to_c_source() function.\n",
        "// This form is useful for compiling into a binary for devices that don't have a\n",
        "// file system.\n",
        "\n",
        "{include_line}\n",
        "#include \"model.h\"\n",
        "\n",
        "alignas(8) const unsigned char {array_name}[] = {{\n",
        "{array_values}}};\n",
        "const int {array_name}_len = {array_length};\n",
        "\"\"\"\n",
        "\n",
        "  source_text = source_template.format(\n",
        "      array_name=array_name,\n",
        "      array_length=len(data),\n",
        "      array_values=array_values,\n",
        "      license_text=license_text,\n",
        "      include_line=include_line)\n",
        "\n",
        "  header_template = \"\"\"\n",
        "{license_text}\n",
        "\n",
        "// This is a TensorFlow Lite model file that has been converted into a C data\n",
        "// array using the tensorflow.lite.util.convert_bytes_to_c_source() function.\n",
        "// This form is useful for compiling into a binary for devices that don't have a\n",
        "// file system.\n",
        "\n",
        "#ifndef {include_guard}\n",
        "#define {include_guard}\n",
        "\n",
        "extern const unsigned char {array_name}[];\n",
        "extern const int {array_name}_len;\n",
        "\n",
        "#endif  // {include_guard}\n",
        "\"\"\"\n",
        "\n",
        "  header_text = header_template.format(\n",
        "      array_name=array_name,\n",
        "      include_guard=include_guard,\n",
        "      license_text=license_text)\n",
        "\n",
        "  return source_text, header_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('int8_quant_mlp.tflite', 'rb') as f:\n",
        "    bytes_data = f.read()\n",
        "    source_text, source_header = convert_bytes_to_c_source(bytes_data, \"g_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('model.cc', 'w') as f:\n",
        "    f.write(source_text)\n",
        "\n",
        "with open('model.h', 'w') as f:\n",
        "    f.write(source_header)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're all set! Now our model is saved at ```model.cc``` and ```model.h``` in a format suitable for use in the ESP32. If you made modifications to the models or the feature engineering process, remember to copy and pastecboth files, as well as the ```preprocessing.cc``` and ```preprocessing.h``` to the ```OnboardInference``` folder."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sbrc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
